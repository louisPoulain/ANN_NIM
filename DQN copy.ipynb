{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon greedy :  0.5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from nim_env import NimEnv, OptimalPlayer\n",
    "from helpers import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = NimEnv(seed = 3)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------- helpers DQN ----------------------------\n",
    "\n",
    "def to_input(heaps):\n",
    "    #change the format of the heaps so that it can be used as an input for the neural network. \n",
    "    init_state = torch.zeros(9, device = device)\n",
    "    for i in range(3):\n",
    "        state = bin(heaps[i])[2:]\n",
    "        j = 0 \n",
    "        while j < len(state):\n",
    "            init_state[i*3 + 2 - j] = np.int16(state[len(state) - 1 - j])\n",
    "            j += 1\n",
    "    return init_state.clone().detach()\n",
    "\n",
    "            \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 21)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x.to(device))\n",
    "        x = x.view(-1, 9)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQN_Player(OptimalPlayer):\n",
    "    def __init__(self, epsilon, player, target_net):\n",
    "        super(DQN_Player, self).__init__(epsilon = epsilon, player = player)\n",
    "        self.target_net = target_net\n",
    "        \n",
    "    def QL_Move(self, heaps):\n",
    "        state = to_input(heaps)\n",
    "        q = self.target_net(state)\n",
    "        argmax = torch.argmax(q)\n",
    "        move = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "        action = move\n",
    "        return move, action\n",
    "           \n",
    "    def act(self, heaps, **kwargs):\n",
    "        return self.QL_Move(heaps)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "#parameters\n",
    "GAMMA = 0.99\n",
    "buffer_size = 10000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE = 500\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "#Epsilon greedy : \n",
    "EPS_GREEDY = 0.5 #random.random()\n",
    "print(\"epsilon greedy : \", EPS_GREEDY)\n",
    "\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer =optim.Adam(policy_net.parameters(), lr = 5*1e-4)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "memory = ReplayMemory(buffer_size)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    #eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > 1-EPS_GREEDY: #eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            q = policy_net(state)\n",
    "            #print(\"policy predicted : \", q)\n",
    "            argmax = torch.argmax(q)\n",
    "            result = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "            #print(\"select action :\", result)\n",
    "            if  result[0]==0 or result[1] == 0 :\n",
    "                print(\"aussercours dans select action avec q = policy_net(state)\")\n",
    "                print(result)\n",
    "                print(\"policy predicted : \", q)\n",
    "            return result\n",
    "    else:\n",
    "        result = torch.tensor([random.randrange(1,4), random.randrange(1,8)], device=device, dtype=torch.long)\n",
    "        #print(\"if sampel > eps : \", result)\n",
    "        if  result[0]==0 or result[1] == 0 :\n",
    "                print(\"aussercours dans select action avec result = random\")\n",
    "                print(result)\n",
    "        return result\n",
    "\n",
    "\"\"\"def select_action_target(state):\n",
    "    q = target_net(state)\n",
    "    argmax = torch.argmax(q)\n",
    "    result = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "    return result\"\"\"\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #print(\"non final mask : \", non_final_mask)\n",
    "    #print(\"non final next state : \", [s for s in batch.next_state if s is not None])\n",
    "    if [s for s in batch.next_state if s is not None] : #is false if the list is empty\n",
    "        #print(\"non empty list\")\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    else : \n",
    "        #print(\"empty list\")\n",
    "        non_final_next_states = torch.empty(1) \n",
    "           \n",
    "    #print(\"non final next states after : \", non_final_next_states)                                      \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    #print(\"state batch :\", state_batch.shape)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch) #64 x 21\n",
    "    #print(\"state action values : \", state_action_values) \n",
    "    #print(\"action \", action_batch)\n",
    "    #print(\"action batch : \", (action_batch[::2]-1+(action_batch[1::2]-1)*3))\n",
    "\n",
    "    state_action_values = state_action_values.gather(1, (action_batch[::2]-1+(action_batch[1::2]-1)*3).view(BATCH_SIZE, 1)) #64 x 1\n",
    "    #print(\"state action values after: \", state_action_values.squeeze(1))\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    if len(non_final_next_states) > 1:\n",
    "        #print(\"target net : \")\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch  #64\n",
    "    #print(\"expected state action values : \", expected_state_action_values) #64 x 1\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def DQN_one_game(playerDQN, playerOpt, env):\n",
    "    heaps, _, _ = env.observe()\n",
    "    i = 0\n",
    "    while not env.end:\n",
    "        if env.current_player == playerOpt.player:\n",
    "            move = playerOpt.act(heaps)\n",
    "            heaps, end, winner = env.step(move)\n",
    "        else: \n",
    "            move, action = playerDQN.act(heaps)\n",
    "            heaps, end, winner = env.step(move)\n",
    "        \n",
    "        i += 1\n",
    "    return env.reward(playerDQN.player)\n",
    "\n",
    "\n",
    "def Q1_DQN(target_net, nb_games = 20000, eps = 0.2, eps_opt = 0.5, step = 250, seed = None, question = 'q3-1'):\n",
    "    Rewards = []\n",
    "    Steps = []\n",
    "    total_reward = 0.0\n",
    "    env = NimEnv(seed = seed)\n",
    "    playerOpt = OptimalPlayer(epsilon = eps_opt, player = 0)\n",
    "    playerDQN = DQN_Player(epsilon = eps, player = 1, target_net = target_net) \n",
    "    for i in range(nb_games):\n",
    "        #print('New game\\n')\n",
    "        # switch turns at every game\n",
    "        if i % 2 == 0:\n",
    "            playerOpt.player = 0\n",
    "            playerDQN.player = 1\n",
    "        else:\n",
    "            playerOpt.player = 1\n",
    "            playerDQN.player = 0\n",
    "        \n",
    "        total_reward += DQN_one_game(playerDQN, playerOpt, env)\n",
    "        if i % step == step - 1:\n",
    "            Rewards.append(total_reward / step)\n",
    "            Steps.append(i)\n",
    "            total_reward = 0.0\n",
    "        env.reset()\n",
    "        #print(playerQL.qvals['746'])\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.plot(Steps, Rewards)\n",
    "    plt.title('Evolution of average reward every 250 games')\n",
    "    plt.xlabel('Number of games played')\n",
    "    plt.ylabel('Average reward for QL-player')\n",
    "    plt.savefig('./Data/' + question + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "Complete\n",
      "───────────────────────────────────\n",
      "Heap 1: |||             \t (3)\n",
      "───────────────────────────────────\n",
      "Heap 2:                 \t (0)\n",
      "───────────────────────────────────\n",
      "Heap 3:                 \t (0)\n",
      "───────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10001\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    heaps, _, _ = env.observe()\n",
    "    state = to_input(heaps)\n",
    "    #print(\"state : \", state.shape)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        is_available = env.check_valid(action)\n",
    "        if not is_available: \n",
    "            #i.e. if the action is not valid, we give the agent a negative reward\n",
    "            #print(\"is not available : \", is_available)\n",
    "            reward = torch.tensor([-1], device=device)\n",
    "            next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            break\n",
    "            \n",
    "        else : #if the action is valid, we make a step\n",
    "            #print(\"is available : \", is_available)\n",
    "            heaps, done, reward = env.step(action)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == None:\n",
    "                #the game has not ended, we add a reward of 0.\n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else : \n",
    "                #the game has ended and we add a reward of 1. \n",
    "                reward = torch.tensor([1], device=device)\n",
    "\n",
    "            if not done:\n",
    "                next_state = to_input(heaps)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                #episode_durations.append(t + 1)\n",
    "                #plot_durations()\n",
    "                break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play against one player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game end, winner is player 0\n",
      "Optimal player 1 = 0\n",
      "DQN player 2 = 1\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "You cannot take more objects than there are in the heap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=13'>14</a>\u001b[0m     move \u001b[39m=\u001b[39m select_action_target(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=14'>15</a>\u001b[0m \u001b[39m#print(\"move : \", move)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=15'>16</a>\u001b[0m heaps, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(move)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=16'>17</a>\u001b[0m \u001b[39m#env.render()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n",
      "File \u001b[0;32m~/Documents/GitHub/ANN_NIM/nim_env.py:59\u001b[0m, in \u001b[0;36mNimEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=56'>57</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheap_avail[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mThe selected heap is already empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=57'>58</a>\u001b[0m \u001b[39massert\u001b[39;00m n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou must take at least 1 object from the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=58'>59</a>\u001b[0m \u001b[39massert\u001b[39;00m (n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]), \u001b[39m\"\u001b[39m\u001b[39mYou cannot take more objects than there are in the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n  \u001b[39m# core of the action\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: You cannot take more objects than there are in the heap"
     ]
    }
   ],
   "source": [
    "Turns = np.array([0,1])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    heaps, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[0]) #not so optimal player (best : epsi = 0)\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1]) #our player\n",
    "    #env.render()\n",
    "    while not env.end:\n",
    "        if env.current_player == player_opt_1.player:\n",
    "            move = player_opt_1.act(heaps)\n",
    "        else:\n",
    "            state = to_input(heaps)\n",
    "            move = select_action_target(state)\n",
    "        #print(\"move : \", move)\n",
    "        heaps, end, winner = env.step(move)\n",
    "        #env.render()\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  str(Turns[0]))\n",
    "            print('DQN player 2 = ' +  str(Turns[1]))\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array([0,1])\n",
    "Player_DQN = DQN_Player(epsilon=0.5, player = Turns[1], target_net = target_net)\n",
    "Player_opt = OptimalPlayer(epsilon=0.5, player=Turns[0])\n",
    "reward = QL_one_game(Player_DQN, Player_opt, eps = 0, eps_opt = 0, alpha = 0, gamma = 0, env = env, update = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses :  []\n",
      "0\n",
      "loss :  0.022673431783914566\n",
      "500\n",
      "loss :  0.006362982094287872\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "loss :  0.004506346769630909\n",
      "loss :  0.0044937958009541035\n",
      "2500\n",
      "loss :  0.004374979995191097\n",
      "loss :  0.002913573756814003\n",
      "3000\n",
      "loss :  0.0028058644384145737\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "loss :  0.0011636404087767005\n",
      "5000\n",
      "5500\n",
      "loss :  0.0009431239450350404\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "loss :  0.0008056056103669107\n",
      "7500\n",
      "loss :  0.0007395156426355243\n",
      "8000\n",
      "8500\n",
      "loss :  0.000545498332940042\n",
      "9000\n",
      "loss :  0.0005287214880809188\n",
      "9500\n",
      "loss :  0.0007824922213330865\n",
      "loss :  0.0005555802490562201\n",
      "loss :  0.0005575870745815337\n",
      "loss :  0.00043190139695070684\n",
      "loss :  0.0008373021264560521\n",
      "10000\n",
      "loss :  0.0008611102239228785\n",
      "10500\n",
      "loss :  0.0017546917079016566\n",
      "11000\n",
      "loss :  0.0005231364048086107\n",
      "loss :  0.0008728599641472101\n",
      "11500\n",
      "loss :  0.0028638294897973537\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "loss :  0.0008176650735549629\n",
      "14500\n",
      "loss :  0.00042911991477012634\n",
      "15000\n",
      "15500\n",
      "loss :  0.0004916220204904675\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "loss :  0.0002499801921658218\n",
      "loss :  0.0002877411898225546\n",
      "19500\n",
      "Complete\n",
      "───────────────────────────────────\n",
      "Heap 1: |               \t (1)\n",
      "───────────────────────────────────\n",
      "Heap 2: |               \t (1)\n",
      "───────────────────────────────────\n",
      "Heap 3: ||              \t (2)\n",
      "───────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 20000\n",
    "losses = []\n",
    "print(\"losses : \", losses)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    heaps, _, _ = env.observe()\n",
    "    state = to_input(heaps)\n",
    "    #print(\"state : \", state.shape)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        is_available = env.check_valid(action)\n",
    "        if not is_available: \n",
    "            #i.e. if the action is not valid, we give the agent a negative reward\n",
    "            #print(\"is not available : \")\n",
    "            reward = torch.tensor([-1], device=device)\n",
    "            next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            break\n",
    "            \n",
    "        else : #if the action is valid, we make a step\n",
    "            _, done, reward = env.step(action)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == None: \n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else : \n",
    "                #print(\"reward not None : \", reward)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            heaps, _, _ = env.observe()\n",
    "            if not done:\n",
    "                next_state = to_input(heaps)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                #episode_durations.append(t + 1)\n",
    "                #plot_durations()\n",
    "                losses.append(loss)\n",
    "                print(\"loss : \", loss)\n",
    "                break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0011116937967017293, 0.0004791765531990677, 0.0014446345157921314, 0.0006353636854328215, 0.0004516641783993691, 0.0005009748274460435, 0.0006440210854634643, 0.0010924437083303928, 0.0003929889062419534, 0.0006850946228951216, 0.0006849135388620198, 0.0003566378145478666, 0.0007577439537271857, 0.0007167788571678102, 0.00022508430993184447, 0.00020175274403300136, 0.000519493711180985, 0.00035526545252650976, 0.00034041202161461115, 0.00026659510331228375, 0.00025759299751371145, 0.000324135588016361, 0.0002875941863749176, 0.00022826428175903857, 0.0002539473935030401, 0.0002804423857014626, 0.00017615678370930254, 0.00019927072571590543, 0.0002800890651997179]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 29 into shape (250)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(losses)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=1'>2</a>\u001b[0m losses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(losses)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=2'>3</a>\u001b[0m aver_losses \u001b[39m=\u001b[39m (losses\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m250\u001b[39;49m))\u001b[39m.\u001b[39mmean(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(aver_losses)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 29 into shape (250)"
     ]
    }
   ],
   "source": [
    "print(losses)\n",
    "losses = np.array(losses)\n",
    "aver_losses = (losses.reshape(-1, 250)).mean(axis = 1)\n",
    "print(aver_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_DQN(target_net, nb_games = 20000, eps = 0.2, eps_opt = 0.5, step = 250, seed = None, question = 'q3-1')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aa7871422cbf9296a09eca5272ae12f42feac121f15077f0682f5d4affb8114"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
