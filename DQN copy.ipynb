{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from nim_env import NimEnv, OptimalPlayer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = NimEnv(seed = 3)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First test of DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 21)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x.to(device))\n",
    "        x = x.view(-1, 9)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "buffer_size = 10000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "def to_input(heaps):\n",
    "    #change the format of the heaps so that it can be used as an input for the neural network. \n",
    "    init_state = torch.zeros(9, device = device)\n",
    "    for i in range(3):\n",
    "        state = bin(heaps[i])[2:]\n",
    "        j = 0 \n",
    "        while j < len(state):\n",
    "            init_state[i*3 + 2 - j] = np.int16(state[len(state) - 1 - j])\n",
    "            j += 1\n",
    "    return init_state.clone().detach()\n",
    "\n",
    "#test\n",
    "heaps = [2, 5, 7]\n",
    "print(to_input(heaps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "#Epsilon greedy : \n",
    "EPS_GREEDY = 0.5 #random.random()\n",
    "print(EPS_GREEDY)\n",
    "\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer =optim.Adam(policy_net.parameters(), lr = 5*1e-4)\n",
    "memory = ReplayMemory(buffer_size)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    #eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > 1-EPS_GREEDY: #eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            q = policy_net(state)\n",
    "            #print(\"policy predicted : \", q)\n",
    "            argmax = torch.argmax(q)\n",
    "            result = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "            #print(\"select action :\", result)\n",
    "            if  result[0]==0 or result[1] == 0 :\n",
    "                print(\"aussercours dans select action avec q = policy_net(state)\")\n",
    "                print(result)\n",
    "                print(\"policy predicted : \", q)\n",
    "            return result\n",
    "    else:\n",
    "        result = torch.tensor([random.randrange(1,4), random.randrange(1,8)], device=device, dtype=torch.long)\n",
    "        #print(\"if sampel > eps : \", result)\n",
    "        if  result[0]==0 or result[1] == 0 :\n",
    "                print(\"aussercours dans select action avec result = random\")\n",
    "                print(result)\n",
    "        return result\n",
    "\n",
    "def select_action_target(state):\n",
    "    q = target_net(state)\n",
    "    argmax = torch.argmax(q)\n",
    "    result = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "    return result\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #print(\"non final mask : \", non_final_mask)\n",
    "    #print(\"non final next state : \", [s for s in batch.next_state if s is not None])\n",
    "    if [s for s in batch.next_state if s is not None] : #is false if the list is empty\n",
    "        #print(\"non empty list\")\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    else : \n",
    "        #print(\"empty list\")\n",
    "        non_final_next_states = torch.empty(1) \n",
    "           \n",
    "    #print(\"non final next states after : \", non_final_next_states)                                      \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    #print(\"state batch :\", state_batch.shape)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch) #64 x 21\n",
    "    #print(\"state action values : \", state_action_values) \n",
    "    #print(\"action \", action_batch)\n",
    "    #print(\"action batch : \", (action_batch[::2]-1+(action_batch[1::2]-1)*3))\n",
    "\n",
    "    state_action_values = state_action_values.gather(1, (action_batch[::2]-1+(action_batch[1::2]-1)*3).view(BATCH_SIZE, 1)) #64 x 1\n",
    "    #print(\"state action values after: \", state_action_values.squeeze(1))\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    if len(non_final_next_states) > 1:\n",
    "        #print(\"target net : \")\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch  #64\n",
    "    #print(\"expected state action values : \", expected_state_action_values) #64 x 1\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "Complete\n",
      "───────────────────────────────────\n",
      "Heap 1: |||||           \t (5)\n",
      "───────────────────────────────────\n",
      "Heap 2:                 \t (0)\n",
      "───────────────────────────────────\n",
      "Heap 3: |||||||         \t (7)\n",
      "───────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5001\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    heaps, _, _ = env.observe()\n",
    "    state = to_input(heaps)\n",
    "    #print(\"state : \", state.shape)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        is_available = env.check_valid(action)\n",
    "        if not is_available: \n",
    "            #i.e. if the action is not valid, we give the agent a negative reward\n",
    "            #print(\"is not available : \")\n",
    "            reward = torch.tensor([-1], device=device)\n",
    "            next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            break\n",
    "            \n",
    "        else : #if the action is valid, we make a step\n",
    "            heaps, done, reward = env.step(action)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == None: \n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else : \n",
    "                #print(\"reward not None : \", reward)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if not done:\n",
    "                next_state = to_input(heaps)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                #episode_durations.append(t + 1)\n",
    "                #plot_durations()\n",
    "                break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play against one player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You cannot take more objects than there are in the heap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=13'>14</a>\u001b[0m     move \u001b[39m=\u001b[39m select_action_target(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=14'>15</a>\u001b[0m \u001b[39m#print(\"move : \", move)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=15'>16</a>\u001b[0m heaps, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(move)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=16'>17</a>\u001b[0m \u001b[39m#env.render()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n",
      "File \u001b[0;32m~/Documents/GitHub/ANN_NIM/nim_env.py:59\u001b[0m, in \u001b[0;36mNimEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=56'>57</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheap_avail[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mThe selected heap is already empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=57'>58</a>\u001b[0m \u001b[39massert\u001b[39;00m n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou must take at least 1 object from the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=58'>59</a>\u001b[0m \u001b[39massert\u001b[39;00m (n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]), \u001b[39m\"\u001b[39m\u001b[39mYou cannot take more objects than there are in the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n  \u001b[39m# core of the action\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: You cannot take more objects than there are in the heap"
     ]
    }
   ],
   "source": [
    "Turns = np.array([0,1])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    heaps, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[0]) #not so optimal player (best : epsi = 0)\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1]) #our player\n",
    "    #env.render()\n",
    "    while not env.end:\n",
    "        if env.current_player == player_opt_1.player:\n",
    "            move = player_opt_1.act(heaps)\n",
    "        else:\n",
    "            state = to_input(heaps)\n",
    "            move = select_action_target(state)\n",
    "        #print(\"move : \", move)\n",
    "        heaps, end, winner = env.step(move)\n",
    "        #env.render()\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  str(Turns[0]))\n",
    "            print('DQN player 2 = ' +  str(Turns[1]))\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses :  []\n",
      "0\n",
      "500\n",
      "1000\n",
      "loss :  0.0011116937967017293\n",
      "1500\n",
      "loss :  0.0004791765531990677\n",
      "loss :  0.0014446345157921314\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "loss :  0.0006353636854328215\n",
      "4500\n",
      "5000\n",
      "loss :  0.0004516641783993691\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "loss :  0.0005009748274460435\n",
      "loss :  0.0006440210854634643\n",
      "8000\n",
      "8500\n",
      "loss :  0.0010924437083303928\n",
      "loss :  0.0003929889062419534\n",
      "9000\n",
      "loss :  0.0006850946228951216\n",
      "9500\n",
      "loss :  0.0006849135388620198\n",
      "10000\n",
      "10500\n",
      "loss :  0.0003566378145478666\n",
      "11000\n",
      "loss :  0.0007577439537271857\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "loss :  0.0007167788571678102\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "loss :  0.00022508430993184447\n",
      "14500\n",
      "loss :  0.00020175274403300136\n",
      "15000\n",
      "loss :  0.000519493711180985\n",
      "15500\n",
      "loss :  0.00035526545252650976\n",
      "16000\n",
      "loss :  0.00034041202161461115\n",
      "16500\n",
      "loss :  0.00026659510331228375\n",
      "loss :  0.00025759299751371145\n",
      "17000\n",
      "loss :  0.000324135588016361\n",
      "17500\n",
      "loss :  0.0002875941863749176\n",
      "18000\n",
      "loss :  0.00022826428175903857\n",
      "18500\n",
      "loss :  0.0002539473935030401\n",
      "loss :  0.0002804423857014626\n",
      "19000\n",
      "loss :  0.00017615678370930254\n",
      "loss :  0.00019927072571590543\n",
      "19500\n",
      "loss :  0.0002800890651997179\n",
      "Complete\n",
      "───────────────────────────────────\n",
      "Heap 1: ||||||          \t (6)\n",
      "───────────────────────────────────\n",
      "Heap 2: |               \t (1)\n",
      "───────────────────────────────────\n",
      "Heap 3: ||||            \t (4)\n",
      "───────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 20000\n",
    "losses = []\n",
    "print(\"losses : \", losses)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    heaps, _, _ = env.observe()\n",
    "    state = to_input(heaps)\n",
    "    #print(\"state : \", state.shape)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        is_available = env.check_valid(action)\n",
    "        if not is_available: \n",
    "            #i.e. if the action is not valid, we give the agent a negative reward\n",
    "            #print(\"is not available : \")\n",
    "            reward = torch.tensor([-1], device=device)\n",
    "            next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            break\n",
    "            \n",
    "        else : #if the action is valid, we make a step\n",
    "            _, done, reward = env.step(action)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == None: \n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else : \n",
    "                #print(\"reward not None : \", reward)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            heaps, _, _ = env.observe()\n",
    "            if not done:\n",
    "                next_state = to_input(heaps)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                #episode_durations.append(t + 1)\n",
    "                #plot_durations()\n",
    "                losses.append(loss)\n",
    "                print(\"loss : \", loss)\n",
    "                break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0011116937967017293, 0.0004791765531990677, 0.0014446345157921314, 0.0006353636854328215, 0.0004516641783993691, 0.0005009748274460435, 0.0006440210854634643, 0.0010924437083303928, 0.0003929889062419534, 0.0006850946228951216, 0.0006849135388620198, 0.0003566378145478666, 0.0007577439537271857, 0.0007167788571678102, 0.00022508430993184447, 0.00020175274403300136, 0.000519493711180985, 0.00035526545252650976, 0.00034041202161461115, 0.00026659510331228375, 0.00025759299751371145, 0.000324135588016361, 0.0002875941863749176, 0.00022826428175903857, 0.0002539473935030401, 0.0002804423857014626, 0.00017615678370930254, 0.00019927072571590543, 0.0002800890651997179]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 29 into shape (250)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(losses)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=1'>2</a>\u001b[0m losses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(losses)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=2'>3</a>\u001b[0m aver_losses \u001b[39m=\u001b[39m (losses\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m250\u001b[39;49m))\u001b[39m.\u001b[39mmean(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(aver_losses)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 29 into shape (250)"
     ]
    }
   ],
   "source": [
    "print(losses)\n",
    "losses = np.array(losses)\n",
    "aver_losses = (losses.reshape(-1, 250)).mean(axis = 1)\n",
    "print(aver_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aa7871422cbf9296a09eca5272ae12f42feac121f15077f0682f5d4affb8114"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
