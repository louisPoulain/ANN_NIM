{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon greedy :  0.5\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from nim_env import NimEnv, OptimalPlayer\n",
    "from helpers import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = NimEnv(seed = 3)\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------- helpers DQN ----------------------------\n",
    "\n",
    "def to_input(heaps):\n",
    "    #change the format of the heaps so that it can be used as an input for the neural network. \n",
    "    init_state = torch.zeros(9, device = device)\n",
    "    for i in range(3):\n",
    "        state = bin(heaps[i])[2:]\n",
    "        j = 0 \n",
    "        while j < len(state):\n",
    "            init_state[i*3 + 2 - j] = np.int16(state[len(state) - 1 - j])\n",
    "            j += 1\n",
    "    return init_state.clone().detach()\n",
    "\n",
    "            \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 21)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x.to(device))\n",
    "        x = x.view(-1, 9)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQN_Player(OptimalPlayer):\n",
    "    def __init__(self, player, target_net):\n",
    "        super(DQN_Player, self).__init__(player = player)\n",
    "        self.target_net = target_net\n",
    "        \n",
    "    def QL_Move(self, heaps):\n",
    "        state = to_input(heaps)\n",
    "        q = self.target_net(state)\n",
    "        print(\"in ql_move, q : \", q)\n",
    "        argmax = torch.argmax(q)\n",
    "        move = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "        print(\"Move :\", move)\n",
    "        action = move\n",
    "        return move, action\n",
    "           \n",
    "    def act(self, heaps, **kwargs):\n",
    "        return self.QL_Move(heaps)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "#parameters\n",
    "GAMMA = 0.99\n",
    "buffer_size = 10000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE = 500\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "#Epsilon greedy : \n",
    "EPS_GREEDY = 0.5 #random.random()\n",
    "print(\"epsilon greedy : \", EPS_GREEDY)\n",
    "\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer =optim.Adam(policy_net.parameters(), lr = 5*1e-4)\n",
    "criterion = nn.HuberLoss()\n",
    "memory = ReplayMemory(buffer_size)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"def select_action_target(state):\n",
    "    q = target_net(state)\n",
    "    argmax = torch.argmax(q)\n",
    "    result = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "    return result\"\"\"\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #print(\"non final mask : \", non_final_mask)\n",
    "    #print(\"non final next state : \", [s for s in batch.next_state if s is not None])\n",
    "    if [s for s in batch.next_state if s is not None] : #is false if the list is empty\n",
    "        #print(\"non empty list\")\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    else : \n",
    "        #print(\"empty list\")\n",
    "        non_final_next_states = torch.empty(1) \n",
    "           \n",
    "    #print(\"non final next states after : \", non_final_next_states)                                      \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    #print(\"state batch :\", state_batch.shape)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    #print(\"reward batch \", reward_batch)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch) #64 x 21\n",
    "    #print(\"state action values : \", state_action_values) \n",
    "    #print(\"action \", action_batch)\n",
    "    #print(\"action batch : \", (action_batch[::2]-1+(action_batch[1::2]-1)*3))\n",
    "\n",
    "    state_action_values = state_action_values.gather(1, (action_batch[::2]-1+(action_batch[1::2]-1)*3).view(BATCH_SIZE, 1)) #64 x 1\n",
    "    #print(\"state action values after: \", state_action_values.squeeze(1))\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    if len(non_final_next_states) > 1:\n",
    "        #print(\"target net : \")\n",
    "        #print(target_net(non_final_next_states).shape)\n",
    "        #print(target_net(non_final_next_states).max(1)[0].shape)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach() # max(1) : take the maximum per batch on the 21 possibilities. [0]: take the max and not the argmax\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch  #64\n",
    "    #print(\"expected state action values : \", expected_state_action_values) #64 x 1\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1).detach())\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    #print(\"before : \", policy_net.named_parameters().data)\n",
    "    loss.backward()\n",
    "    #print(\"after : \", policy_net.named_parameters().data)\n",
    "    \n",
    "    #for param in policy_net.parameters():\n",
    "        #param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    #eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > 1-EPS_GREEDY: #eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            q = policy_net(state) #21\n",
    "            print(\"policy predicted : \", q)\n",
    "            argmax = torch.argmax(q)\n",
    "            result = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "            print(\"select action :\", result)\n",
    "            if  result[0]==0 or result[1] == 0 :\n",
    "                print(\"aussercours dans select action avec q = policy_net(state)\")\n",
    "                print(result)\n",
    "                print(\"policy predicted : \", q)\n",
    "            return result\n",
    "    else:\n",
    "        random.seed()\n",
    "        result = torch.tensor([random.randrange(1,4), random.randrange(1,8)], device=device, dtype=torch.long)\n",
    "        print(\"if sample > eps : \", result)\n",
    "        if  result[0]==0 or result[1] == 0 :\n",
    "                print(\"aux secours dans select action avec result = random\")\n",
    "                print(result)\n",
    "        return result\n",
    "\n",
    "def DQN_one_game(playerDQN, playerOpt, env):\n",
    "    heaps, _, _ = env.observe()\n",
    "    i = 0\n",
    "    while not env.end:\n",
    "        env.render()\n",
    "        print(\"to input \", to_input(heaps))\n",
    "        if env.current_player == playerOpt.player:\n",
    "            move = playerOpt.act(heaps)\n",
    "            heaps, end, winner = env.step(move)\n",
    "        else: \n",
    "            move, action = playerDQN.act(heaps)\n",
    "            heaps, end, winner = env.step(move)\n",
    "        \n",
    "        i += 1\n",
    "    return env.reward(playerDQN.player)\n",
    "\n",
    "\n",
    "def Q1_DQN(target_net, nb_games = 20000, eps = 0.2, eps_opt = 0.5, step = 250, seed = None, question = 'q3-11'):\n",
    "    Rewards = []\n",
    "    Steps = []\n",
    "    total_reward = 0.0\n",
    "    env = NimEnv(seed = seed)\n",
    "    playerOpt = OptimalPlayer(epsilon = eps_opt, player = 0)\n",
    "    playerDQN = DQN_Player(epsilon = eps, player = 1, target_net = target_net) \n",
    "    for i in range(nb_games):\n",
    "        #print('New game\\n')\n",
    "        # switch turns at every game\n",
    "        if i % 2 == 0:\n",
    "            playerOpt.player = 0\n",
    "            playerDQN.player = 1\n",
    "        else:\n",
    "            playerOpt.player = 1\n",
    "            playerDQN.player = 0\n",
    "        \n",
    "        total_reward += DQN_one_game(playerDQN, playerOpt, env)\n",
    "        if i % step == step - 1:\n",
    "            Rewards.append(total_reward / step)\n",
    "            Steps.append(i)\n",
    "            total_reward = 0.0\n",
    "        env.reset()\n",
    "        #print(playerQL.qvals['746'])\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.plot(Steps, Rewards)\n",
    "    plt.title('Evolution of average reward every 250 games')\n",
    "    plt.xlabel('Number of games played')\n",
    "    plt.ylabel('Average reward for QL-player')\n",
    "    plt.savefig('./Data/' + question + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  None\n",
      "loss :  None\n",
      "loss :  None\n",
      "loss :  None\n",
      "loss :  None\n",
      "loss :  0.327739417552948\n",
      "loss :  0.11074086278676987\n",
      "loss :  0.019862208515405655\n",
      "loss :  2.3793839432073582e-07\n",
      "loss :  9.283280633098911e-08\n",
      "loss :  1.1977892455661276e-08\n",
      "loss :  7.667255719212562e-12\n",
      "loss :  9.414691248821327e-14\n",
      "loss :  1.7208456881689926e-15\n",
      "loss :  9.71445146547012e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  4.440892098500626e-16\n",
      "loss :  4.996003610813204e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  4.163336342344337e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  3.0531133177191805e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  6.106226635438361e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  5.273559366969494e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  0.0\n",
      "loss :  5.828670879282072e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  3.0531133177191805e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  3.608224830031759e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  4.440892098500626e-16\n",
      "loss :  6.661338147750939e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  8.326672684688674e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  4.718447854656915e-16\n",
      "loss :  7.494005416219807e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  4.163336342344337e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  6.661338147750939e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  2.7755575615628914e-17\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  3.0531133177191805e-16\n",
      "loss :  3.608224830031759e-16\n",
      "loss :  5.273559366969494e-16\n",
      "loss :  3.885780586188048e-16\n",
      "loss :  3.3306690738754696e-16\n",
      "loss :  6.938893903907228e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  2.7755575615628914e-17\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  3.3306690738754696e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  3.608224830031759e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  1.3600232051658168e-15\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  6.938893903907228e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  3.608224830031759e-16\n",
      "loss :  3.3306690738754696e-16\n",
      "loss :  0.0\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  3.3306690738754696e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  4.718447854656915e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  4.440892098500626e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  2.7755575615628914e-17\n",
      "loss :  3.608224830031759e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  3.3306690738754696e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  9.992007221626409e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  3.0531133177191805e-16\n",
      "loss :  3.885780586188048e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  8.604228440844963e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  8.881784197001252e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  6.661338147750939e-16\n",
      "loss :  4.718447854656915e-16\n",
      "loss :  0.0\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  6.661338147750939e-16\n",
      "loss :  4.440892098500626e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  4.163336342344337e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  4.718447854656915e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  4.440892098500626e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  6.661338147750939e-16\n",
      "loss :  5.551115123125783e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  2.7755575615628914e-17\n",
      "loss :  4.440892098500626e-16\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  4.440892098500626e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  0.0\n",
      "loss :  1.1102230246251565e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  5.551115123125783e-17\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  9.992007221626409e-16\n",
      "loss :  2.7755575615628914e-16\n",
      "loss :  3.0531133177191805e-16\n",
      "loss :  8.881784197001252e-16\n",
      "loss :  6.661338147750939e-16\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  8.326672684688674e-17\n",
      "loss :  2.220446049250313e-16\n",
      "loss :  3.3306690738754696e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  1.3877787807814457e-16\n",
      "loss :  2.498001805406602e-16\n",
      "loss :  1.6653345369377348e-16\n",
      "loss :  6.38378239159465e-16\n",
      "loss :  3.608224830031759e-16\n",
      "loss :  5.273559366969494e-16\n",
      "loss :  6.38378239159465e-16\n",
      "loss :  1.582067810090848e-15\n",
      "loss :  1.942890293094024e-16\n",
      "loss :  7.771561172376096e-16\n",
      "loss :  4.440892098500626e-16\n",
      "loss :  8.604228440844963e-16\n",
      "loss :  1.1379786002407855e-15\n",
      "loss :  1.6653345369377348e-15\n",
      "loss :  9.992007221626409e-16\n",
      "loss :  2.886579864025407e-15\n",
      "loss :  1.5265566588595902e-15\n",
      "loss :  1.1934897514720433e-15\n",
      "loss :  2.1094237467877974e-15\n",
      "loss :  1.609823385706477e-15\n",
      "loss :  1.2212453270876722e-15\n",
      "loss :  2.942091015256665e-15\n",
      "loss :  1.915134717478395e-15\n",
      "loss :  1.3822276656583199e-14\n",
      "loss :  8.326672684688674e-15\n",
      "loss :  4.191091917959966e-15\n",
      "loss :  2.9976021664879227e-15\n",
      "loss :  2.7200464103316335e-15\n",
      "loss :  1.6375789613221059e-15\n",
      "loss :  1.27675647831893e-15\n",
      "loss :  1.582067810090848e-15\n",
      "loss :  2.1371793224034263e-15\n",
      "loss :  7.993605777301127e-15\n",
      "loss :  5.051514762044462e-15\n",
      "loss :  1.6794898805017056e-13\n",
      "loss :  4.2529868515828184e-13\n",
      "loss :  3.3095748364075916e-13\n",
      "loss :  4.338196468722799e-14\n",
      "loss :  9.808820422563258e-14\n",
      "loss :  6.022959908591474e-14\n",
      "loss :  4.6629367034256575e-15\n",
      "loss :  1.5210055437364645e-14\n",
      "loss :  2.5590640717609858e-14\n",
      "loss :  2.0261570199409107e-15\n",
      "loss :  6.716849298982197e-15\n",
      "loss :  7.965850201685498e-15\n",
      "loss :  5.245803791353865e-15\n",
      "loss :  3.4111602431607935e-14\n",
      "loss :  9.880984919163893e-15\n",
      "loss :  3.472222509515177e-14\n",
      "loss :  1.457167719820518e-14\n",
      "loss :  2.040034807748725e-14\n",
      "loss :  9.825473767932635e-15\n",
      "loss :  2.245426067304379e-14\n",
      "loss :  5.304923167415154e-13\n",
      "loss :  3.6454173013567015e-12\n",
      "loss :  5.171640893308904e-12\n",
      "loss :  2.314093361377445e-12\n",
      "loss :  5.131173264061317e-13\n",
      "loss :  3.7980729672426605e-13\n",
      "loss :  2.3037127760972e-13\n",
      "loss :  2.174371793728369e-13\n",
      "loss :  1.264266469291897e-13\n",
      "loss :  7.310818617156656e-14\n",
      "loss :  1.5803192088270634e-12\n",
      "loss :  1.885519518296519e-12\n",
      "loss :  2.1464219290834308e-12\n",
      "loss :  1.3674616994308053e-12\n",
      "loss :  6.829814491737807e-13\n",
      "loss :  2.6678936837498668e-12\n",
      "loss :  2.168848434180859e-12\n",
      "loss :  2.289779477138154e-12\n",
      "loss :  2.2188639814402222e-12\n",
      "loss :  5.17800580190908e-10\n",
      "loss :  7.18697101831367e-10\n",
      "loss :  2.985542646438688e-10\n",
      "loss :  1.1109110298335167e-09\n",
      "loss :  1.836089724704948e-09\n",
      "loss :  6.026946941517508e-09\n",
      "loss :  1.1353240569889067e-09\n",
      "loss :  5.147368087321524e-10\n",
      "loss :  1.2792706449715752e-08\n",
      "loss :  9.983165938365346e-08\n",
      "loss :  1.327155274566394e-07\n",
      "loss :  7.182492822721542e-08\n",
      "loss :  7.80000632971678e-08\n",
      "loss :  4.5131837822509624e-08\n",
      "loss :  2.0300078062973625e-07\n",
      "loss :  7.988815298176632e-08\n",
      "loss :  1.8207786069979193e-06\n",
      "loss :  5.710279538106988e-07\n",
      "loss :  4.4610018790081085e-07\n",
      "loss :  3.8417465475504287e-07\n",
      "loss :  9.199286068906076e-06\n",
      "loss :  1.563863520459563e-06\n",
      "loss :  2.3315067210205598e-06\n",
      "loss :  5.997794687573332e-06\n",
      "loss :  1.771764004843135e-06\n",
      "loss :  1.1736103715520585e-06\n",
      "loss :  2.0452514490898466e-06\n",
      "loss :  1.3406787502390216e-06\n",
      "loss :  9.146550610239501e-07\n",
      "loss :  5.492564696396585e-07\n",
      "loss :  3.4445827168383403e-07\n",
      "loss :  2.922971589214285e-07\n",
      "loss :  6.433149906115432e-07\n",
      "loss :  1.1205791139445864e-07\n",
      "loss :  1.5458813606983313e-07\n",
      "loss :  1.5193724323125934e-07\n",
      "loss :  1.9249662841502868e-07\n",
      "loss :  4.7324267171688916e-08\n",
      "loss :  3.492422351314417e-08\n",
      "loss :  6.310968103662162e-08\n",
      "loss :  1.2335529042672988e-08\n",
      "loss :  7.014259750803831e-08\n",
      "loss :  8.537729172530817e-07\n",
      "loss :  5.555569373427716e-07\n",
      "loss :  2.634084921737667e-06\n",
      "loss :  8.796994279691717e-07\n",
      "loss :  9.281526445192867e-07\n",
      "loss :  1.485232132836245e-06\n",
      "loss :  9.970550308935344e-06\n",
      "loss :  4.055391400470398e-06\n",
      "loss :  2.292998715347494e-06\n",
      "loss :  3.0066873932810267e-06\n",
      "loss :  4.292304765840527e-06\n",
      "loss :  5.179634172236547e-06\n",
      "loss :  3.0419496397371404e-06\n",
      "loss :  1.8663984064914985e-06\n",
      "loss :  7.031493396425503e-07\n",
      "loss :  5.742376743000932e-07\n",
      "loss :  1.9438067511146073e-07\n",
      "loss :  3.9114985384003376e-07\n",
      "loss :  3.8932893176024663e-07\n",
      "loss :  7.114777531569416e-07\n",
      "loss :  2.03628610506712e-06\n",
      "loss :  5.809714821225498e-06\n",
      "loss :  4.692670700023882e-06\n",
      "loss :  2.229487563454313e-06\n",
      "loss :  1.714085783532937e-06\n",
      "loss :  6.616463679165463e-07\n",
      "loss :  6.811350203861366e-07\n",
      "loss :  3.094654346114112e-07\n",
      "loss :  1.216591499542119e-06\n",
      "loss :  3.666585826067603e-07\n",
      "loss :  2.1314147602424782e-07\n",
      "loss :  9.575397541539132e-08\n",
      "loss :  4.1377222714800155e-07\n",
      "loss :  4.632555032912933e-07\n",
      "loss :  6.555646905326284e-07\n",
      "loss :  9.86039367489866e-07\n",
      "loss :  1.1307942259008996e-05\n",
      "loss :  7.158456810429925e-06\n",
      "loss :  4.216791239741724e-06\n",
      "loss :  1.9063184026890667e-06\n",
      "loss :  1.2234512496434036e-06\n",
      "loss :  1.9563599380489904e-06\n",
      "loss :  2.350735258005443e-06\n",
      "loss :  4.330567833221721e-07\n",
      "loss :  3.2406532568529656e-07\n",
      "loss :  5.318510716278979e-07\n",
      "loss :  5.195493031351361e-07\n",
      "loss :  2.9043002314210753e-07\n",
      "loss :  1.2403538107719214e-07\n",
      "loss :  1.4844626150534168e-07\n",
      "loss :  3.800428771683073e-07\n",
      "loss :  2.01862619064741e-07\n",
      "loss :  1.429752671811002e-07\n",
      "loss :  3.7016607734585705e-07\n",
      "loss :  6.624638331231836e-07\n",
      "loss :  6.334516911010724e-07\n",
      "loss :  4.052330382364744e-07\n",
      "loss :  6.921979434082459e-07\n",
      "loss :  8.469252179565956e-07\n",
      "loss :  4.724919335785671e-07\n",
      "loss :  1.795698381101829e-06\n",
      "loss :  6.711080118293467e-07\n",
      "loss :  2.240411504317308e-06\n",
      "loss :  1.629311555007007e-06\n",
      "loss :  3.457561660979991e-06\n",
      "loss :  4.97021801493247e-06\n",
      "loss :  9.263388164981734e-06\n",
      "loss :  4.482957592699677e-06\n",
      "loss :  2.973740265588276e-06\n",
      "loss :  7.54016491555376e-06\n",
      "loss :  6.51219943392789e-06\n",
      "loss :  8.036222993723641e-07\n",
      "loss :  1.948089902725769e-06\n",
      "loss :  2.606823045425699e-06\n",
      "loss :  1.052275592883234e-06\n",
      "loss :  1.5211472828013939e-06\n",
      "loss :  7.60217017159448e-06\n",
      "loss :  5.543635325011564e-06\n",
      "loss :  6.782005129934987e-06\n",
      "loss :  1.4970379197620787e-05\n",
      "loss :  9.735201274452265e-06\n",
      "loss :  6.211149866430787e-06\n",
      "loss :  2.8513329652923858e-06\n",
      "loss :  4.32901106250938e-06\n",
      "loss :  7.771935997880064e-06\n",
      "loss :  7.858277967898175e-06\n",
      "loss :  3.752146312763216e-06\n",
      "loss :  2.0986417439416982e-06\n",
      "loss :  4.370471287984401e-06\n",
      "loss :  5.751457592850784e-06\n",
      "loss :  4.260000423528254e-06\n",
      "loss :  2.0103368569834856e-06\n",
      "loss :  5.333699846232776e-07\n",
      "loss :  3.376215715888975e-07\n",
      "loss :  4.7098613009666224e-08\n",
      "loss :  4.460375535586536e-08\n",
      "loss :  2.2819580891564328e-08\n",
      "loss :  7.517940758816621e-08\n",
      "loss :  3.347346932969231e-08\n",
      "loss :  7.064245721721818e-08\n",
      "loss :  8.479207735945238e-08\n",
      "loss :  2.415814037703967e-07\n",
      "loss :  3.565511121905729e-07\n",
      "loss :  8.736141694498656e-07\n",
      "loss :  1.1227459708607057e-06\n",
      "loss :  3.2365767310693627e-06\n",
      "loss :  1.3289179605635582e-06\n",
      "loss :  6.530901373480447e-06\n",
      "loss :  3.6134668334852904e-05\n",
      "loss :  4.255293606547639e-05\n",
      "loss :  9.452742233406752e-06\n",
      "loss :  9.297019460063893e-06\n",
      "loss :  2.7124791813548654e-05\n",
      "loss :  2.7848409445141442e-05\n",
      "loss :  4.600939973897766e-06\n",
      "loss :  8.485387297696434e-06\n",
      "loss :  6.477037459262647e-06\n",
      "loss :  4.148004791204585e-06\n",
      "loss :  1.8604839624458691e-06\n",
      "loss :  7.535350050602574e-07\n",
      "loss :  4.749612969590089e-07\n",
      "loss :  5.04109266330488e-07\n",
      "loss :  1.4989068120030424e-07\n",
      "loss :  8.891481684258906e-08\n",
      "loss :  6.889965931122788e-08\n",
      "loss :  3.077115451333157e-08\n",
      "loss :  3.82259237596827e-08\n",
      "loss :  2.767342088816349e-08\n",
      "loss :  8.375927507131564e-09\n",
      "loss :  3.751761123993447e-09\n",
      "loss :  3.338306076017261e-09\n",
      "loss :  4.704554967815966e-09\n",
      "loss :  6.45836273349687e-09\n",
      "loss :  3.7193290669534917e-09\n",
      "loss :  1.7704122612371975e-09\n",
      "loss :  3.775833423702579e-09\n",
      "loss :  2.5848323481625357e-09\n",
      "loss :  1.1815447509277988e-09\n",
      "loss :  2.6559285881688766e-09\n",
      "loss :  2.1540844663547887e-09\n",
      "loss :  1.9636932080402403e-09\n",
      "loss :  6.2566862801816114e-09\n",
      "loss :  5.9768550109140506e-09\n",
      "loss :  9.728908345607579e-09\n",
      "loss :  1.9916233995331822e-08\n",
      "loss :  1.3856109148946416e-08\n",
      "loss :  1.7106808414268926e-08\n",
      "loss :  4.235469219793231e-08\n",
      "Complete\n",
      "───────────────────────────────────\n",
      "Heap 1:                 \t (0)\n",
      "───────────────────────────────────\n",
      "Heap 2:                 \t (0)\n",
      "───────────────────────────────────\n",
      "Heap 3:                 \t (0)\n",
      "───────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "losses = []\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset(seed = 10)\n",
    "    env.step([1,5])\n",
    "    env.step([3,4])\n",
    "    heaps, _, _ = env.observe()\n",
    "    state = to_input(heaps)\n",
    "    #print(\"state : \", state.shape)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        #print(\"action : \", action )\n",
    "        is_available = env.check_valid(action)\n",
    "        if not is_available: \n",
    "            #i.e. if the action is not valid, we give the agent a negative reward\n",
    "            #print(\"is not available : \", is_available)\n",
    "            #env.render()\n",
    "            #print(\"action : \", action)\n",
    "            reward = torch.tensor([-1], device=device)\n",
    "            next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            loss = optimize_model()\n",
    "            break\n",
    "            \n",
    "        else : #if the action is valid, we make a step\n",
    "            #print(\"is available : \", is_available)\n",
    "            heaps, done, winner = env.step(action)\n",
    "            #env.render()\n",
    "\n",
    "            if winner == None:\n",
    "                #the game has not ended, we add a reward of 0.\n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else : \n",
    "                #the game has ended and we add a reward of 1. \n",
    "                #print(\"got reward 1\")\n",
    "                reward = torch.tensor([1], device=device)\n",
    "\n",
    "            if not done:\n",
    "                next_state = to_input(heaps)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                #episode_durations.append(t + 1)\n",
    "                #plot_durations()\n",
    "                losses.append(loss)\n",
    "                print(\"loss : \", loss)\n",
    "                break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        #print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play against one player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "───────────────────────────────────\n",
      "Heap 1: |               \t (1)\n",
      "───────────────────────────────────\n",
      "Heap 2: |               \t (1)\n",
      "───────────────────────────────────\n",
      "Heap 3: |               \t (1)\n",
      "───────────────────────────────────\n",
      "to input  tensor([0., 0., 1., 0., 0., 1., 0., 0., 1.])\n",
      "in ql_move, q :  tensor([[-1.3103,  1.3901, -1.4312, -1.3654, -1.3616, -1.3832, -1.3968, -1.3304,\n",
      "         -1.3394, -1.3420, -1.4600, -1.4452, -1.4062, -1.3269, -1.3737, -1.3282,\n",
      "         -1.4324, -1.3706, -1.3822, -1.3991, -1.3575]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Move : tensor([1, 2])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "You cannot take more objects than there are in the heap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000004?line=4'>5</a>\u001b[0m player_DQN \u001b[39m=\u001b[39m DQN_Player(player \u001b[39m=\u001b[39m Turns[\u001b[39m1\u001b[39m], target_net \u001b[39m=\u001b[39m target_net)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000004?line=5'>6</a>\u001b[0m player_Opt \u001b[39m=\u001b[39m OptimalPlayer(epsilon\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, player \u001b[39m=\u001b[39m Turns[\u001b[39m0\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000004?line=6'>7</a>\u001b[0m reward \u001b[39m=\u001b[39m DQN_one_game(player_DQN, player_Opt, env)\n",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 1'\u001b[0m in \u001b[0;36mDQN_one_game\u001b[0;34m(playerDQN, playerOpt, env)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=263'>264</a>\u001b[0m     \u001b[39melse\u001b[39;00m: \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=264'>265</a>\u001b[0m         move, action \u001b[39m=\u001b[39m playerDQN\u001b[39m.\u001b[39mact(heaps)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=265'>266</a>\u001b[0m         heaps, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(move)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=267'>268</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=268'>269</a>\u001b[0m \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39mreward(playerDQN\u001b[39m.\u001b[39mplayer)\n",
      "File \u001b[0;32m~/Documents/GitHub/ANN_NIM/nim_env.py:59\u001b[0m, in \u001b[0;36mNimEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=56'>57</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheap_avail[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mThe selected heap is already empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=57'>58</a>\u001b[0m \u001b[39massert\u001b[39;00m n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou must take at least 1 object from the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=58'>59</a>\u001b[0m \u001b[39massert\u001b[39;00m (n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]), \u001b[39m\"\u001b[39m\u001b[39mYou cannot take more objects than there are in the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n  \u001b[39m# core of the action\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: You cannot take more objects than there are in the heap"
     ]
    }
   ],
   "source": [
    "Turns = np.array([1,0])\n",
    "env.reset(seed = 10)\n",
    "env.step([1,4])\n",
    "env.step([3,3])\n",
    "player_DQN = DQN_Player(player = Turns[1], target_net = target_net)\n",
    "player_Opt = OptimalPlayer(epsilon=1, player = Turns[0])\n",
    "reward = DQN_one_game(player_DQN, player_Opt, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses :  []\n",
      "0\n",
      "loss :  0.09418950229883194\n",
      "500\n",
      "loss :  0.012352041900157928\n",
      "1000\n",
      "loss :  0.009407424367964268\n",
      "1500\n",
      "2000\n",
      "loss :  0.005369816441088915\n",
      "loss :  0.00959593802690506\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "loss :  0.004149775952100754\n",
      "4000\n",
      "loss :  0.003389356890693307\n",
      "4500\n",
      "loss :  0.002350096357986331\n",
      "loss :  0.0025525314267724752\n",
      "5000\n",
      "loss :  0.0017838203348219395\n",
      "loss :  0.0020934275817126036\n",
      "loss :  0.0012633720180019736\n",
      "5500\n",
      "loss :  0.002002154244109988\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "loss :  0.001159275067038834\n",
      "loss :  0.0005984290037304163\n",
      "8000\n",
      "loss :  0.0009978021262213588\n",
      "loss :  0.0008489639731124043\n",
      "8500\n",
      "loss :  0.0005450227181427181\n",
      "9000\n",
      "loss :  0.000949956476688385\n",
      "9500\n",
      "10000\n",
      "loss :  0.0006908607319928706\n",
      "10500\n",
      "loss :  0.0005831870948895812\n",
      "11000\n",
      "loss :  0.0005882562836632133\n",
      "11500\n",
      "loss :  0.000834739999845624\n",
      "12000\n",
      "loss :  0.0005364901735447347\n",
      "loss :  0.0004033526056446135\n",
      "12500\n",
      "loss :  0.0006887504714541137\n",
      "13000\n",
      "loss :  0.0006723542464897037\n",
      "loss :  0.0006765167345292866\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "loss :  0.00043335638474673033\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "loss :  0.0003587591927498579\n",
      "16500\n",
      "loss :  0.0002708180691115558\n",
      "17000\n",
      "loss :  0.0007074534660205245\n",
      "loss :  0.00040250865276902914\n",
      "loss :  0.0005335558671504259\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "loss :  0.0003518717421684414\n",
      "19000\n",
      "loss :  0.0005703429342247546\n",
      "loss :  0.0004064883687533438\n",
      "19500\n",
      "20000\n",
      "Complete\n",
      "───────────────────────────────────\n",
      "Heap 1: |||||||         \t (7)\n",
      "───────────────────────────────────\n",
      "Heap 2:                 \t (0)\n",
      "───────────────────────────────────\n",
      "Heap 3: |               \t (1)\n",
      "───────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10001\n",
    "losses = []\n",
    "print(\"losses : \", losses)\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    heaps, _, _ = env.observe()\n",
    "    state = to_input(heaps)\n",
    "    #print(\"state : \", state.shape)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        is_available = env.check_valid(action)\n",
    "        if not is_available: \n",
    "            #i.e. if the action is not valid, we give the agent a negative reward\n",
    "            #print(\"is not available : \", is_available)\n",
    "            reward = torch.tensor([-1], device=device)\n",
    "            next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            break\n",
    "            \n",
    "        else : #if the action is valid, we make a step\n",
    "            #print(\"is available : \", is_available)\n",
    "            heaps, done, reward = env.step(action)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == None:\n",
    "                #the game has not ended, we add a reward of 0.\n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else : \n",
    "                #the game has ended and we add a reward of 1. \n",
    "                reward = torch.tensor([1], device=device)\n",
    "\n",
    "            if not done:\n",
    "                next_state = to_input(heaps)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                #episode_durations.append(t + 1)\n",
    "                #plot_durations()\n",
    "                losses.append(loss)\n",
    "                print(\"loss : \", loss)\n",
    "                break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09418950229883194, 0.012352041900157928, 0.009407424367964268, 0.005369816441088915, 0.00959593802690506, 0.004149775952100754, 0.003389356890693307, 0.002350096357986331, 0.0025525314267724752, 0.0017838203348219395, 0.0020934275817126036, 0.0012633720180019736, 0.002002154244109988, 0.001159275067038834, 0.0005984290037304163, 0.0009978021262213588, 0.0008489639731124043, 0.0005450227181427181, 0.000949956476688385, 0.0006908607319928706, 0.0005831870948895812, 0.0005882562836632133, 0.000834739999845624, 0.0005364901735447347, 0.0004033526056446135, 0.0006887504714541137, 0.0006723542464897037, 0.0006765167345292866, 0.00043335638474673033, 0.0003587591927498579, 0.0002708180691115558, 0.0007074534660205245, 0.00040250865276902914, 0.0005335558671504259, 0.0003518717421684414, 0.0005703429342247546, 0.0004064883687533438]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(losses)\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You cannot take more objects than there are in the heap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=0'>1</a>\u001b[0m Q1_DQN(target_net, nb_games \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, eps \u001b[39m=\u001b[39;49m \u001b[39m0.2\u001b[39;49m, eps_opt \u001b[39m=\u001b[39;49m \u001b[39m0.5\u001b[39;49m, step \u001b[39m=\u001b[39;49m \u001b[39m250\u001b[39;49m, seed \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, question \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mq3-1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 1'\u001b[0m in \u001b[0;36mQ1_DQN\u001b[0;34m(target_net, nb_games, eps, eps_opt, step, seed, question)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=275'>276</a>\u001b[0m     playerOpt\u001b[39m.\u001b[39mplayer \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=276'>277</a>\u001b[0m     playerDQN\u001b[39m.\u001b[39mplayer \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=278'>279</a>\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m DQN_one_game(playerDQN, playerOpt, env)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=279'>280</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m step \u001b[39m==\u001b[39m step \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=280'>281</a>\u001b[0m     Rewards\u001b[39m.\u001b[39mappend(total_reward \u001b[39m/\u001b[39m step)\n",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 1'\u001b[0m in \u001b[0;36mDQN_one_game\u001b[0;34m(playerDQN, playerOpt, env)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=253'>254</a>\u001b[0m     \u001b[39melse\u001b[39;00m: \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=254'>255</a>\u001b[0m         move, action \u001b[39m=\u001b[39m playerDQN\u001b[39m.\u001b[39mact(heaps)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=255'>256</a>\u001b[0m         heaps, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(move)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=257'>258</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000000?line=258'>259</a>\u001b[0m \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39mreward(playerDQN\u001b[39m.\u001b[39mplayer)\n",
      "File \u001b[0;32m~/Documents/GitHub/ANN_NIM/nim_env.py:59\u001b[0m, in \u001b[0;36mNimEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=56'>57</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheap_avail[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mThe selected heap is already empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=57'>58</a>\u001b[0m \u001b[39massert\u001b[39;00m n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou must take at least 1 object from the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=58'>59</a>\u001b[0m \u001b[39massert\u001b[39;00m (n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]), \u001b[39m\"\u001b[39m\u001b[39mYou cannot take more objects than there are in the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n  \u001b[39m# core of the action\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: You cannot take more objects than there are in the heap"
     ]
    }
   ],
   "source": [
    "Q1_DQN(target_net, nb_games = 1, eps = 0.2, eps_opt = 0.5, step = 250, seed = None, question = 'q3-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aa7871422cbf9296a09eca5272ae12f42feac121f15077f0682f5d4affb8114"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
