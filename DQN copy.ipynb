{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon greedy :  0.5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from nim_env import NimEnv, OptimalPlayer\n",
    "from helpers import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = NimEnv(seed = 3)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------- helpers DQN ----------------------------\n",
    "\n",
    "def to_input(heaps):\n",
    "    #change the format of the heaps so that it can be used as an input for the neural network. \n",
    "    init_state = torch.zeros(9, device = device)\n",
    "    for i in range(3):\n",
    "        state = bin(heaps[i])[2:]\n",
    "        j = 0 \n",
    "        while j < len(state):\n",
    "            init_state[i*3 + 2 - j] = np.int16(state[len(state) - 1 - j])\n",
    "            j += 1\n",
    "    return init_state.clone().detach()\n",
    "            \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 21)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x.to(device))\n",
    "        x = x.view(-1, 9)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DQN_Player(OptimalPlayer):\n",
    "    def __init__(self, epsilon, player, target_net):\n",
    "        super(DQN_Player, self).__init__(epsilon = epsilon, player = player)\n",
    "        self.target_net = target_net\n",
    "        \"\"\"Qvals = {}\n",
    "        for i in range(0, 8):\n",
    "            for j in range(0, 8):\n",
    "                for k in range(0, 8):\n",
    "                    next_actions = {}\n",
    "                    for l in range(i):\n",
    "                        next_actions[str(l) + str(j) + str(k)] = 0\n",
    "                    for l in range(j):\n",
    "                        next_actions[str(i) + str(l) + str(k)] = 0\n",
    "                    for l in range(k):\n",
    "                        next_actions[str(i) + str(j) + str(l)] = 0\n",
    "                    \n",
    "                    string = str(i) + str(j) + str(k)\n",
    "                    Qvals[string] = next_actions\n",
    "        self.qvals = Qvals\"\"\"\n",
    "        \n",
    "    def QL_Move(self, heaps):\n",
    "        \n",
    "        \"\"\"# choose A from state s with eps-greedy policy\n",
    "        current_config = str(heaps[0]) + str(heaps[1]) + str(heaps[2])\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            i, j, k = random.choice(list(self.qvals[current_config]))\n",
    "        else:\n",
    "            #print(self.qvals[current_config])\n",
    "            max_val = max(self.qvals[current_config].values())\n",
    "            max_keys = []\n",
    "            for key, value in self.qvals[current_config].items():\n",
    "                if value == max_val:\n",
    "                    max_keys.append(key)\n",
    "            #print(max_keys)\n",
    "            i, j, k = random.choice(max_keys) # choose at random amongst the best options \n",
    "        \n",
    "        action = [int(i), int(j), int(k)]\n",
    "        #print('action: ', action)\n",
    "        nb_stick, pile_to_take = max(np.array(heaps) - np.array(action)), np.argmax(np.array(heaps) - np.array(action))\n",
    "\n",
    "        move = [pile_to_take + 1, nb_stick]\n",
    "        #print('move: ', move)\"\"\"\n",
    "        state = to_input(heaps)\n",
    "        q = self.target_net(state)\n",
    "        argmax = torch.argmax(q)\n",
    "        move = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "        action = move\n",
    "        return move, action\n",
    "           \n",
    "    def act(self, heaps, **kwargs):\n",
    "        return self.QL_Move(heaps)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "#parameters\n",
    "GAMMA = 0.99\n",
    "buffer_size = 10000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE = 500\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "#Epsilon greedy : \n",
    "EPS_GREEDY = 0.5 #random.random()\n",
    "print(\"epsilon greedy : \", EPS_GREEDY)\n",
    "\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer =optim.Adam(policy_net.parameters(), lr = 5*1e-4)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "memory = ReplayMemory(buffer_size)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    #eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > 1-EPS_GREEDY: #eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            q = policy_net(state)\n",
    "            #print(\"policy predicted : \", q)\n",
    "            argmax = torch.argmax(q)\n",
    "            result = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "            #print(\"select action :\", result)\n",
    "            if  result[0]==0 or result[1] == 0 :\n",
    "                print(\"aussercours dans select action avec q = policy_net(state)\")\n",
    "                print(result)\n",
    "                print(\"policy predicted : \", q)\n",
    "            return result\n",
    "    else:\n",
    "        result = torch.tensor([random.randrange(1,4), random.randrange(1,8)], device=device, dtype=torch.long)\n",
    "        #print(\"if sampel > eps : \", result)\n",
    "        if  result[0]==0 or result[1] == 0 :\n",
    "                print(\"aussercours dans select action avec result = random\")\n",
    "                print(result)\n",
    "        return result\n",
    "\n",
    "\"\"\"def select_action_target(state):\n",
    "    q = target_net(state)\n",
    "    argmax = torch.argmax(q)\n",
    "    result = torch.tensor([argmax.div(7, rounding_mode=\"floor\")+1, torch.remainder(argmax, 7)+1], device = device)\n",
    "    return result\"\"\"\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #print(\"non final mask : \", non_final_mask)\n",
    "    #print(\"non final next state : \", [s for s in batch.next_state if s is not None])\n",
    "    if [s for s in batch.next_state if s is not None] : #is false if the list is empty\n",
    "        #print(\"non empty list\")\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    else : \n",
    "        #print(\"empty list\")\n",
    "        non_final_next_states = torch.empty(1) \n",
    "           \n",
    "    #print(\"non final next states after : \", non_final_next_states)                                      \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    #print(\"state batch :\", state_batch.shape)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch) #64 x 21\n",
    "    #print(\"state action values : \", state_action_values) \n",
    "    #print(\"action \", action_batch)\n",
    "    #print(\"action batch : \", (action_batch[::2]-1+(action_batch[1::2]-1)*3))\n",
    "\n",
    "    state_action_values = state_action_values.gather(1, (action_batch[::2]-1+(action_batch[1::2]-1)*3).view(BATCH_SIZE, 1)) #64 x 1\n",
    "    print(\"state action values after: \", state_action_values.squeeze(1))\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    if len(non_final_next_states) > 1:\n",
    "        #print(\"target net : \")\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch  #64\n",
    "    print(\"expected state action values : \", expected_state_action_values) #64 x 1\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "state action values after:  tensor([-0.0720,  0.0256, -0.1110, -0.0878,  0.0144, -0.0787, -0.0251, -0.1090,\n",
      "        -0.0499, -0.1649,  0.0161, -0.0236, -0.0404,  0.0391,  0.0406, -0.0183,\n",
      "        -0.0207, -0.0366,  0.0558, -0.0050,  0.0451,  0.0199, -0.0401, -0.0157,\n",
      "        -0.0264, -0.1260, -0.0882, -0.1343,  0.0145, -0.0658,  0.0921,  0.0048,\n",
      "        -0.0163,  0.0083, -0.1696, -0.0365, -0.0144,  0.0062, -0.0594, -0.0190,\n",
      "        -0.0636,  0.0055, -0.1041, -0.0720, -0.0213, -0.0718,  0.0171, -0.0590,\n",
      "        -0.0088,  0.0284,  0.0105,  0.0212,  0.0136, -0.0301, -0.0246, -0.0325,\n",
      "         0.0092, -0.0499, -0.0567, -0.0832, -0.0878, -0.0112, -0.0291, -0.1755],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1586,  0.1659, -1.0000,  0.0996, -1.0000, -1.0000,  0.1923,\n",
      "         0.1637,  0.1141,  0.1064, -1.0000, -1.0000, -1.0000,  0.0598,  0.1777,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1484, -1.0000,  0.1108,  0.1656,  0.1596,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1046,  0.1321, -1.0000, -1.0000,  0.1068,\n",
      "         0.1515, -1.0000, -1.0000,  0.1115,  0.1622, -1.0000, -1.0000,  0.1184,\n",
      "         0.1512,  0.1226,  0.1699, -1.0000, -1.0000,  0.1484, -1.0000, -1.0000,\n",
      "         0.1060, -1.0000,  0.1307,  0.0980,  0.1214,  0.1090, -1.0000,  0.0939,\n",
      "         0.1073, -1.0000, -1.0000, -1.0000, -1.0000,  0.1323,  0.1098,  0.1295])\n",
      "state action values after:  tensor([-0.1227, -0.0372, -0.0360, -0.0398, -0.0424, -0.0925, -0.0082, -0.1547,\n",
      "        -0.0258,  0.0064, -0.1784,  0.0146,  0.0293,  0.0319, -0.0742, -0.1241,\n",
      "         0.0031, -0.1008, -0.0415, -0.0339, -0.0266, -0.1212, -0.0661,  0.0276,\n",
      "        -0.0464, -0.1206, -0.0405,  0.0141,  0.0017, -0.0441, -0.1091, -0.0043,\n",
      "         0.0363,  0.0835, -0.1091, -0.0617, -0.0925, -0.0312,  0.0086, -0.0921,\n",
      "         0.0343, -0.1623, -0.0718, -0.0814, -0.0022, -0.0234, -0.0276, -0.0199,\n",
      "        -0.0139, -0.0425,  0.0036, -0.0344, -0.0039, -0.0807, -0.0773, -0.0508,\n",
      "        -0.1364, -0.0248,  0.0015, -0.0599,  0.0031, -0.0922, -0.0740, -0.0664],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1659,  0.0939, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1141,\n",
      "         0.1184,  0.0980, -1.0000,  0.1321,  0.1586, -1.0000, -1.0000,  0.1923,\n",
      "         0.1068, -1.0000,  0.1098,  0.1090,  0.1515,  0.1699, -1.0000,  0.0598,\n",
      "         0.1115,  0.1046, -1.0000,  0.1108, -1.0000,  0.1316, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1622,  0.1064, -1.0000,\n",
      "        -1.0000,  0.1295,  0.1512, -1.0000,  0.1226,  0.1596,  0.1777,  0.1060,\n",
      "         0.1323, -1.0000,  0.0996, -1.0000,  0.1307, -1.0000, -1.0000,  0.1656,\n",
      "        -1.0000, -1.0000,  0.1073, -1.0000,  0.1214, -1.0000,  0.1484,  0.1637])\n",
      "state action values after:  tensor([-0.0061, -0.0954, -0.0554, -0.0186, -0.0817,  0.0336, -0.0318, -0.0579,\n",
      "        -0.0492,  0.0750, -0.0898, -0.0568, -0.1049, -0.0608, -0.0610, -0.0778,\n",
      "        -0.0298,  0.0019, -0.0086, -0.0094, -0.0353, -0.1291, -0.0841, -0.0057,\n",
      "        -0.1879,  0.0148, -0.0811, -0.1487, -0.0559,  0.0011, -0.0790, -0.1436,\n",
      "         0.0287,  0.0253, -0.0093, -0.0322, -0.0564, -0.0392, -0.1394,  0.0146,\n",
      "        -0.0471, -0.0957, -0.0216,  0.0130, -0.1123, -0.0481,  0.0085, -0.1398,\n",
      "        -0.0612, -0.1291, -0.0821, -0.1070, -0.1121, -0.0457, -0.0139, -0.0405,\n",
      "        -0.1334, -0.1039, -0.0383, -0.1478, -0.0559, -0.1008, -0.0363, -0.0164],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.0996, -1.0000, -1.0000,  0.1307, -1.0000,  0.1586,  0.1596, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1656,  0.1484,\n",
      "         0.1060,  0.1064,  0.1484,  0.1226,  0.1777, -1.0000, -1.0000,  0.1073,\n",
      "        -1.0000,  0.0598,  0.1637,  0.1295,  0.1115,  0.1068,  0.1512,  0.1141,\n",
      "        -1.0000, -1.0000,  0.0980,  0.1184,  0.1316,  0.1090,  0.1923,  0.1321,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1622,  0.1108,  0.1699,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1046, -1.0000, -1.0000, -1.0000,  0.0939,\n",
      "         0.1659, -1.0000, -1.0000, -1.0000,  0.1115, -1.0000,  0.1515,  0.1323])\n",
      "state action values after:  tensor([-0.0005, -0.0102,  0.0035, -0.0859, -0.0149, -0.0411, -0.0180, -0.0570,\n",
      "        -0.1097, -0.1023,  0.0217, -0.1026, -0.0432, -0.1266, -0.1128, -0.0249,\n",
      "        -0.0454, -0.1067,  0.0366, -0.0386, -0.0554, -0.0712, -0.1346, -0.0641,\n",
      "        -0.0463, -0.1482, -0.0822, -0.0411, -0.0726, -0.0283, -0.0382, -0.0884,\n",
      "        -0.0686, -0.1521, -0.0044, -0.0144, -0.0652, -0.0945, -0.1306, -0.0129,\n",
      "        -0.0710, -0.0075, -0.0647, -0.1096, -0.1991, -0.0813,  0.0655,  0.0133,\n",
      "        -0.0736, -0.0651, -0.1231, -0.0342, -0.0651, -0.1482, -0.1051, -0.1306,\n",
      "        -0.0816, -0.1316,  0.0021, -0.1169, -0.1572, -0.1606,  0.0861, -0.0183],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1068,  0.1484,  0.1108,  0.1512,  0.1214,  0.1596,  0.1226, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.0939, -1.0000, -1.0000,  0.0980,\n",
      "         0.1090, -1.0000,  0.1586,  0.1060, -1.0000,  0.1656,  0.1295, -1.0000,\n",
      "         0.1515, -1.0000,  0.1484,  0.1777, -1.0000, -1.0000,  0.1184, -1.0000,\n",
      "         0.1316,  0.1923,  0.1064,  0.0996,  0.1622,  0.1637, -1.0000,  0.1073,\n",
      "        -1.0000, -1.0000,  0.1098, -1.0000, -1.0000, -1.0000, -1.0000,  0.1321,\n",
      "        -1.0000,  0.1115, -1.0000, -1.0000,  0.1115, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1141,  0.0598, -1.0000,  0.1699, -1.0000,  0.1606,  0.1323])\n",
      "state action values after:  tensor([-0.0867, -0.0509, -0.2127, -0.0097, -0.0475, -0.0416, -0.1672, -0.0543,\n",
      "         0.0126, -0.0464, -0.0828, -0.0267, -0.0409, -0.1743, -0.0108, -0.0881,\n",
      "        -0.0888, -0.0787, -0.0950, -0.0195, -0.0470, -0.1067, -0.1682, -0.1496,\n",
      "        -0.1682, -0.0767, -0.1200, -0.0735, -0.1404,  0.0386, -0.0005, -0.1201,\n",
      "         0.0149, -0.1550, -0.0423, -0.1280, -0.1329,  0.0153, -0.1194, -0.0227,\n",
      "        -0.1501, -0.1172, -0.1092, -0.1215, -0.0214, -0.1305, -0.1753,  0.0551,\n",
      "        -0.0271, -0.1017, -0.0692, -0.1248, -0.0455, -0.0805, -0.0862, -0.0735,\n",
      "        -0.1224, -0.1501, -0.0106, -0.0024, -0.0524, -0.0820, -0.0787, -0.1196],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1596, -1.0000,  0.1484,  0.1307, -1.0000,  0.1923,  0.1515,\n",
      "         0.1321,  0.1060,  0.1656,  0.1226,  0.0980, -1.0000,  0.1064,  0.1484,\n",
      "        -1.0000, -1.0000,  0.1512,  0.1073,  0.1777,  0.1637, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1098,  0.1295,  0.1115, -1.0000,  0.1586,  0.1068, -1.0000,\n",
      "        -1.0000,  0.1659,  0.1184, -1.0000, -1.0000, -1.0000,  0.1141,  0.1214,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.0996, -1.0000,  0.1699, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1316, -1.0000,  0.1115,\n",
      "        -1.0000, -1.0000,  0.0598,  0.1108,  0.1090,  0.1622,  0.1046, -1.0000])\n",
      "state action values after:  tensor([-0.1374, -0.1877, -0.0168, -0.2173,  0.0112, -0.0445, -0.1508, -0.1372,\n",
      "        -0.1745, -0.1482, -0.1436,  0.0444, -0.1710, -0.1262, -0.0941, -0.1389,\n",
      "         0.0971, -0.0464, -0.0799, -0.0960, -0.1488, -0.1710, -0.0346, -0.1695,\n",
      "        -0.0591, -0.0883, -0.1195, -0.1753,  0.0426,  0.0070, -0.1896, -0.0989,\n",
      "        -0.0583, -0.0555, -0.0078, -0.2286, -0.0616, -0.0740, -0.1208, -0.1363,\n",
      "        -0.1869, -0.1408, -0.1071, -0.0972, -0.0994, -0.0994, -0.0601, -0.0107,\n",
      "        -0.0264, -0.0525, -0.0544,  0.0133, -0.1048, -0.0956, -0.0292,  0.0016,\n",
      "        -0.1167, -0.0455, -0.0204, -0.1070, -0.0619, -0.0182, -0.1951, -0.0508],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.1064,  0.1088, -1.0000,  0.1184, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1473, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1606,  0.0939,  0.1115,  0.1484, -1.0000, -1.0000,  0.1226,  0.1659,\n",
      "         0.1307,  0.1098, -1.0000, -1.0000,  0.1586, -1.0000, -1.0000, -1.0000,\n",
      "         0.0980, -1.0000,  0.1108, -1.0000,  0.1090, -1.0000, -1.0000, -1.0000,\n",
      "         0.1923, -1.0000,  0.1512,  0.1656, -1.0000, -1.0000,  0.1515,  0.1484,\n",
      "         0.0996,  0.1060, -1.0000,  0.1321,  0.1295,  0.1622,  0.1214,  0.1068,\n",
      "         0.1637, -1.0000,  0.0598,  0.1141,  0.1596,  0.1323,  0.1699,  0.1777])\n",
      "state action values after:  tensor([-0.1965, -0.0120,  0.0060, -0.0013, -0.1479, -0.2460, -0.1820, -0.0415,\n",
      "        -0.0470, -0.0293, -0.0317, -0.0122, -0.0997, -0.1539, -0.1027, -0.1123,\n",
      "        -0.1500, -0.2067, -0.1097, -0.0660, -0.0674, -0.1364, -0.1096,  0.0470,\n",
      "        -0.0837, -0.0887, -0.1108, -0.1397, -0.1915, -0.2029, -0.0872, -0.1261,\n",
      "         0.1034, -0.1682, -0.0146, -0.1786, -0.0355, -0.1669, -0.2100, -0.0226,\n",
      "        -0.0744, -0.0946, -0.0657, -0.1976, -0.1180, -0.2246, -0.0477, -0.0946,\n",
      "        -0.1379, -0.0687, -0.1915, -0.0655, -0.1117, -0.1565, -0.0497,  0.0045,\n",
      "        -0.0310, -0.0710, -0.2144, -0.1832, -0.1466, -0.1086,  0.0147, -0.1789],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1226,\n",
      "         0.0939,  0.0598,  0.0996,  0.1484,  0.1354, -1.0000,  0.1484, -1.0000,\n",
      "        -1.0000,  0.1923, -1.0000, -1.0000, -1.0000, -1.0000,  0.1656,  0.1586,\n",
      "        -1.0000,  0.1295, -1.0000, -1.0000, -1.0000, -1.0000,  0.1115,  0.1637,\n",
      "         0.1606, -1.0000,  0.1108, -1.0000,  0.1214,  0.1473, -1.0000,  0.1064,\n",
      "         0.0980,  0.1141,  0.1515, -1.0000,  0.1512,  0.1088,  0.1184, -1.0000,\n",
      "        -1.0000,  0.1090, -1.0000, -1.0000, -1.0000, -1.0000,  0.1046,  0.1068,\n",
      "         0.1073,  0.1307,  0.1699,  0.1659, -1.0000,  0.1622,  0.1321, -1.0000])\n",
      "state action values after:  tensor([-0.0358, -0.1126, -0.0404, -0.0472, -0.1688, -0.0698, -0.0559, -0.1086,\n",
      "        -0.2199, -0.1248, -0.1280, -0.0821, -0.2123, -0.2315, -0.2132, -0.1111,\n",
      "         0.1337, -0.2355, -0.0726, -0.0111, -0.0356, -0.1890, -0.0759,  0.0524,\n",
      "        -0.0366, -0.2312, -0.0910, -0.1531,  0.0233, -0.0942, -0.0811,  0.0086,\n",
      "        -0.1575, -0.0630, -0.1804, -0.2037, -0.0269, -0.1267, -0.0791, -0.1431,\n",
      "        -0.0809, -0.0487,  0.0169, -0.1994, -0.2643, -0.0846, -0.0942, -0.1950,\n",
      "        -0.0221, -0.2315, -0.1098, -0.0007, -0.1203, -0.2292, -0.2208, -0.1224,\n",
      "        -0.0149, -0.1552, -0.1631, -0.0827, -0.1309, -0.2019, -0.1338, -0.0493],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.0598,  0.1098,  0.1214,  0.0939, -1.0000,  0.1515,  0.1777,  0.1316,\n",
      "        -1.0000,  0.1656, -1.0000,  0.1596, -1.0000, -1.0000, -1.0000,  0.1484,\n",
      "         0.1380,  0.1699,  0.1295, -1.0000,  0.0996, -1.0000, -1.0000,  0.1586,\n",
      "         0.1073,  0.1088,  0.0980, -1.0000, -1.0000,  0.1115,  0.1307,  0.1068,\n",
      "        -1.0000,  0.1060, -1.0000, -1.0000,  0.1064, -1.0000,  0.1090, -1.0000,\n",
      "        -1.0000,  0.1226,  0.1321,  0.1659, -1.0000, -1.0000,  0.1115, -1.0000,\n",
      "         0.1323, -1.0000,  0.1354, -1.0000,  0.1622,  0.1923, -1.0000, -1.0000,\n",
      "         0.1484, -1.0000, -1.0000,  0.1141, -1.0000, -1.0000,  0.1637,  0.1184])\n",
      "state action values after:  tensor([-0.0839, -0.1584,  0.0599, -0.1141,  0.0508, -0.1820, -0.0386,  0.0132,\n",
      "        -0.2234, -0.1436, -0.1616, -0.2348, -0.2334, -0.0949, -0.2298, -0.0987,\n",
      "        -0.2172, -0.2195, -0.0574, -0.0453,  0.0196, -0.2273, -0.1942, -0.0948,\n",
      "        -0.1736, -0.2548, -0.1310, -0.1165, -0.1190, -0.1868, -0.1256, -0.1279,\n",
      "         0.1395, -0.1584, -0.2588, -0.1223, -0.1306, -0.2346, -0.2550, -0.2119,\n",
      "        -0.1468, -0.1474, -0.1715, -0.1987, -0.0556, -0.0246, -0.1006, -0.1066,\n",
      "        -0.2505, -0.2548, -0.1251, -0.0546, -0.0713, -0.0654, -0.0981, -0.2831,\n",
      "        -0.1836, -0.0390,  0.1168, -0.2447, -0.0884, -0.0223, -0.2334, -0.0254],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.1586,  0.1316,  0.1777, -1.0000,  0.0598,  0.1068,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1596, -1.0000,  0.1115,\n",
      "        -1.0000,  0.1659,  0.1295,  0.0939,  0.1321, -1.0000,  0.1473, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1354, -1.0000, -1.0000, -1.0000,  0.1622,\n",
      "         0.1380, -1.0000,  0.1699, -1.0000, -1.0000,  0.1088,  0.1923, -1.0000,\n",
      "         0.1512, -1.0000, -1.0000, -1.0000,  0.1777,  0.1323, -1.0000,  0.0980,\n",
      "        -1.0000, -1.0000,  0.1098,  0.1226,  0.1515,  0.1060, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1073,  0.1606, -1.0000,  0.1307,  0.1046, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.1102, -0.0912, -0.1191, -0.0422, -0.0201, -0.0432, -0.1350, -0.1440,\n",
      "        -0.2749, -0.1093,  0.1442, -0.2556, -0.1220, -0.1364, -0.2500, -0.0947,\n",
      "        -0.1371, -0.0143, -0.0452, -0.1439, -0.2610, -0.2416,  0.0187, -0.1883,\n",
      "        -0.1670, -0.2814,  0.0558, -0.3019, -0.1754, -0.0602, -0.1351,  0.1239,\n",
      "        -0.1243, -0.2369, -0.0736, -0.0348, -0.1095, -0.2818, -0.1096, -0.2168,\n",
      "        -0.0363, -0.1621, -0.1606, -0.2371,  0.0237, -0.2530, -0.0409, -0.2842,\n",
      "        -0.1685, -0.1650, -0.1890, -0.0919, -0.2532, -0.1648, -0.2794, -0.0326,\n",
      "        -0.1039, -0.1402, -0.0377, -0.2397, -0.1374, -0.1213, -0.1968, -0.1039],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1596, -1.0000,  0.1316,  0.1295,  0.1484,  0.0939,  0.1622, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1380, -1.0000,  0.1354,  0.1484, -1.0000,  0.1307,\n",
      "        -1.0000, -1.0000,  0.1214,  0.1637, -1.0000,  0.1659,  0.1068, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1777, -1.0000, -1.0000,  0.1226, -1.0000,  0.1606,\n",
      "        -1.0000, -1.0000,  0.1515, -1.0000, -1.0000,  0.1923,  0.1090, -1.0000,\n",
      "         0.1064, -1.0000,  0.1656,  0.1088,  0.1321, -1.0000,  0.0598,  0.1699,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1884, -1.0000,  0.1512, -1.0000, -1.0000,\n",
      "         0.1115, -1.0000,  0.1108, -1.0000,  0.1098,  0.0980, -1.0000,  0.1115])\n",
      "state action values after:  tensor([-0.2353, -0.0407, -0.1084, -0.1497, -0.2709, -0.3047, -0.2628,  0.0019,\n",
      "        -0.2702, -0.1404,  0.0771, -0.0432, -0.2772, -0.2370, -0.1422,  0.0244,\n",
      "         0.0622, -0.3084, -0.0428,  0.0297, -0.0915, -0.0293, -0.2026, -0.3035,\n",
      "        -0.2772, -0.2142, -0.1235, -0.1265, -0.0222, -0.1270, -0.3116, -0.1999,\n",
      "        -0.1840, -0.1353, -0.0662, -0.1817, -0.0698, -0.3223, -0.1676, -0.0545,\n",
      "        -0.0480, -0.0461, -0.0371, -0.1251, -0.1752, -0.1498, -0.1799, -0.1084,\n",
      "        -0.0998, -0.0468, -0.1508, -0.0435, -0.2682, -0.1247, -0.1493, -0.1779,\n",
      "        -0.0216, -0.2193, -0.1530, -0.0983, -0.3079, -0.0759, -0.3035, -0.1627],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.0939,  0.1115,  0.1098, -1.0000, -1.0000, -1.0000,  0.1046,\n",
      "        -1.0000,  0.1622,  0.1586,  0.1073, -1.0000,  0.1088, -1.0000,  0.1068,\n",
      "         0.1777,  0.1699,  0.1064,  0.1321,  0.1884,  0.1323, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1473,  0.1316,  0.1354,  0.1484,  0.1090, -1.0000, -1.0000,\n",
      "        -1.0000,  0.0980,  0.1226,  0.1512,  0.1060, -1.0000, -1.0000,  0.1777,\n",
      "         0.1214, -1.0000,  0.0996,  0.1596, -1.0000, -1.0000,  0.1656,  0.1115,\n",
      "         0.1307,  0.1184,  0.1484, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1923,  0.1515, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.1653, -0.1962, -0.1005, -0.0489,  0.0306, -0.3216,  0.0135, -0.1601,\n",
      "        -0.1998, -0.2882, -0.2999, -0.1662, -0.1423, -0.1805, -0.2055, -0.3066,\n",
      "        -0.0750, -0.1451, -0.0400, -0.1115, -0.0125, -0.2185, -0.1723, -0.2999,\n",
      "        -0.3337,  0.0369, -0.2051, -0.1789, -0.0347, -0.0437, -0.0430, -0.0695,\n",
      "        -0.3337, -0.2900, -0.1567, -0.2369, -0.2188, -0.1504, -0.1450, -0.0509,\n",
      "        -0.0552, -0.3435, -0.0893, -0.3118, -0.1724, -0.0304, -0.1627, -0.1995,\n",
      "        -0.1250, -0.3284, -0.1509, -0.1410, -0.2179, -0.3284, -0.0486, -0.0364,\n",
      "        -0.3425, -0.2124,  0.1403,  0.1466, -0.0729, -0.2123, -0.1115, -0.2363],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.1307,  0.1064,  0.1068, -1.0000,  0.1046,  0.1098,\n",
      "         0.1512, -1.0000, -1.0000,  0.1484,  0.1622, -1.0000, -1.0000, -1.0000,\n",
      "         0.1515,  0.1090,  0.1141,  0.1115, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1321, -1.0000, -1.0000,  0.0996,  0.1184,  0.1073,  0.1060,\n",
      "         0.1699, -1.0000, -1.0000,  0.1088,  0.1473,  0.0980, -1.0000,  0.1777,\n",
      "        -1.0000, -1.0000,  0.1884, -1.0000, -1.0000, -1.0000, -1.0000,  0.1656,\n",
      "         0.1316, -1.0000,  0.1637,  0.1596, -1.0000, -1.0000,  0.1214,  0.0939,\n",
      "        -1.0000, -1.0000,  0.1606,  0.1380,  0.1226, -1.0000,  0.1115, -1.0000])\n",
      "state action values after:  tensor([-0.3761, -0.1842, -0.1658,  0.0007, -0.1839, -0.1702, -0.1554, -0.0652,\n",
      "        -0.2342, -0.0407, -0.2152, -0.2205, -0.3144, -0.2270, -0.3071, -0.0546,\n",
      "        -0.0392, -0.2374, -0.3202, -0.0975, -0.3196, -0.0663, -0.2686, -0.1451,\n",
      "         0.0760, -0.1357, -0.1878, -0.3374, -0.0282, -0.1660,  0.1465, -0.2036,\n",
      "        -0.3556, -0.0440, -0.2372, -0.3664,  0.0956, -0.0710, -0.3497, -0.2539,\n",
      "        -0.2142,  0.0262, -0.3349, -0.0308, -0.3045, -0.0786, -0.1247, -0.3656,\n",
      "        -0.1130, -0.3624, -0.2443, -0.1803, -0.3162, -0.1803, -0.1225,  0.1483,\n",
      "        -0.1586, -0.2332, -0.0458, -0.3236, -0.2675, -0.0704, -0.1509, -0.0673],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.0980,  0.1295,  0.1484,  0.1098, -1.0000,  0.1108,\n",
      "        -1.0000,  0.1073, -1.0000,  0.1512,  0.1659, -1.0000, -1.0000,  0.1064,\n",
      "         0.1323,  0.1550, -1.0000,  0.1307, -1.0000,  0.1060, -1.0000, -1.0000,\n",
      "         0.1777, -1.0000, -1.0000, -1.0000,  0.1484,  0.1090,  0.1380, -1.0000,\n",
      "        -1.0000,  0.1777,  0.1088,  0.1923,  0.1586, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1046, -1.0000,  0.1141, -1.0000,  0.1226,  0.1354, -1.0000,\n",
      "         0.1115,  0.1699, -1.0000, -1.0000, -1.0000, -1.0000,  0.1316,  0.1606,\n",
      "         0.1596, -1.0000,  0.1214, -1.0000, -1.0000,  0.1515,  0.1637, -1.0000])\n",
      "state action values after:  tensor([-0.2173, -0.3771, -0.1815, -0.1142, -0.1461, -0.2177, -0.2478, -0.2425,\n",
      "        -0.1992, -0.0816, -0.3476, -0.0238, -0.2608, -0.1855,  0.0141, -0.1079,\n",
      "        -0.2356,  0.0836, -0.0365, -0.3398, -0.2241, -0.3415, -0.2533, -0.4092,\n",
      "        -0.1750, -0.1225, -0.2490, -0.1142, -0.0857, -0.3829, -0.2865, -0.2922,\n",
      "        -0.2158, -0.1825, -0.0382, -0.2340,  0.1443, -0.3920, -0.0946, -0.2395,\n",
      "        -0.2392, -0.0622, -0.3630,  0.1324, -0.3276, -0.3905, -0.2511, -0.0422,\n",
      "        -0.1960, -0.0227,  0.1572, -0.0238, -0.0853, -0.3955, -0.1983, -0.1824,\n",
      "        -0.2486, -0.0314, -0.0739, -0.2009,  0.0522, -0.3480, -0.3466, -0.1880],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000, -1.0000,  0.1115, -1.0000, -1.0000, -1.0000,  0.1656,\n",
      "        -1.0000,  0.1884, -1.0000,  0.0996, -1.0000,  0.1090,  0.1295, -1.0000,\n",
      "         0.1088,  0.1777,  0.0598,  0.1659, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1596,  0.1354, -1.0000,  0.1115, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1473,  0.1098,  0.1777, -1.0000,  0.1380, -1.0000,  0.1307,  0.1512,\n",
      "        -1.0000,  0.1064, -1.0000,  0.1562, -1.0000,  0.1699, -1.0000,  0.1214,\n",
      "        -1.0000,  0.1141,  0.1606,  0.0939,  0.1226,  0.1923, -1.0000,  0.0980,\n",
      "        -1.0000,  0.1184,  0.1108,  0.1484,  0.1321, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.3653, -0.1920, -0.0328, -0.1705, -0.2745, -0.3072, -0.4133, -0.2783,\n",
      "        -0.3531, -0.1024, -0.2127, -0.3917,  0.0499,  0.1413, -0.0258, -0.4425,\n",
      "        -0.0780, -0.0359, -0.1999, -0.3461, -0.3773,  0.0290, -0.2447, -0.0075,\n",
      "        -0.1146, -0.1356, -0.1481, -0.2176, -0.3892, -0.4102, -0.1146, -0.1542,\n",
      "        -0.2192, -0.1843, -0.0573,  0.0918, -0.0331, -0.3162, -0.1916,  0.1665,\n",
      "        -0.0145, -0.4048, -0.2780,  0.1404, -0.2684, -0.1958, -0.0136, -0.0698,\n",
      "        -0.2406, -0.1180, -0.0178, -0.2626, -0.2716, -0.1864, -0.2702, -0.2218,\n",
      "        -0.3720, -0.0922,  0.0514, -0.2139, -0.3667, -0.2046, -0.0940, -0.2902],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1659, -1.0000,  0.1777, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1068,  0.1562,  0.1184, -1.0000,\n",
      "         0.1884,  0.1073,  0.0980, -1.0000, -1.0000,  0.1295, -1.0000,  0.1503,\n",
      "         0.1115,  0.1622, -1.0000,  0.1484, -1.0000, -1.0000,  0.1115, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1060,  0.1777,  0.0598, -1.0000,  0.1596,  0.1606,\n",
      "         0.1022, -1.0000,  0.1550,  0.1380, -1.0000,  0.1098,  0.1141,  0.1064,\n",
      "        -1.0000,  0.1316,  0.0996, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1307,  0.1046,  0.1473, -1.0000,  0.1090, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.0516,  0.1508, -0.4589, -0.3005, -0.2097,  0.0051, -0.0600, -0.3997,\n",
      "        -0.4390, -0.2852, -0.3410, -0.0123, -0.4153, -0.3934, -0.0073, -0.1496,\n",
      "        -0.1338, -0.0199,  0.1007, -0.2589,  0.1766,  0.0356, -0.2936, -0.0433,\n",
      "        -0.0888, -0.0340, -0.2145, -0.0992,  0.1363, -0.2973, -0.4782, -0.2969,\n",
      "        -0.0034, -0.3974, -0.3666, -0.1164, -0.0382, -0.2671, -0.2966, -0.2796,\n",
      "         0.0436, -0.1920,  0.0647, -0.2670,  0.1683,  0.0701, -0.3085, -0.2407,\n",
      "        -0.1203, -0.4390, -0.2769, -0.1957, -0.0515, -0.4346, -0.0743, -0.1079,\n",
      "        -0.2268, -0.2204, -0.0781, -0.0272, -0.2346, -0.0943, -0.1141, -0.0283],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1323,  0.1562,  0.1923,  0.1550,  0.1098,  0.1503, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.0939, -1.0000, -1.0000,  0.1022, -1.0000,\n",
      "         0.1622,  0.1184,  0.1777, -1.0000,  0.1606,  0.1064, -1.0000, -1.0000,\n",
      "         0.1307,  0.1073,  0.0980,  0.1226,  0.1380, -1.0000, -1.0000, -1.0000,\n",
      "         0.1141, -1.0000, -1.0000,  0.1354,  0.1484, -1.0000, -1.0000,  0.1512,\n",
      "         0.1295, -1.0000,  0.1046, -1.0000,  0.1623,  0.1321, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1060, -1.0000,  0.1884, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1064,  0.1777,  0.1088,  0.1108,  0.1115,  0.0598])\n",
      "state action values after:  tensor([-0.3370, -0.4935, -0.3023, -0.1124, -0.4100, -0.4664, -0.2146, -0.0126,\n",
      "        -0.0064, -0.2947, -0.4240, -0.4219, -0.3508, -0.3441,  0.0637, -0.1396,\n",
      "        -0.4687, -0.0452, -0.3274,  0.1338, -0.2345, -0.4432,  0.1863, -0.5156,\n",
      "        -0.0285, -0.2231, -0.4650, -0.2847, -0.4687, -0.3022, -0.0524, -0.4209,\n",
      "        -0.1777, -0.2800, -0.1048, -0.4422,  0.0015, -0.2707, -0.1538, -0.4255,\n",
      "        -0.2550,  0.0481, -0.1068, -0.0496, -0.3183, -0.2283, -0.2718, -0.1131,\n",
      "        -0.0562, -0.0225, -0.2065, -0.0704, -0.2490, -0.1988,  0.1317, -0.2619,\n",
      "         0.0191, -0.0839, -0.2906,  0.1095, -0.2313,  0.0077,  0.0799, -0.4518],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.2057,  0.1923,  0.1512, -1.0000, -1.0000, -1.0000, -1.0000,  0.1184,\n",
      "         0.0939, -1.0000, -1.0000,  0.1659, -1.0000, -1.0000,  0.1068, -1.0000,\n",
      "        -1.0000,  0.1060, -1.0000,  0.1586,  0.1088, -1.0000,  0.1606, -1.0000,\n",
      "         0.1214,  0.1098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1108, -1.0000,  0.1022, -1.0000,  0.1637, -1.0000,\n",
      "         0.1484,  0.1064,  0.1226,  0.1515, -1.0000,  0.0980, -1.0000,  0.1115,\n",
      "         0.1323,  0.0598,  0.1473,  0.1884,  0.1090, -1.0000,  0.1380, -1.0000,\n",
      "         0.1503,  0.1307, -1.0000,  0.1777, -1.0000,  0.1141,  0.1321, -1.0000])\n",
      "state action values after:  tensor([-0.3017, -0.3494, -0.0008,  0.0200, -0.2846, -0.2698, -0.4832, -0.3744,\n",
      "        -0.1532, -0.4507,  0.0911, -0.0786, -0.1147, -0.0630, -0.4108, -0.0434,\n",
      "        -0.3150,  0.1186, -0.3480,  0.0113, -0.3397, -0.3477, -0.2440, -0.0655,\n",
      "        -0.2009, -0.4530, -0.4991, -0.3255, -0.4772, -0.1059, -0.4957,  0.1445,\n",
      "        -0.4505,  0.1952, -0.0298,  0.1635, -0.3068, -0.2979, -0.0385,  0.0705,\n",
      "         0.0936, -0.4699,  0.0765, -0.1504, -0.1123, -0.3663,  0.0040, -0.3231,\n",
      "        -0.2393, -0.4484, -0.3195, -0.2027, -0.1604, -0.1374, -0.4942, -0.3435,\n",
      "        -0.4507, -0.2426,  0.1285, -0.0709, -0.4413, -0.1135,  0.0372, -0.2950],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.0939,  0.1141, -1.0000,  0.1090, -1.0000, -1.0000,\n",
      "         0.1637, -1.0000,  0.1321,  0.1307,  0.1226, -1.0000, -1.0000,  0.1515,\n",
      "        -1.0000,  0.1777,  0.1550,  0.1022,  0.1656, -1.0000,  0.0980,  0.1884,\n",
      "         0.1473, -1.0000, -1.0000,  0.1512, -1.0000,  0.1316, -1.0000,  0.1586,\n",
      "         0.1659,  0.1606,  0.1073,  0.1623, -1.0000, -1.0000,  0.1060,  0.1068,\n",
      "         0.1046, -1.0000,  0.1295, -1.0000,  0.1115, -1.0000,  0.0996, -1.0000,\n",
      "         0.1098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1380, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.0721, -0.2543, -0.1988, -0.0111, -0.1477, -0.1206, -0.5154, -0.1210,\n",
      "        -0.3688, -0.5619, -0.2732, -0.5457, -0.2619, -0.3472, -0.2923,  0.1299,\n",
      "         0.1083, -0.2048, -0.4987,  0.0222, -0.5933, -0.3909, -0.1216, -0.0980,\n",
      "        -0.3400,  0.0495, -0.3490, -0.1827,  0.0728, -0.2094, -0.4218,  0.0331,\n",
      "        -0.3210, -0.5304,  0.2039, -0.5304, -0.0307,  0.1033, -0.1794, -0.0758,\n",
      "        -0.4747, -0.1101,  0.0061, -0.4800, -0.5442, -0.1139, -0.3673, -0.0967,\n",
      "        -0.1505,  0.0777,  0.1272, -0.5275, -0.1932, -0.2439,  0.0124, -0.0590,\n",
      "        -0.0355, -0.2312, -0.3657, -0.3114, -0.2912,  0.1805, -0.2976,  0.1273],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1307, -1.0000, -1.0000,  0.0598, -1.0000,  0.1108, -1.0000,  0.1622,\n",
      "        -1.0000,  0.1923, -1.0000,  0.1699,  0.0980, -1.0000,  0.1484,  0.1617,\n",
      "         0.1046, -1.0000, -1.0000, -1.0000, -1.0000,  0.2057,  0.1226,  0.1064,\n",
      "        -1.0000,  0.1503,  0.1512, -1.0000,  0.1064, -1.0000, -1.0000,  0.1141,\n",
      "        -1.0000, -1.0000,  0.1606, -1.0000,  0.1060,  0.1321, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1115,  0.0939,  0.1659, -1.0000, -1.0000, -1.0000,  0.1354,\n",
      "         0.1637,  0.1068,  0.1380, -1.0000,  0.1473, -1.0000,  0.0996,  0.1884,\n",
      "         0.1515, -1.0000,  0.1656, -1.0000,  0.1090,  0.1562, -1.0000,  0.1777])\n",
      "state action values after:  tensor([-0.2792, -0.1476, -0.5030, -0.1173, -0.3856, -0.0903, -0.5293, -0.0241,\n",
      "        -0.1815, -0.3906, -0.0962, -0.0828, -0.1029, -0.5080,  0.1388,  0.1112,\n",
      "        -0.5582, -0.1248, -0.3387, -0.1289, -0.5016,  0.1156,  0.1692, -0.0305,\n",
      "        -0.0535, -0.2665, -0.5888, -0.1286, -0.3277, -0.3053, -0.2336,  0.0117,\n",
      "        -0.5016,  0.0849, -0.1059, -0.3961, -0.3665, -0.3395, -0.4271, -0.1160,\n",
      "         0.0637, -0.0467, -0.0915, -0.5033, -0.4456, -0.5092, -0.5767,  0.1884,\n",
      "        -0.3878, -0.1885, -0.0126, -0.2402, -0.0687, -0.3889, -0.2600,  0.2115,\n",
      "        -0.5509, -0.3115, -0.4335, -0.3069,  0.0305, -0.0699, -0.5536, -0.1090],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.0980, -1.0000, -1.0000,  0.1622,  0.1656, -1.0000, -1.0000,  0.1073,\n",
      "        -1.0000, -1.0000,  0.1316, -1.0000, -1.0000, -1.0000,  0.1617,  0.1295,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1226, -1.0000,  0.1321,  0.1586,  0.1515,\n",
      "         0.1884, -1.0000,  0.1923,  0.1108, -1.0000,  0.1484, -1.0000,  0.0939,\n",
      "        -1.0000,  0.1068,  0.1064, -1.0000,  0.1512, -1.0000, -1.0000, -1.0000,\n",
      "         0.1503,  0.1484,  0.1354,  0.1659, -1.0000, -1.0000, -1.0000,  0.1562,\n",
      "        -1.0000,  0.1473,  0.1214,  0.1088,  0.1307, -1.0000, -1.0000,  0.1606,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1090,  0.1022,  0.1323, -1.0000,  0.1115])\n",
      "state action values after:  tensor([-0.5798, -0.1087, -0.3534, -0.4688, -0.4039, -0.2802, -0.1367, -0.0929,\n",
      "        -0.2125, -0.3159,  0.1817, -0.4143, -0.0501, -0.6134,  0.0923,  0.0402,\n",
      "        -0.5673, -0.5242, -0.3728, -0.4093, -0.5836, -0.0230, -0.5247,  0.2171,\n",
      "        -0.4065, -0.3828, -0.4374, -0.5780, -0.0746, -0.1087, -0.5673, -0.4564,\n",
      "         0.0243,  0.1426, -0.2769, -0.0098, -0.5705,  0.1286, -0.1334,  0.0780,\n",
      "        -0.5247, -0.1508, -0.5836, -0.3190,  0.1635, -0.6610, -0.3018, -0.0672,\n",
      "         0.0968, -0.1056,  0.1277, -0.5616, -0.4063, -0.1850, -0.4133, -0.5374,\n",
      "        -0.2268, -0.2253, -0.0873, -0.1145, -0.1833, -0.5242, -0.0204, -0.5267],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1115, -1.0000, -1.0000,  0.1656, -1.0000,  0.1226,  0.1316,\n",
      "        -1.0000,  0.1484,  0.1586, -1.0000,  0.1484,  0.1923,  0.1068,  0.1022,\n",
      "         0.2007, -1.0000, -1.0000, -1.0000, -1.0000,  0.1073,  0.1659,  0.1606,\n",
      "         0.1550,  0.1512, -1.0000, -1.0000,  0.1323,  0.1115, -1.0000, -1.0000,\n",
      "         0.0996,  0.1777, -1.0000,  0.1214, -1.0000,  0.1380,  0.1108,  0.1503,\n",
      "         0.1659,  0.1637, -1.0000,  0.1090,  0.1623, -1.0000, -1.0000,  0.1307,\n",
      "         0.1064, -1.0000,  0.1321, -1.0000, -1.0000,  0.1473, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1354,  0.1622, -1.0000, -1.0000,  0.1060, -1.0000])\n",
      "state action values after:  tensor([-0.2502, -0.0170, -0.0184, -0.3405, -0.5453, -0.4212, -0.0513,  0.1653,\n",
      "        -0.2953, -0.1193, -0.0851, -0.0399, -0.1245, -0.4373, -0.2342, -0.5950,\n",
      "        -0.4220, -0.4440, -0.1448, -0.1759, -0.3983, -0.5762, -0.0946, -0.6925,\n",
      "         0.0726, -0.4779, -0.5420, -0.4908,  0.0083, -0.2383,  0.0501, -0.1220,\n",
      "         0.0991, -0.3914, -0.5906, -0.3162, -0.3914, -0.4301, -0.4287, -0.5940,\n",
      "        -0.4855, -0.4199, -0.1590, -0.2942, -0.3722, -0.3229,  0.0919, -0.6337,\n",
      "        -0.3026, -0.0304, -0.1056,  0.0249, -0.1849, -0.3889, -0.1996,  0.1489,\n",
      "        -0.0615,  0.0339, -0.3154, -0.5478, -0.3112, -0.5881, -0.0128, -0.5020],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1515,  0.1073, -1.0000, -1.0000, -1.0000,  0.1484,  0.1623,\n",
      "        -1.0000, -1.0000,  0.1316,  0.1884, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.2057,  0.1226,  0.1473, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1141, -1.0000,  0.1659, -1.0000,  0.1777, -1.0000,  0.1022, -1.0000,\n",
      "         0.1068, -1.0000, -1.0000,  0.0980, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1550, -1.0000, -1.0000, -1.0000, -1.0000,  0.1503,  0.1923,\n",
      "        -1.0000, -1.0000,  0.1115,  0.0939, -1.0000,  0.0532, -1.0000,  0.1777,\n",
      "         0.1307,  0.0996, -1.0000, -1.0000,  0.1098,  0.2007,  0.1060, -1.0000])\n",
      "state action values after:  tensor([-0.6390, -0.4180,  0.0082, -0.6150, -0.3983, -0.4615, -0.3371, -0.0764,\n",
      "        -0.2545, -0.5135, -0.2755, -0.4075, -0.3336, -0.3748, -0.1449, -0.6214,\n",
      "        -0.1104, -0.4488, -0.1530, -0.0816,  0.0445,  0.1620, -0.1192, -0.5587,\n",
      "         0.1311,  0.1648, -0.1401,  0.0342, -0.5006, -0.1255,  0.0158, -0.0507,\n",
      "         0.1667, -0.1671, -0.5051, -0.2171, -0.3136, -0.4606, -0.0046, -0.4115,\n",
      "        -0.5251, -0.2154,  0.1575, -0.1462,  0.2047,  0.1063, -0.5668, -0.6786,\n",
      "        -0.5587, -0.4120, -0.0077, -0.6359, -0.5939,  0.0854, -0.4445, -0.1424,\n",
      "         0.1059, -0.4379, -0.3394,  0.0327,  0.1552, -0.3304, -0.3976, -0.6079],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.0598, -1.0000, -1.0000,  0.2057,  0.1090,  0.1316,\n",
      "         0.1088, -1.0000, -1.0000, -1.0000,  0.0980, -1.0000,  0.1637, -1.0000,\n",
      "         0.0530, -1.0000,  0.1226,  0.1323,  0.0996,  0.1295, -1.0000,  0.1659,\n",
      "         0.1380,  0.1046, -1.0000,  0.1184, -1.0000,  0.1064,  0.1783, -1.0000,\n",
      "         0.1623,  0.1473, -1.0000, -1.0000, -1.0000, -1.0000,  0.1060, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1617,  0.1108,  0.1562,  0.1068, -1.0000, -1.0000,\n",
      "         0.1659,  0.1512,  0.1515,  0.1699, -1.0000,  0.1141, -1.0000, -1.0000,\n",
      "         0.1503,  0.1656, -1.0000,  0.0939,  0.1777,  0.1484, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-1.0480e-01,  2.0846e-01, -1.8618e-01, -6.4243e-01,  1.6073e-01,\n",
      "        -5.3409e-01, -4.1759e-01, -2.3618e-01, -6.6942e-01,  1.6785e-01,\n",
      "         1.7835e-01,  9.8090e-02, -1.5079e-01, -5.3469e-01, -5.2143e-01,\n",
      "        -4.2061e-01, -1.2037e-01, -1.9999e-01,  1.7650e-01, -5.8613e-01,\n",
      "        -9.9628e-02, -6.5125e-01,  3.4090e-04, -1.0267e-02,  5.3303e-02,\n",
      "        -1.6007e-01, -1.6135e-01, -6.5125e-01, -4.5058e-01, -6.2344e-01,\n",
      "        -7.4318e-02, -3.2491e-01,  1.3230e-01, -9.9628e-02,  1.6310e-01,\n",
      "         1.2976e-01, -3.3451e-01, -2.2630e-02,  2.6268e-02,  2.4129e-03,\n",
      "        -4.7573e-01,  9.3718e-02, -2.5309e-01, -6.4031e-01, -6.4028e-01,\n",
      "         1.1304e-01, -4.0598e-01, -1.4200e-01,  3.9466e-02, -4.3060e-01,\n",
      "        -3.3234e-01, -5.3409e-02, -4.5293e-01, -5.1022e-01, -4.3605e-01,\n",
      "        -6.2190e-01, -2.5840e-01,  1.6262e-01, -3.9023e-01,  4.1316e-02,\n",
      "        -1.1221e-01, -4.9683e-02, -4.2452e-01, -9.0603e-02],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1562, -1.0000, -1.0000,  0.1777, -1.0000, -1.0000, -1.0000,\n",
      "         0.1923,  0.1623,  0.1046,  0.1141,  0.1108, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1295, -1.0000,  0.1115, -1.0000,  0.1515,  0.1073,\n",
      "         0.0996, -1.0000,  0.1226, -1.0000, -1.0000,  0.2007, -1.0000, -1.0000,\n",
      "         0.1380,  0.1115,  0.1321,  0.1064,  0.1484,  0.1884,  0.1777,  0.1060,\n",
      "         0.2057,  0.1039, -1.0000, -1.0000, -1.0000,  0.1068, -1.0000,  0.1637,\n",
      "         0.0939, -1.0000, -1.0000,  0.1484,  0.1656, -1.0000, -1.0000, -1.0000,\n",
      "         0.1088,  0.1617, -1.0000,  0.1184,  0.0530,  0.1307,  0.1512,  0.1622])\n",
      "state action values after:  tensor([ 0.1337, -0.1371, -0.5402,  0.1109, -0.6820, -0.5359, -0.0964, -0.0610,\n",
      "         0.2311, -0.2635, -0.1532, -0.4838, -0.4728, -0.4438, -0.4387, -0.0964,\n",
      "         0.0482, -0.0419, -0.5810, -0.1221, -0.3412,  0.0081, -0.6352, -0.1389,\n",
      "        -0.5105, -0.5714, -0.4651, -0.1128, -0.0550, -0.4228, -0.4453, -0.3521,\n",
      "        -0.6699,  0.0884, -0.2520,  0.1914, -0.6038, -0.4528,  0.1647, -0.6012,\n",
      "         0.0462, -0.3368,  0.1677, -0.4866, -0.5625, -0.2211, -0.4344, -0.6576,\n",
      "        -0.1002, -0.0557, -0.6745,  0.0959,  0.0617, -0.3513, -0.6699, -0.2226,\n",
      "        -0.3555, -0.6038, -0.4409, -0.4560, -0.4613, -0.6665, -0.6544, -0.3638],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1380, -1.0000, -1.0000,  0.1141,  0.1923, -1.0000,  0.1115,  0.1316,\n",
      "         0.1606, -1.0000,  0.1108, -1.0000, -1.0000, -1.0000, -1.0000,  0.1115,\n",
      "         0.1184, -1.0000,  0.1659, -1.0000, -1.0000,  0.1515,  0.2007,  0.1637,\n",
      "        -1.0000, -1.0000,  0.1656,  0.0530,  0.1484, -1.0000, -1.0000,  0.1596,\n",
      "        -1.0000,  0.1451, -1.0000,  0.1046, -1.0000, -1.0000,  0.1777, -1.0000,\n",
      "         0.0939,  0.1484,  0.1623,  0.2057, -1.0000, -1.0000,  0.1512, -1.0000,\n",
      "        -1.0000,  0.1354, -1.0000,  0.1039,  0.0996, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1550, -1.0000,  0.1699, -1.0000,  0.1098])\n",
      "state action values after:  tensor([ 0.1680, -0.3954, -0.4656,  0.0179, -0.6127, -0.1260, -0.6744,  0.1450,\n",
      "        -0.4382, -0.0561, -0.6410,  0.2035, -0.4719,  0.1232,  0.0134,  0.0537,\n",
      "        -0.5452, -0.3354, -0.4707, -0.0093,  0.0870, -0.7741, -0.1533, -0.1174,\n",
      "        -0.3927, -0.3824, -0.6176, -0.5018, -0.4420, -0.6847, -0.1372, -0.0554,\n",
      "         0.1356, -0.2380, -0.2593, -0.4213, -0.1380, -0.2702, -0.2757, -0.1135,\n",
      "        -0.6809, -0.4908, -0.3529, -0.3715, -0.5709, -0.6176,  0.1818,  0.0518,\n",
      "         0.2361,  0.2312, -0.1794,  0.0139,  0.0130, -0.0940,  0.1671, -0.2263,\n",
      "        -0.5925, -0.3586,  0.0531, -0.2030,  0.0858, -0.1560, -0.5845, -0.4661],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1617, -1.0000, -1.0000,  0.0598, -1.0000, -1.0000,  0.1699,  0.1503,\n",
      "         0.1512, -1.0000,  0.2007,  0.1046,  0.1656,  0.1141,  0.1515,  0.1783,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1884,  0.1022, -1.0000,  0.1473, -1.0000,\n",
      "         0.0980,  0.1098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1484,\n",
      "         0.1380, -1.0000, -1.0000, -1.0000,  0.1064, -1.0000, -1.0000,  0.0530,\n",
      "        -1.0000,  0.2057, -1.0000, -1.0000, -1.0000, -1.0000,  0.1321,  0.0939,\n",
      "         0.1586,  0.1606,  0.1226,  0.1214,  0.1060,  0.1115,  0.1777, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1184, -1.0000,  0.1342,  0.1108,  0.1659,  0.0532])\n",
      "state action values after:  tensor([-0.1248, -0.7583,  0.2294, -0.3767,  0.0161,  0.0408, -0.3920,  0.1672,\n",
      "        -0.5130, -0.4567, -0.7084, -0.5831,  0.1556, -0.0069, -0.3444, -0.4833,\n",
      "         0.2433, -0.0571, -0.2683, -0.0488, -0.0559, -0.4904, -0.1570, -0.4102,\n",
      "        -0.6231, -0.1144, -0.2677, -0.1275, -0.6283, -0.4158, -0.4743,  0.0138,\n",
      "        -0.3539, -0.0840, -0.2756, -0.2894, -0.0716, -0.4572, -0.4749,  0.1315,\n",
      "        -0.5607, -0.5641, -0.2263, -0.1385, -0.6920,  0.0547, -0.3417, -0.4083,\n",
      "        -0.4758,  0.1677, -0.3781, -0.6755, -0.4744, -0.0941, -0.7441,  0.1035,\n",
      "        -0.4363, -0.6961, -0.0071, -0.8158, -0.1941,  0.1386, -0.1334, -0.2341],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.1606, -1.0000,  0.0598,  0.1777, -1.0000,  0.1617,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1659,  0.1064,  0.1073, -1.0000,  0.0532,\n",
      "         0.1586,  0.1484, -1.0000,  0.1307,  0.1316,  0.2057,  0.1473,  0.0980,\n",
      "        -1.0000,  0.0530, -1.0000,  0.0961, -1.0000, -1.0000, -1.0000,  0.1060,\n",
      "         0.1596,  0.1622,  0.1088, -1.0000, -1.0000,  0.1550, -1.0000,  0.1068,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1637, -1.0000,  0.1184,  0.1090, -1.0000,\n",
      "        -1.0000,  0.1777, -1.0000, -1.0000,  0.1656,  0.1115, -1.0000,  0.1451,\n",
      "        -1.0000, -1.0000,  0.1884, -1.0000, -1.0000,  0.1380, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.1506, -0.1588,  0.1643, -0.1340,  0.2262, -0.0538, -0.7344, -0.4359,\n",
      "        -0.6406, -0.3575,  0.1944, -0.4914, -0.1810, -0.8297, -0.0880, -0.5182,\n",
      "        -0.6354, -0.5526, -0.4521, -0.3203,  0.1112, -0.1391, -0.0980, -0.2926,\n",
      "        -0.2508, -0.4135, -0.6276, -0.0915,  0.1341, -0.1985, -0.3348, -0.5726,\n",
      "        -0.6377, -0.4680, -0.1482, -0.2489, -0.4366, -0.1663,  0.2264, -0.6079,\n",
      "        -0.0939, -0.2093, -0.1444,  0.0986,  0.0134, -0.0071, -0.5196,  0.2471,\n",
      "        -0.3963, -0.1168, -0.4522, -0.5847,  0.1653, -0.4750, -0.6853, -0.7791,\n",
      "        -0.4214,  0.1115, -0.6788,  0.1022,  0.0156, -0.0939, -0.5967, -0.2011],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1064,  0.1473,  0.1064, -1.0000,  0.1046,  0.1316, -1.0000,  0.1512,\n",
      "         0.2007, -1.0000,  0.1321, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1484,  0.1957,  0.1637,  0.1323, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1068, -1.0000,  0.1090, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1108,  0.1606, -1.0000,\n",
      "         0.1115, -1.0000, -1.0000,  0.1039,  0.1214,  0.1073, -1.0000,  0.1586,\n",
      "        -1.0000,  0.0530,  0.1550, -1.0000,  0.1503, -1.0000,  0.1923, -1.0000,\n",
      "        -1.0000,  0.1451,  0.1699,  0.1022,  0.1060,  0.1115, -1.0000,  0.1226])\n",
      "state action values after:  tensor([-0.5714, -0.0971, -0.8639, -0.1628, -0.1408, -0.3430, -0.2495, -0.5147,\n",
      "        -0.2275,  0.2060,  0.1103,  0.1614, -0.1481, -0.0668,  0.2353, -0.6095,\n",
      "        -0.3169, -0.5176, -0.6786, -0.3718, -0.8398, -0.0534, -0.0942, -0.4347,\n",
      "        -0.2047,  0.1199, -0.2882,  0.0141,  0.1670,  0.1721, -0.7140, -0.6797,\n",
      "        -0.1012, -0.5506, -0.2874, -0.4735, -0.4646, -0.0942, -0.1056, -0.6322,\n",
      "        -0.6570, -0.1494, -0.8066,  0.0454,  0.1661,  0.1352, -0.6783, -0.5260,\n",
      "        -0.2559, -0.8144, -0.6756,  0.1090,  0.0847, -0.4165, -0.4427, -0.7612,\n",
      "         0.0607, -0.4821, -0.7119, -0.0537, -0.7140, -0.0011, -0.4691,  0.2224],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1659, -1.0000, -1.0000,  0.1473,  0.1637,  0.1596, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1562,  0.1342,  0.1617, -1.0000,  0.1484,  0.1046, -1.0000,\n",
      "        -1.0000,  0.0532, -1.0000, -1.0000, -1.0000,  0.1307,  0.1115, -1.0000,\n",
      "        -1.0000,  0.1451, -1.0000,  0.1515,  0.1623,  0.1064, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1088,  0.1656, -1.0000,  0.1115, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1777,  0.1777,  0.1068,  0.1923, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1699,  0.1022,  0.1783, -1.0000,  0.1550, -1.0000,\n",
      "         0.0939,  0.2057, -1.0000,  0.1316, -1.0000,  0.1884, -1.0000,  0.1606])\n",
      "state action values after:  tensor([-5.1445e-01, -5.7833e-01, -1.6847e-01,  6.1069e-02,  1.1480e-01,\n",
      "        -5.3456e-01,  2.4277e-01, -6.3521e-01, -4.8659e-01,  1.1871e-01,\n",
      "        -4.4858e-01, -3.0200e-01, -7.1853e-01,  2.0068e-01,  1.3254e-01,\n",
      "        -4.7172e-01, -4.7675e-01, -4.2112e-01,  1.5632e-01,  1.2825e-01,\n",
      "         1.6539e-01, -7.1853e-01, -9.4113e-02, -6.2743e-01, -4.5701e-01,\n",
      "        -7.8915e-01, -5.3075e-01, -1.8349e-01, -2.3824e-01, -5.8746e-02,\n",
      "        -4.6224e-01,  5.8682e-02, -5.4416e-01, -2.9484e-01,  9.7867e-02,\n",
      "        -9.6327e-02, -1.0860e-01, -5.5861e-01, -3.8173e-04, -2.9970e-01,\n",
      "         8.2235e-03, -5.3388e-01, -3.8754e-01, -6.3000e-01, -1.8326e-01,\n",
      "        -6.7910e-01,  4.3895e-02, -1.5771e-01, -6.7547e-01, -8.9336e-01,\n",
      "        -1.2301e-01, -2.2476e-01,  2.0179e-01, -4.2848e-01, -3.4092e-01,\n",
      "        -8.4896e-01, -1.4527e-01, -6.4547e-01,  1.5753e-01, -5.3559e-01,\n",
      "        -7.1734e-01, -6.7425e-01, -1.1556e-01,  1.0410e-02],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.1064,  0.0939,  0.1022, -1.0000,  0.1295, -1.0000,\n",
      "        -1.0000,  0.1342, -1.0000, -1.0000, -1.0000,  0.1562,  0.1957,  0.2057,\n",
      "        -1.0000,  0.1512,  0.1617,  0.1451,  0.1623, -1.0000,  0.1622,  0.2007,\n",
      "        -1.0000, -1.0000, -1.0000,  0.0961, -1.0000,  0.1354,  0.0980,  0.1184,\n",
      "        -1.0000,  0.1484,  0.1039,  0.1115, -1.0000,  0.1659,  0.1884, -1.0000,\n",
      "         0.0598,  0.0532, -1.0000, -1.0000,  0.1108, -1.0000,  0.1777, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1226,  0.1321,  0.1550, -1.0000, -1.0000,\n",
      "         0.1637, -1.0000,  0.1967, -1.0000, -1.0000, -1.0000, -1.0000,  0.1214])\n",
      "state action values after:  tensor([ 0.1848, -0.4136, -0.4478, -0.2194,  0.2496, -0.4890, -0.4808, -0.5005,\n",
      "        -0.3287, -0.3408, -0.6307, -0.2826,  0.2507, -0.4302,  0.1634,  0.0094,\n",
      "        -0.5536, -0.6492, -0.4746, -0.8164, -0.2090,  0.2113,  0.1603, -0.5417,\n",
      "        -0.5045, -0.4735, -0.4631, -0.4500, -0.0958, -0.4635, -0.2646, -0.6835,\n",
      "        -0.0953, -0.2629,  0.0113, -0.4165, -0.7018, -0.2680, -0.1274, -0.1765,\n",
      "        -0.1649, -0.4738, -0.2735,  0.0074, -0.1745, -0.4696, -0.4058,  0.1265,\n",
      "        -0.6580, -0.5502,  0.2032, -0.1335, -0.2969, -0.6607, -0.1466, -0.6723,\n",
      "        -0.1416, -0.5485, -0.2085, -0.7463, -0.7247, -0.6197, -0.4619, -0.5173],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1064,  0.1512, -1.0000, -1.0000,  0.1586, -1.0000,  0.0980, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1484,  0.1046, -1.0000,  0.1278,  0.1515,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1606,  0.1777, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1656, -1.0000,  0.1115,  0.2057, -1.0000, -1.0000,\n",
      "         0.1622,  0.1541,  0.1214,  0.1550, -1.0000, -1.0000,  0.0530, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.0598,  0.1473, -1.0000, -1.0000,  0.1342,\n",
      "         0.1923, -1.0000,  0.1321, -1.0000,  0.1088, -1.0000,  0.1637, -1.0000,\n",
      "        -1.0000,  0.1659, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.3664, -0.6316, -0.2269, -0.5436, -0.3022, -0.4587, -0.1682, -0.1321,\n",
      "        -0.0971, -0.3267, -0.6527,  0.2026,  0.1809, -0.4908, -0.7309,  0.2582,\n",
      "         0.0074, -0.4434, -0.7249, -0.2707, -0.5257,  0.2558, -0.6485,  0.1419,\n",
      "        -0.1542,  0.0148, -0.7305, -0.3527, -0.1736, -0.1619, -0.2763, -0.2317,\n",
      "        -0.5724,  0.1881, -0.4922,  0.1494,  0.0958, -0.6527, -0.7305, -0.6722,\n",
      "         0.0478,  0.1950, -0.9144, -0.5509, -0.4907, -0.2073, -0.1794, -0.5316,\n",
      "        -0.5626, -0.4711, -0.5374, -0.0670, -0.1524,  0.1683, -0.2862,  0.2472,\n",
      "        -0.7091, -0.4875, -0.4266, -0.3157, -0.6571, -0.9060, -0.6128, -0.3697],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000, -1.0000, -1.0000,  0.1088,  0.1656, -1.0000,  0.0530,\n",
      "         0.1115, -1.0000, -1.0000,  0.1321,  0.1141, -1.0000, -1.0000,  0.1295,\n",
      "         0.1515, -1.0000, -1.0000,  0.1484, -1.0000,  0.1046,  0.1923,  0.1451,\n",
      "        -1.0000,  0.1060, -1.0000, -1.0000, -1.0000, -1.0000,  0.1541, -1.0000,\n",
      "        -1.0000,  0.1562, -1.0000,  0.1380,  0.1039, -1.0000, -1.0000, -1.0000,\n",
      "         0.1777,  0.1503, -1.0000, -1.0000, -1.0000,  0.1108,  0.1473, -1.0000,\n",
      "         0.0532, -1.0000,  0.1659,  0.1307,  0.1323,  0.1278,  0.1090,  0.1586,\n",
      "        -1.0000,  0.1098, -1.0000,  0.1596,  0.1699, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-1.0172e-01, -6.0965e-01, -4.4632e-01,  8.1796e-02, -2.1215e-01,\n",
      "        -3.9288e-01,  1.4497e-01, -2.7370e-01, -5.2539e-01, -5.4619e-01,\n",
      "        -6.3243e-01, -7.4790e-01, -5.7600e-01, -5.1514e-01,  1.7039e-01,\n",
      "        -2.9144e-01, -6.6870e-01, -9.1755e-01, -3.8992e-01,  6.8907e-03,\n",
      "        -2.5860e-01, -5.2539e-01, -7.3430e-01, -3.0699e-01,  4.8084e-02,\n",
      "         1.5424e-01, -4.8493e-01, -3.8206e-01, -4.9503e-01, -4.4681e-01,\n",
      "        -3.6348e-01, -2.5046e-01, -2.6174e-01, -4.2283e-01, -5.3449e-01,\n",
      "        -2.1902e-01, -8.0152e-01,  1.3050e-02, -3.0237e-01,  4.0249e-03,\n",
      "         2.5748e-01,  1.5745e-01, -4.7484e-01,  1.2732e-01, -2.8096e-01,\n",
      "         1.3774e-01,  2.4928e-01,  3.8692e-04, -3.0598e-01, -3.4035e-01,\n",
      "         2.0011e-01, -6.0491e-02, -5.9014e-01, -1.8301e-01,  2.4174e-01,\n",
      "        -5.3991e-01, -7.2839e-02, -2.8795e-01,  7.8146e-03, -2.0208e-01,\n",
      "        -6.3264e-01, -1.4408e-02, -6.7045e-01, -5.9734e-01],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1622,  0.2007,  0.2057,  0.0996, -1.0000,  0.1550,  0.1451,  0.1090,\n",
      "         0.1659, -1.0000, -1.0000, -1.0000,  0.0532,  0.0980,  0.1278, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1884,  0.1484,  0.1659, -1.0000,  0.1596,\n",
      "         0.1777,  0.1623, -1.0000, -1.0000, -1.0000,  0.1076, -1.0000,  0.1226,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1108, -1.0000,  0.1060, -1.0000,  0.1515,\n",
      "         0.1046,  0.1957, -1.0000,  0.1022, -1.0000,  0.1342,  0.1784,  0.0598,\n",
      "         0.1088, -1.0000,  0.1606,  0.1354, -1.0000, -1.0000,  0.1586, -1.0000,\n",
      "         0.1307,  0.1541,  0.1214,  0.1064, -1.0000,  0.1073, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.3698, -0.2023, -0.1824, -0.3774, -0.1026, -0.7698, -0.2297, -0.7389,\n",
      "         0.1403, -0.5552, -0.5104, -0.5856, -0.8745,  0.1950, -0.1048, -0.2782,\n",
      "        -0.3312, -0.4783, -0.5815, -0.2306, -0.4731, -0.5996, -0.4148,  0.0935,\n",
      "        -0.5006, -0.5873, -0.3938,  0.0495, -0.6638, -0.0620, -0.3013, -0.1026,\n",
      "        -0.5600, -0.4646,  0.2229, -0.7324,  0.1320, -0.4970, -0.5342, -0.2952,\n",
      "         0.1523, -0.5528, -0.5104,  0.2672, -0.4643, -0.6661,  0.1534,  0.2522,\n",
      "         0.1807,  0.1988, -0.3972,  0.0608,  0.1943,  0.1500, -0.5573, -0.6250,\n",
      "        -0.3538, -0.2538, -0.4381, -0.8273,  0.1986,  0.0115, -0.1374, -0.6408],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000, -1.0000,  0.1550,  0.1115, -1.0000,  0.1108, -1.0000,\n",
      "         0.1342, -1.0000,  0.1659, -1.0000, -1.0000,  0.1064,  0.1622, -1.0000,\n",
      "        -1.0000,  0.1076, -1.0000, -1.0000, -1.0000,  0.2007, -1.0000,  0.1039,\n",
      "        -1.0000,  0.0532, -1.0000,  0.1777, -1.0000,  0.1354, -1.0000,  0.1115,\n",
      "        -1.0000, -1.0000,  0.1648, -1.0000,  0.1068, -1.0000, -1.0000,  0.1596,\n",
      "         0.1777, -1.0000,  0.1659,  0.1295, -1.0000, -1.0000,  0.1623,  0.1784,\n",
      "         0.1967,  0.1503, -1.0000,  0.1184,  0.1606,  0.1380, -1.0000,  0.1923,\n",
      "        -1.0000,  0.1226, -1.0000, -1.0000,  0.1321,  0.1060,  0.1484,  0.1699])\n",
      "state action values after:  tensor([-0.2959, -0.7526, -0.4843,  0.0925,  0.0511, -0.4590, -0.2485, -0.9998,\n",
      "        -0.4916, -0.5862, -0.3693, -0.4140,  0.1513, -0.1862, -0.5558,  0.2265,\n",
      "        -0.1437, -0.7912, -0.5295, -0.1078, -0.2148, -0.6388, -0.5754, -0.6216,\n",
      "         0.1312,  0.1932,  0.1298, -0.3123, -0.6587,  0.1870,  0.1946, -0.0636,\n",
      "        -0.6454, -0.2275, -0.8764,  0.1628, -0.5735, -0.3586,  0.1977, -0.6257,\n",
      "        -0.7384, -0.4097,  0.0654, -0.2235, -0.5572,  0.2585, -0.5876, -0.5277,\n",
      "        -0.4275, -0.5363, -0.0184,  0.1497,  0.1811, -0.2013, -0.4152,  0.0043,\n",
      "        -0.8517,  0.1705, -0.0637, -0.2514, -0.2818, -0.4069,  0.1294, -0.2252],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1541, -1.0000, -1.0000,  0.1039,  0.1777, -1.0000, -1.0000, -1.0000,\n",
      "         0.1659,  0.2007,  0.1512, -1.0000,  0.1380, -1.0000, -1.0000,  0.1586,\n",
      "         0.0530, -1.0000,  0.1098,  0.1622, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1617,  0.1064,  0.1068, -1.0000, -1.0000,  0.1141,  0.1321,  0.1354,\n",
      "        -1.0000,  0.1484, -1.0000,  0.1957, -1.0000, -1.0000,  0.1503, -1.0000,\n",
      "        -1.0000, -1.0000,  0.0939, -1.0000, -1.0000,  0.1046, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1073,  0.1777,  0.1967, -1.0000, -1.0000,  0.1214,\n",
      "        -1.0000,  0.1278,  0.1316, -1.0000,  0.1596, -1.0000,  0.1022,  0.1064])\n",
      "state action values after:  tensor([ 0.1515, -0.5997, -0.2978,  0.2513, -0.1972, -0.2521, -0.3810, -0.1083,\n",
      "        -0.9446, -0.3059, -0.8800,  0.2176,  0.1382,  0.1945, -0.6534, -0.3463,\n",
      "        -0.2461,  0.0525, -0.4554, -0.3572, -0.6385, -0.4593, -0.4317,  0.1854,\n",
      "         0.1486, -0.5910,  0.0086, -0.6226, -0.3307, -0.6325, -0.5501,  0.1266,\n",
      "         0.1285, -0.4881,  0.1891, -0.0197, -0.1472, -0.0122, -0.2025, -1.0237,\n",
      "         0.0861, -0.4772, -0.4464, -0.2283, -0.2059, -0.1684,  0.0020, -0.6384,\n",
      "        -0.5877, -0.0660, -0.2607, -0.5149, -0.7730, -0.4045, -0.5811, -0.5625,\n",
      "        -0.6080, -0.5561,  0.1856, -1.0211, -0.3931,  0.0576, -0.5400, -0.4753],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1623, -1.0000,  0.1541,  0.1784, -1.0000,  0.1108, -1.0000,  0.1115,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1586,  0.1342,  0.1503, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1777, -1.0000,  0.1512, -1.0000, -1.0000, -1.0000,  0.1141,\n",
      "         0.1380,  0.1923,  0.1060, -1.0000, -1.0000, -1.0000,  0.1076,  0.1068,\n",
      "         0.1617, -1.0000,  0.1321,  0.1073,  0.0530,  0.0598,  0.1473, -1.0000,\n",
      "         0.2005, -1.0000, -1.0000,  0.1090, -1.0000,  0.1637,  0.1214, -1.0000,\n",
      "        -1.0000,  0.1316, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.0532, -1.0000,  0.1606, -1.0000, -1.0000,  0.1184,  0.1098,  0.1659])\n",
      "state action values after:  tensor([-0.3940, -0.5241, -1.0754, -0.5766, -0.2475, -0.4848, -0.6529, -0.1987,\n",
      "         0.1288, -0.7402, -0.5644, -0.5686,  0.1648, -0.3592, -0.5324, -0.5886,\n",
      "        -0.4182,  0.1511, -0.2694, -0.1729, -0.8355,  0.1072, -0.0196, -0.5888,\n",
      "        -0.4514, -0.0647, -0.6074, -0.4875, -0.2605, -0.2698, -1.0434,  0.2669,\n",
      "         0.1825, -0.5083, -0.3357, -0.7932, -0.6510, -0.2632, -0.6449,  0.1642,\n",
      "        -0.4543,  0.1775, -0.6199,  0.2520, -0.7011, -0.2550,  0.0660, -0.6127,\n",
      "        -0.6529,  0.1753, -0.7462, -0.2265, -0.2472, -0.6980, -0.5004, -0.5635,\n",
      "        -0.3462, -0.6489, -0.0921,  0.1905, -0.4882, -0.3300, -0.6066, -0.5106],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000, -1.0000,  0.1923,  0.1323, -1.0000, -1.0000,  0.1484,\n",
      "         0.1516, -1.0000,  0.2007, -1.0000,  0.1278, -1.0000, -1.0000,  0.1076,\n",
      "        -1.0000,  0.1623, -1.0000,  0.1637, -1.0000,  0.1783,  0.1073, -1.0000,\n",
      "        -1.0000,  0.1354, -1.0000, -1.0000,  0.1596, -1.0000, -1.0000,  0.1295,\n",
      "         0.1141, -1.0000, -1.0000, -1.0000, -1.0000,  0.1108, -1.0000,  0.1562,\n",
      "        -1.0000,  0.1967, -1.0000,  0.1046, -1.0000,  0.1226,  0.0939, -1.0000,\n",
      "        -1.0000,  0.1318, -1.0000, -1.0000,  0.1064, -1.0000, -1.0000, -1.0000,\n",
      "         0.1512, -1.0000,  0.1307,  0.1503, -1.0000,  0.1550,  0.1699, -1.0000])\n",
      "state action values after:  tensor([-0.4051, -0.9097, -0.6224, -0.6168, -0.4218, -0.2519, -0.6726, -0.5435,\n",
      "        -0.2877, -0.5676, -0.5053, -0.3148, -0.7263, -0.5132,  0.0972, -0.2675,\n",
      "         0.1649, -0.6471, -0.5512,  0.1553, -0.3149,  0.2465,  0.1769, -0.5600,\n",
      "        -0.0670, -0.2794, -0.6445, -0.8515,  0.1740, -0.7153, -0.0639, -0.2928,\n",
      "        -0.3091, -0.3889,  0.2113, -0.5061, -0.2094, -0.3338,  0.2005, -0.3859,\n",
      "        -0.0956,  0.1155, -0.5951,  0.1241, -0.5746,  0.1240, -1.0628, -0.0203,\n",
      "        -0.1555,  0.1679,  0.1849, -0.2013, -0.3471, -0.5547, -0.6976, -0.4136,\n",
      "         0.0568, -0.2950, -0.3902, -0.2088,  0.0094,  0.1551, -0.6300, -0.5334],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000, -1.0000, -1.0000,  0.1656,  0.1226, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1550, -1.0000, -1.0000,  0.2005,  0.1323,\n",
      "         0.1562, -1.0000, -1.0000,  0.1777,  0.1088,  0.1046,  0.1321,  0.1923,\n",
      "         0.1316, -1.0000, -1.0000, -1.0000,  0.1967, -1.0000,  0.1354,  0.1541,\n",
      "        -1.0000,  0.2057,  0.1648, -1.0000,  0.1473,  0.1512,  0.1586, -1.0000,\n",
      "         0.1307,  0.1529,  0.1699,  0.1022, -1.0000,  0.1516, -1.0000,  0.1073,\n",
      "         0.0530,  0.1318,  0.1064,  0.1090, -1.0000,  0.1098, -1.0000, -1.0000,\n",
      "         0.1777, -1.0000, -1.0000,  0.1484,  0.1060,  0.1623,  0.1076, -1.0000])\n",
      "state action values after:  tensor([-0.1858,  0.1325, -0.5864,  0.0926, -0.6598, -0.5708, -0.6337, -0.8904,\n",
      "        -0.0968, -0.5809,  0.2446, -0.2276, -0.4248, -0.1188, -0.2454, -0.9250,\n",
      "        -0.5918, -0.5709,  0.1012, -0.3125, -0.6084, -0.5707, -0.7392, -0.2877,\n",
      "        -0.5769,  0.2392,  0.1687,  0.1549, -0.1188, -0.0198, -0.6260, -0.5409,\n",
      "         0.1618, -0.6479,  0.0980, -1.0191, -0.6388,  0.1115,  0.1787, -0.3160,\n",
      "        -0.4926, -0.4668, -0.8740, -0.6123, -0.1676, -0.3770, -0.6871, -0.2163,\n",
      "        -0.0069,  0.1515,  0.0702,  0.1283, -0.0605,  0.2042, -0.3288, -1.1212,\n",
      "         0.1129, -0.2685, -0.3196,  0.2626, -0.6530, -0.2968, -0.6689, -0.5631],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1090,  0.1451, -1.0000,  0.0996, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1307,  0.1699,  0.1784,  0.1484,  0.1659,  0.1115,  0.1226, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1039, -1.0000,  0.0980, -1.0000, -1.0000,  0.1323,\n",
      "        -1.0000,  0.1046,  0.1967,  0.1278,  0.1115,  0.1073, -1.0000,  0.1923,\n",
      "         0.1623, -1.0000,  0.1783, -1.0000, -1.0000,  0.1068,  0.1503,  0.1088,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1484,  0.2057, -1.0000, -1.0000,\n",
      "         0.1515,  0.1380,  0.0939,  0.1342,  0.1354,  0.1648, -1.0000, -1.0000,\n",
      "         0.1529,  0.1064, -1.0000,  0.1295, -1.0000,  0.1550,  0.1076, -1.0000])\n",
      "state action values after:  tensor([-6.1878e-01,  2.5723e-01, -5.8185e-01, -4.3351e-01, -1.1259e+00,\n",
      "        -7.8325e-01, -4.0079e-01, -2.2206e-01, -3.4386e-01,  2.3828e-01,\n",
      "        -1.1712e-01, -2.0708e-02, -6.4479e-01,  1.0513e-01,  1.0708e-01,\n",
      "        -6.3775e-01, -1.6318e-01, -2.2497e-01, -2.7481e-01, -2.7580e-01,\n",
      "        -7.3547e-01, -1.0938e+00, -1.2251e-01, -4.8902e-01, -7.4805e-01,\n",
      "         9.6472e-02, -4.0278e-01, -6.4525e-01, -8.9920e-01, -6.4149e-01,\n",
      "         1.4874e-01, -4.3622e-01, -4.6345e-01,  1.6487e-01, -5.7317e-01,\n",
      "        -9.8111e-02, -5.1788e-01, -3.3561e-01, -2.2967e-01, -5.1406e-01,\n",
      "        -6.3034e-01, -2.1121e-01, -1.1429e+00, -4.2925e-01,  1.1570e-01,\n",
      "        -7.0299e-01, -5.6260e-01, -6.1438e-01, -3.6223e-01, -1.4940e-01,\n",
      "         1.2589e-01, -5.2662e-04,  5.8587e-02, -6.4268e-01,  1.0978e-01,\n",
      "        -3.0076e-01, -4.4525e-01, -5.6617e-01, -1.2251e-01, -1.8533e-01,\n",
      "        -6.3387e-02,  1.6925e-01, -3.4750e-01,  1.6550e-01],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1295, -1.0000,  0.0961, -1.0000, -1.0000,  0.1656, -1.0000,\n",
      "        -1.0000,  0.1784,  0.1622,  0.0598, -1.0000,  0.1039,  0.1068, -1.0000,\n",
      "         0.0530,  0.1596,  0.1541,  0.1550, -1.0000, -1.0000,  0.1115, -1.0000,\n",
      "        -1.0000,  0.0996,  0.1659, -1.0000, -1.0000, -1.0000,  0.1278, -1.0000,\n",
      "        -1.0000,  0.1623, -1.0000,  0.1307,  0.1923, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1473, -1.0000, -1.0000,  0.2005, -1.0000,  0.1699, -1.0000,\n",
      "         0.2057,  0.1484,  0.1451,  0.1214,  0.1184,  0.0532,  0.1529,  0.1512,\n",
      "        -1.0000,  0.1098,  0.1115,  0.1637,  0.1316,  0.1957, -1.0000,  0.1141])\n",
      "state action values after:  tensor([-1.2896e-01, -6.3159e-01,  1.2710e-01, -6.6854e-01, -2.2472e-01,\n",
      "        -3.6802e-01, -6.4399e-01, -1.0578e+00, -4.4984e-01, -3.2681e-01,\n",
      "         2.0503e-01, -5.8609e-01, -2.0830e-01, -3.3953e-01, -2.0708e-01,\n",
      "         1.4387e-01, -5.7666e-01,  7.8212e-02, -6.1931e-01, -3.0353e-01,\n",
      "        -4.5862e-01, -3.6803e-01, -1.1620e+00, -3.3736e-01, -2.7868e-01,\n",
      "        -1.1144e+00, -4.7797e-01, -7.1143e-01, -7.1628e-01, -3.1174e-01,\n",
      "        -1.6720e-01, -4.3820e-01, -3.6785e-01, -4.7428e-01,  1.7725e-01,\n",
      "         1.1111e-01, -1.1297e-01,  1.6754e-01,  1.1451e-01, -3.7053e-01,\n",
      "        -5.7208e-01, -2.6273e-01, -5.6074e-01,  1.1789e-01, -1.2281e-01,\n",
      "         1.4244e-01,  8.5385e-02, -5.4582e-01, -2.5028e-01,  5.2954e-04,\n",
      "        -1.3747e-02, -6.0571e-01,  1.0334e-01, -5.0370e-01, -3.8399e-01,\n",
      "         1.6264e-01, -1.0848e+00, -6.4912e-01, -6.2869e-01, -2.6243e-01,\n",
      "        -6.4949e-01, -6.3159e-01, -9.4916e-01, -8.8410e-01],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1484, -1.0000,  0.2005, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1323,  0.1529, -1.0000,  0.1596, -1.0000,  0.1473,  0.1957,\n",
      "         0.1502,  0.0939, -1.0000, -1.0000,  0.0961, -1.0000, -1.0000, -1.0000,\n",
      "         0.1512, -1.0000, -1.0000, -1.0000, -1.0000,  0.1088,  0.0530, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1562,  0.1022,  0.1622,  0.1623,  0.1516, -1.0000,\n",
      "        -1.0000,  0.1484, -1.0000,  0.1451,  0.1115,  0.1278,  0.1783, -1.0000,\n",
      "         0.1550,  0.1515,  0.1073, -1.0000,  0.1068, -1.0000,  0.1656,  0.1503,\n",
      "        -1.0000,  0.0532, -1.0000,  0.1541, -1.0000, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.9289, -0.1248, -0.5559,  0.1085, -0.7093, -0.2617, -0.5907, -0.6238,\n",
      "         0.1568, -0.6614, -0.6432, -0.4364, -1.0760, -0.6518,  0.0247,  0.0815,\n",
      "         0.0768, -0.1715, -0.7356, -0.8746, -0.5902, -0.3089, -0.5046, -0.0116,\n",
      "        -0.6850, -0.0214, -0.5048, -0.1961, -0.3457, -0.7292, -0.3782,  0.1385,\n",
      "        -0.7269, -0.3151, -0.3000, -0.3911,  0.1735, -0.5850, -0.3250, -0.9324,\n",
      "        -0.4822, -0.3619,  0.1828, -0.1359,  0.0023,  0.2109, -0.6615, -0.5740,\n",
      "        -0.5837,  0.2227, -0.0565, -0.1248,  0.1545, -1.1646, -1.1751,  0.1413,\n",
      "         0.1528, -0.6262, -0.3867,  0.1840, -0.7269, -0.0977, -0.3288, -0.2067],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1115, -1.0000,  0.1342, -1.0000,  0.1512, -1.0000, -1.0000,\n",
      "         0.1064, -1.0000,  0.0980, -1.0000, -1.0000,  0.0532,  0.1060,  0.1783,\n",
      "         0.1777,  0.0530, -1.0000, -1.0000, -1.0000,  0.1088, -1.0000,  0.1073,\n",
      "        -1.0000,  0.0598, -1.0000,  0.1596, -1.0000, -1.0000, -1.0000,  0.1278,\n",
      "        -1.0000,  0.1108,  0.1064, -1.0000,  0.1586, -1.0000,  0.0880, -1.0000,\n",
      "         0.0961, -1.0000,  0.1777,  0.1090,  0.1214,  0.1606, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1784,  0.1316,  0.1115,  0.1321, -1.0000, -1.0000,  0.1318,\n",
      "         0.1967, -1.0000, -1.0000,  0.1562, -1.0000,  0.1307,  0.2057,  0.1473])\n",
      "state action values after:  tensor([-0.7304, -0.6506, -0.3611,  0.0809, -0.1872,  0.1016, -0.4847, -0.6442,\n",
      "        -0.6676, -0.2924, -1.1063, -0.4615, -0.7214, -0.1244, -0.2160,  0.0878,\n",
      "        -0.1261,  0.1144, -0.2014, -0.3193, -0.6201, -0.5907, -0.0395, -0.1244,\n",
      "        -0.3617, -0.4753, -1.0907, -0.9664,  0.1421, -0.6506, -0.3470, -1.1447,\n",
      "         0.1366, -0.6334, -0.3642, -0.5716, -0.5045, -0.6252, -0.2350, -0.4145,\n",
      "        -0.7387,  0.0062, -0.1011, -0.7562,  0.1687, -0.1940, -0.3146, -0.7392,\n",
      "        -1.1231, -0.6139, -0.6476, -0.4522, -0.4696, -0.7603, -0.7515, -0.6758,\n",
      "        -0.6671,  0.2144,  0.1477, -0.1907, -0.3375,  0.0324,  0.1078, -0.3502],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.0980,  0.1323,  0.1783,  0.1596,  0.1342,  0.2007, -1.0000,\n",
      "        -1.0000,  0.1484, -1.0000, -1.0000, -1.0000,  0.1115,  0.1550,  0.0939,\n",
      "         0.1090,  0.1516,  0.1473,  0.2057, -1.0000, -1.0000,  0.1354,  0.1115,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1380, -1.0000,  0.1659, -1.0000,\n",
      "         0.1278, -1.0000,  0.1656,  0.1098,  0.0961, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1214,  0.1484, -1.0000,  0.1697, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1606,  0.1064,  0.1637, -1.0000,  0.1884,  0.1529, -1.0000])\n",
      "state action values after:  tensor([ 7.8575e-04,  1.0743e-01, -1.7614e-01, -7.5813e-01, -1.2238e-01,\n",
      "        -6.5824e-01, -2.3179e-01, -3.0316e-01, -6.9860e-01, -3.3692e-01,\n",
      "        -3.1132e-01,  8.3878e-02, -7.7424e-01,  1.3053e-01,  1.3645e-01,\n",
      "        -6.5307e-01, -2.9739e-01, -4.7139e-01, -7.7756e-01, -1.1944e+00,\n",
      "        -6.1583e-01, -3.1774e-01,  1.9044e-01,  1.3684e-01, -2.8910e-02,\n",
      "         9.5757e-02, -6.5093e-01, -5.1734e-01, -1.1723e-01, -4.2691e-02,\n",
      "        -4.5943e-01, -1.1158e+00, -5.8315e-01, -6.7141e-01, -3.7396e-01,\n",
      "        -8.6955e-01, -1.1275e+00, -9.0675e-02,  1.5046e-01, -9.7843e-02,\n",
      "        -3.2810e-01, -5.4857e-01, -1.2238e-01, -3.3692e-01,  1.6574e-01,\n",
      "        -3.8378e-01,  1.7737e-01, -7.3364e-01, -3.6445e-01, -7.3121e-01,\n",
      "         2.1194e-01, -4.7811e-01, -1.7972e-01, -6.6309e-01,  1.8361e-01,\n",
      "         1.2424e-02, -1.9192e-01, -6.5749e-01, -4.6789e-01,  1.6000e-01,\n",
      "        -6.5319e-01, -2.8492e-01,  1.5757e-01,  9.5981e-02],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1073,  0.1022,  0.0530, -1.0000,  0.1115, -1.0000, -1.0000,  0.1484,\n",
      "        -1.0000,  0.1659,  0.2057,  0.1184, -1.0000,  0.0996,  0.1278,  0.0532,\n",
      "         0.1088, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1562,  0.1380,\n",
      "         0.1354,  0.0939, -1.0000, -1.0000,  0.1090,  0.1316, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1323, -1.0000, -1.0000,  0.1484,  0.1141,  0.1622,\n",
      "         0.1108, -1.0000,  0.1115,  0.1659,  0.1586, -1.0000,  0.1648, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1046,  0.2007,  0.1596, -1.0000,  0.1529,  0.1214,\n",
      "        -1.0000,  0.1502, -1.0000,  0.1957,  0.0980,  0.1286,  0.2005,  0.1342])\n",
      "state action values after:  tensor([-0.0861, -0.3559,  0.2087, -0.6404, -0.2184, -0.6090, -0.7703,  0.1603,\n",
      "         0.1290, -0.9734,  0.1936, -0.6800, -0.2019, -0.3035, -0.5844, -0.7711,\n",
      "        -0.8068, -0.7656, -0.2862, -0.6687, -0.6846, -0.5661,  0.0920,  0.1463,\n",
      "        -0.3216, -0.4663, -0.5009,  0.1342, -0.7374, -0.1092,  0.0465, -0.7458,\n",
      "        -0.1210, -0.9616, -0.6508, -0.3220,  0.2178, -0.5712, -0.6170, -0.1893,\n",
      "        -0.7795, -0.1792,  0.2266,  0.1652, -0.7582,  0.0462, -0.9016,  0.1032,\n",
      "         0.0175, -0.7260, -0.3363, -0.6193, -0.5455, -0.4600,  0.2004, -0.0195,\n",
      "         0.1058, -1.1306,  0.2073, -0.1937, -0.3216, -0.2323, -0.5885, -0.5885],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1307,  0.0880,  0.0797, -1.0000,  0.1541, -1.0000, -1.0000,  0.1957,\n",
      "         0.1380, -1.0000,  0.1562,  0.1502,  0.1226,  0.2057, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1286, -1.0000, -1.0000,  0.1098,  0.1451,  0.1623,\n",
      "        -1.0000, -1.0000,  0.1699,  0.1278, -1.0000,  0.1090,  0.1884, -1.0000,\n",
      "         0.1115, -1.0000,  0.0532, -1.0000,  0.1606, -1.0000, -1.0000,  0.1637,\n",
      "        -1.0000,  0.0530,  0.1295,  0.2005, -1.0000,  0.1060, -1.0000,  0.0939,\n",
      "         0.1214, -1.0000,  0.1108, -1.0000,  0.0961, -1.0000,  0.1784,  0.1354,\n",
      "         0.1022, -1.0000,  0.1046, -1.0000,  0.1064, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([ 0.1924, -0.1022, -0.3879, -0.0051,  0.2152, -0.5928, -0.6917, -1.2119,\n",
      "         0.1686,  0.1527, -0.6811, -0.6829, -0.8485,  0.1466, -0.5946, -0.5122,\n",
      "         0.0241, -0.2130, -1.1263, -0.1189, -0.3852, -0.3116,  0.1620,  0.1547,\n",
      "         0.0840,  0.0545,  0.0759, -0.1970, -0.7566, -0.3746,  0.0551, -0.1793,\n",
      "         0.1919,  0.1118, -0.4656,  0.0948, -0.7026, -0.3479, -0.3194,  0.1119,\n",
      "        -0.5894, -0.3029, -1.2025,  0.2194, -0.2861, -0.2319, -0.9085, -0.7419,\n",
      "        -0.6188,  0.1480, -0.4805, -0.0083, -0.6374, -0.9157,  0.1682, -0.6748,\n",
      "        -0.4749, -0.1763, -0.6048, -0.7874, -0.5635, -0.7504, -0.9748,  0.0854],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1784,  0.1090, -1.0000,  0.0598,  0.1606, -1.0000, -1.0000, -1.0000,\n",
      "         0.1648,  0.1967, -1.0000, -1.0000, -1.0000,  0.1957, -1.0000, -1.0000,\n",
      "         0.1214,  0.1541, -1.0000,  0.1115,  0.1323,  0.1484,  0.1957,  0.0996,\n",
      "         0.1342, -1.0000,  0.1783,  0.1226, -1.0000, -1.0000,  0.1884,  0.0530,\n",
      "         0.1562,  0.0939,  0.2007,  0.1068, -1.0000,  0.1656,  0.1659,  0.1529,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1295,  0.1286, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1697, -1.0000,  0.1354, -1.0000, -1.0000,  0.1529, -1.0000,\n",
      "        -1.0000,  0.1473, -1.0000, -1.0000,  0.0961, -1.0000, -1.0000,  0.1451])\n",
      "state action values after:  tensor([ 0.1987, -0.4590, -1.1168,  0.0643, -0.5921, -0.9475, -0.9278, -0.6313,\n",
      "         0.1193, -0.1677, -0.3462,  0.1260, -0.6140, -0.8051, -0.2326,  0.1682,\n",
      "         0.1887, -0.1967, -0.2935, -0.2319,  0.1034, -0.3139, -0.3523, -0.0751,\n",
      "         0.1654,  0.1670,  0.1660, -0.3139, -0.8829,  0.2108, -0.5012, -0.2846,\n",
      "        -0.5866,  0.1415, -0.5478, -0.4358, -0.6190,  0.1332,  0.0954, -0.7062,\n",
      "        -0.6333, -0.1716, -0.3308,  0.1552, -0.9724,  0.1158, -0.7731, -0.4987,\n",
      "        -0.3668, -0.0652,  0.0497,  0.1251, -0.6719, -0.4868, -0.8552,  0.1635,\n",
      "        -0.9804, -0.3546, -0.8172, -0.7286, -0.5362, -0.8059, -0.1884, -0.5244],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1046, -1.0000, -1.0000,  0.1884, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1064,  0.1473,  0.1656,  0.1516, -1.0000, -1.0000, -1.0000,  0.1648,\n",
      "         0.1562, -1.0000,  0.2057, -1.0000,  0.1022,  0.1659, -1.0000,  0.1307,\n",
      "         0.1957,  0.0996,  0.2005,  0.1659, -1.0000,  0.1606, -1.0000,  0.1286,\n",
      "        -1.0000,  0.1697, -1.0000, -1.0000, -1.0000,  0.1318,  0.1068, -1.0000,\n",
      "        -1.0000,  0.1550,  0.1064,  0.1967, -1.0000,  0.1529, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1484,  0.1515,  0.1321, -1.0000, -1.0000, -1.0000,  0.1586,\n",
      "        -1.0000,  0.0880, -1.0000, -1.0000, -1.0000, -1.0000,  0.1226, -1.0000])\n",
      "state action values after:  tensor([-0.9843, -0.4916, -0.9218, -0.6952,  0.1143, -0.8334, -0.7770,  0.1647,\n",
      "         0.1539, -0.0702, -0.2990, -1.1217,  0.1561, -0.5885,  0.1361, -0.6399,\n",
      "        -0.5239,  0.1384, -0.9674, -0.6428, -0.9206, -0.1143, -0.4963, -0.8674,\n",
      "         0.0134,  0.2070, -0.6809,  0.0767, -0.6254, -0.7486, -0.6016, -0.4938,\n",
      "        -0.3930,  0.0976, -0.7163,  0.1331, -0.3612, -0.8284,  0.1458,  0.1682,\n",
      "        -1.2240,  0.1201, -0.3356, -0.2890, -0.7004, -0.3104,  0.0761, -0.6133,\n",
      "        -0.6107, -0.1821,  0.1315, -0.2915, -0.7997, -0.0126, -0.9739, -0.3514,\n",
      "         0.1038, -0.3455, -0.8794, -0.1629, -0.7567, -0.0621,  0.1172, -0.5133],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1699, -1.0000, -1.0000,  0.1380, -1.0000, -1.0000,  0.2005,\n",
      "         0.1957,  0.1307, -1.0000, -1.0000,  0.1529, -1.0000,  0.1623,  0.0532,\n",
      "        -1.0000,  0.1318, -1.0000, -1.0000, -1.0000,  0.1115, -1.0000, -1.0000,\n",
      "         0.1354,  0.1295, -1.0000,  0.1451, -1.0000,  0.1502,  0.0961, -1.0000,\n",
      "        -1.0000,  0.1068, -1.0000,  0.1516, -1.0000, -1.0000,  0.1617,  0.1648,\n",
      "        -1.0000,  0.1529, -1.0000,  0.1088, -1.0000,  0.1484,  0.1342, -1.0000,\n",
      "        -1.0000,  0.0530,  0.1278,  0.2057, -1.0000,  0.1316, -1.0000,  0.1108,\n",
      "         0.1022,  0.1656, -1.0000,  0.1596, -1.0000,  0.1622,  0.1321, -1.0000])\n",
      "state action values after:  tensor([ 0.1093, -0.3751,  0.0289, -0.2158, -0.6058,  0.1656, -0.9043, -0.2963,\n",
      "        -0.4423, -1.2036,  0.2025,  0.1947,  0.1749, -0.9841, -1.1348, -0.6814,\n",
      "         0.0716, -0.6728, -0.5361, -0.1101, -0.5136, -0.9907, -0.4978, -0.1101,\n",
      "        -0.3607,  0.0450,  0.0828, -0.8579, -0.3886, -0.3557, -0.6624,  0.1587,\n",
      "         0.0260, -0.6515, -0.2045, -0.5490, -0.5445,  0.0743, -0.7107,  0.0334,\n",
      "         0.1412, -0.4020,  0.1623, -1.2270, -0.5821, -0.9537, -0.3236, -0.7319,\n",
      "        -0.5379,  0.1924,  0.0783, -0.8388, -0.3077,  0.1328, -0.2789,  0.2055,\n",
      "        -0.1826,  0.1149, -1.1255, -0.6583, -0.8580, -0.1768, -0.7554, -0.4116],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1064, -1.0000, -1.0000,  0.1512, -1.0000,  0.2005, -1.0000,  0.2057,\n",
      "        -1.0000, -1.0000,  0.1606,  0.1046,  0.1784, -1.0000, -1.0000, -1.0000,\n",
      "         0.1515, -1.0000, -1.0000,  0.1115, -1.0000, -1.0000,  0.1699,  0.1115,\n",
      "        -1.0000,  0.1214,  0.1884, -1.0000,  0.1323,  0.1108, -1.0000,  0.1957,\n",
      "         0.1786, -1.0000, -1.0000, -1.0000, -1.0000,  0.1451, -1.0000,  0.1073,\n",
      "         0.1516,  0.1083,  0.1967, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.0996,  0.1060, -1.0000, -1.0000,  0.1278,  0.1286,  0.0797,\n",
      "         0.0530,  0.1380, -1.0000, -1.0000, -1.0000,  0.1226, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.6791, -0.8370,  0.1510, -0.9979, -0.4903, -0.8482,  0.0431, -0.6426,\n",
      "         0.1345, -0.2730, -0.0354, -0.2112, -0.4001, -0.8637,  0.2081, -0.3245,\n",
      "         0.1769,  0.0871, -0.4454,  0.1968, -0.9893, -0.4379, -0.3584, -0.8971,\n",
      "        -0.3448, -0.6964, -0.6678, -0.2860, -0.0698, -0.3991,  0.0434,  0.1497,\n",
      "         0.0740, -0.3661, -0.6605, -0.5422, -0.4443, -0.6081, -0.1819, -0.5616,\n",
      "        -0.3088, -0.7517, -0.3897, -0.3281, -0.9929, -0.1038, -0.0520, -0.1777,\n",
      "         0.1390,  0.1723, -0.9219, -0.8095, -0.8832,  0.1063, -0.6876,  0.1037,\n",
      "         0.1815, -0.5107,  0.0310,  0.0554,  0.1688,  0.1463, -0.3457,  0.0097],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.1318, -1.0000,  0.2007, -1.0000,  0.1073, -1.0000,\n",
      "         0.1766,  0.1286,  0.1622, -1.0000,  0.1083, -1.0000,  0.0996, -1.0000,\n",
      "         0.1586,  0.1515,  0.1923,  0.1606, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.0880, -1.0000, -1.0000,  0.1088,  0.1484, -1.0000,  0.1354,  0.1516,\n",
      "         0.1451, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1596, -1.0000,\n",
      "         0.2057, -1.0000, -1.0000, -1.0000, -1.0000,  0.1115,  0.1307,  0.1637,\n",
      "         0.1617,  0.1784, -1.0000, -1.0000, -1.0000,  0.1064, -1.0000,  0.1068,\n",
      "         0.1777,  0.1699,  0.1786,  0.1214,  0.1967,  0.1141,  0.1064,  0.1316])\n",
      "state action values after:  tensor([-0.5510, -0.6490, -0.3033, -0.7400, -0.2673, -0.6830, -0.5286, -0.3865,\n",
      "         0.1710, -0.2812, -0.7087, -0.2331, -0.6544, -1.0047, -0.5150, -0.5220,\n",
      "        -0.0419,  0.1708, -0.4154,  0.1525, -0.8378,  0.0042, -1.0227,  0.1545,\n",
      "        -0.9958,  0.1584,  0.1480,  0.1971, -0.6597, -1.0259, -0.2184, -0.0774,\n",
      "         0.1714, -0.6351, -0.6824,  0.1642,  0.1023, -1.2291, -0.2226, -0.9788,\n",
      "         0.0517, -0.4589, -0.3279,  0.0895,  0.0600, -0.1727, -0.4525,  0.1431,\n",
      "        -0.6296,  0.1210,  0.2236, -1.0142, -0.2043, -0.3413, -0.5869, -0.7612,\n",
      "        -0.7625,  0.1606, -0.3803,  0.1982, -0.3848,  0.1797, -0.3941, -0.1830],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.0980,  0.1484, -1.0000,  0.1286, -1.0000, -1.0000,  0.1323,\n",
      "         0.1784,  0.1088, -1.0000,  0.1512,  0.0961, -1.0000, -1.0000,  0.1699,\n",
      "         0.1307,  0.1562, -1.0000,  0.1503, -1.0000, -1.0000, -1.0000,  0.1318,\n",
      "        -1.0000,  0.0939,  0.1623,  0.1046, -1.0000, -1.0000, -1.0000,  0.1484,\n",
      "         0.1784, -1.0000, -1.0000,  0.1184,  0.1515, -1.0000, -1.0000, -1.0000,\n",
      "         0.1073,  0.1923, -1.0000,  0.1321,  0.1354,  0.1226, -1.0000,  0.1529,\n",
      "         0.0532,  0.1039,  0.0996, -1.0000,  0.1407,  0.1659, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1529, -1.0000,  0.0797, -1.0000,  0.1648,  0.1083,  0.0530])\n",
      "state action values after:  tensor([ 0.1635, -0.7582, -0.2647, -0.1747, -0.3848, -0.6737, -0.7323, -0.1824,\n",
      "         0.2097, -1.0189,  0.1509,  0.1551, -0.6699, -0.9809, -1.1493, -0.7608,\n",
      "        -0.3316,  0.1846,  0.1292,  0.1799, -0.5271,  0.2007,  0.1253, -1.0417,\n",
      "        -0.1084,  0.1817,  0.1910, -0.6665, -0.5430,  0.1654, -0.9760, -0.4415,\n",
      "         0.1835, -0.5938, -0.6545, -1.0547, -0.3531,  0.1833, -0.3808, -0.4847,\n",
      "        -0.1308, -0.0357, -0.6956, -0.7662, -0.4309,  0.1035, -1.0049, -1.0609,\n",
      "        -0.0976, -0.2518, -1.1314, -0.0124,  0.2035, -0.3483,  0.1529,  0.1597,\n",
      "        -0.3821, -0.0811,  0.1561,  0.1146, -0.2370,  0.1535, -0.8950, -0.9422],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1529, -1.0000,  0.1286,  0.1226,  0.1083, -1.0000, -1.0000,  0.0530,\n",
      "         0.1777, -1.0000,  0.1529,  0.2005,  0.0961, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1967,  0.1056,  0.1586,  0.1699, -1.0000,  0.1697, -1.0000,\n",
      "         0.1473,  0.1777,  0.0797, -1.0000,  0.1098,  0.0939, -1.0000, -1.0000,\n",
      "         0.1648, -1.0000, -1.0000, -1.0000, -1.0000,  0.1957, -1.0000, -1.0000,\n",
      "         0.1090,  0.1307, -1.0000, -1.0000, -1.0000,  0.1064, -1.0000,  0.1076,\n",
      "         0.1115,  0.1423, -1.0000,  0.1622, -1.0000,  0.1659,  0.1623,  0.1516,\n",
      "         0.1323,  0.1484,  0.1318,  0.1884,  0.1512,  0.1141, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.7434, -0.2414,  0.0824, -0.9773,  0.1252, -0.7825, -0.0849,  0.0676,\n",
      "        -0.3361, -1.0385, -0.0949, -0.7607, -0.7533, -0.3735, -0.7045, -0.2162,\n",
      "        -1.0736,  0.1494,  0.1136, -1.0100, -0.6234,  0.0658, -0.3549,  0.0916,\n",
      "         0.1887, -1.1927, -0.9945, -1.0768,  0.1886, -0.7487,  0.1922, -0.3749,\n",
      "        -0.7056, -0.3812,  0.0623, -0.8225, -0.9610,  0.0883, -0.9066, -0.6821,\n",
      "        -0.1703, -0.3156,  0.1484, -0.2129, -0.7321, -1.1338, -0.8544, -0.7855,\n",
      "        -0.3695, -1.0550, -0.6584,  0.1262, -0.6889,  0.1408, -0.7763,  0.1532,\n",
      "         0.1914, -0.2154, -0.1379, -0.5235,  0.1571, -1.1473,  0.1137, -0.0961],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1512,  0.1214, -1.0000,  0.1884, -1.0000,  0.1484,  0.1451,\n",
      "         0.2057, -1.0000,  0.1115, -1.0000, -1.0000,  0.1083, -1.0000,  0.1407,\n",
      "         0.1076,  0.1562,  0.1697, -1.0000, -1.0000,  0.1342,  0.1659,  0.1064,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1967, -1.0000,  0.0970,  0.1323,\n",
      "        -1.0000, -1.0000,  0.1073, -1.0000, -1.0000,  0.1354, -1.0000, -1.0000,\n",
      "         0.1637,  0.0880,  0.1278, -1.0000, -1.0000, -1.0000,  0.1502, -1.0000,\n",
      "         0.1108, -1.0000, -1.0000,  0.1515, -1.0000,  0.2005, -1.0000,  0.1141,\n",
      "         0.1957, -1.0000,  0.1090,  0.2007,  0.1623, -1.0000,  0.1022,  0.1473])\n",
      "state action values after:  tensor([-0.0878,  0.1514,  0.1895, -0.7923, -0.3025,  0.0984, -0.8214,  0.0995,\n",
      "        -0.9562, -0.7928, -0.9927, -1.0728, -0.8306, -0.2128, -1.0643, -0.2120,\n",
      "         0.0541, -0.7608, -0.7072, -0.8578, -0.3358,  0.1586,  0.1247, -0.9276,\n",
      "         0.0988, -0.6901, -0.2388,  0.1617, -0.1753, -0.5290, -0.6893, -1.0744,\n",
      "         0.1437,  0.1501, -0.0930,  0.1051, -0.0396, -0.3668, -0.2615,  0.1926,\n",
      "        -1.0057, -0.1369, -0.3712,  0.1691, -0.8920,  0.1684,  0.1662, -0.1976,\n",
      "         0.0812, -0.1698, -0.3879, -0.6670,  0.1410,  0.0556,  0.1543, -1.1438,\n",
      "        -0.7948, -0.2138, -0.9195,  0.0606,  0.1593, -0.3890, -1.1865, -0.6394],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1473,  0.1141,  0.1967, -1.0000,  0.0880,  0.1056, -1.0000,  0.1354,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1541, -1.0000,  0.1407,\n",
      "         0.1321, -1.0000, -1.0000,  0.1502, -1.0000,  0.1503,  0.1060, -1.0000,\n",
      "         0.1783, -1.0000,  0.1512,  0.1516,  0.1226,  0.1699, -1.0000,  0.1076,\n",
      "         0.1766,  0.1278,  0.1115,  0.1068, -1.0000,  0.1108, -1.0000,  0.1957,\n",
      "        -1.0000,  0.1090, -1.0000,  0.1295, -1.0000,  0.1529, -1.0000,  0.1550,\n",
      "         0.1064,  0.1637, -1.0000, -1.0000,  0.1784,  0.0598,  0.1777, -1.0000,\n",
      "        -1.0000,  0.1596, -1.0000,  0.1451,  0.0797,  0.1656, -1.0000,  0.0980])\n",
      "state action values after:  tensor([ 0.1102, -0.0559,  0.0757, -0.8218, -0.4644,  0.0946,  0.0113, -0.7355,\n",
      "        -0.3825, -0.5232, -0.7217, -0.1310, -0.2886, -0.2293, -0.2094, -1.1848,\n",
      "         0.1139, -0.6832,  0.1922,  0.1255,  0.0534,  0.0535, -0.8074, -0.2296,\n",
      "        -0.3559, -0.9693, -0.3605, -1.0928, -0.2550,  0.1280,  0.1754, -1.2105,\n",
      "        -0.4234, -0.9470, -0.9575, -1.0794, -0.2731,  0.1963, -1.0034, -0.2736,\n",
      "        -0.5973,  0.2433, -0.7726, -0.0936, -1.0455, -0.3476, -0.5344, -0.3719,\n",
      "        -1.0686, -0.5188, -1.0642, -0.6317,  0.2623, -0.9295, -0.8929,  0.0488,\n",
      "        -0.3775, -0.7018,  0.1382, -0.1899, -0.7104, -0.7707, -0.6403,  0.1598],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1562, -1.0000,  0.1056, -1.0000,  0.1923,  0.1039,  0.1622, -1.0000,\n",
      "         0.1656,  0.1098, -1.0000,  0.1090,  0.0880,  0.1423,  0.1596, -1.0000,\n",
      "         0.1022, -1.0000,  0.1046,  0.1380,  0.1451,  0.1786, -1.0000,  0.1512,\n",
      "        -1.0000, -1.0000,  0.1108, -1.0000,  0.1088,  0.1784,  0.1586, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1484,  0.1184, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1777, -1.0000,  0.1115, -1.0000,  0.1659, -1.0000, -1.0000,\n",
      "        -1.0000,  0.2007,  0.1076,  0.0980,  0.0996, -1.0000, -1.0000,  0.1342,\n",
      "        -1.0000, -1.0000,  0.1884,  0.1550, -1.0000, -1.0000, -1.0000,  0.1503])\n",
      "state action values after:  tensor([ 0.0490,  0.0862, -1.0184,  0.2452, -0.3597, -1.0681, -0.5389, -0.8201,\n",
      "        -0.6978, -0.4466,  0.1920,  0.1182, -0.2790, -0.7656,  0.0589, -0.3647,\n",
      "        -0.6072, -1.0443, -0.9669, -1.0045,  0.1273, -0.3746,  0.0892,  0.1925,\n",
      "         0.1838, -0.1687, -0.1813, -0.6091, -0.6602, -1.0400, -0.9973, -0.7758,\n",
      "        -0.6851, -0.4173, -0.4016, -1.0812, -0.1770, -1.1821, -0.2749,  0.0558,\n",
      "         0.1163, -0.4523, -0.7023,  0.1438, -0.3668, -0.1174,  0.1899, -0.9312,\n",
      "         0.1476,  0.0108, -0.6390, -0.9122,  0.0911, -0.8426, -0.0219, -0.3434,\n",
      "        -0.1964, -0.2573, -0.7148,  0.1295,  0.1997, -0.0749, -0.3322, -0.9436],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1451,  0.1784, -1.0000,  0.1777, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1923,  0.1967,  0.1784, -1.0000, -1.0000,  0.1316, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1380, -1.0000,  0.1697,  0.1957,\n",
      "         0.0939,  0.0530,  0.1407, -1.0000,  0.0961, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1226, -1.0000,  0.0880,  0.1786,\n",
      "         0.1606,  0.1210, -1.0000,  0.1515,  0.1656,  0.1090,  0.1046, -1.0000,\n",
      "         0.1141,  0.1622, -1.0000, -1.0000,  0.1562,  0.1502,  0.1307,  0.1323,\n",
      "         0.1596,  0.1286, -1.0000,  0.1060,  0.1957, -1.0000,  0.1659, -1.0000])\n",
      "state action values after:  tensor([ 0.0592, -1.0284, -0.3501, -1.0077, -0.6101, -0.8187, -0.0489,  0.1059,\n",
      "        -0.3721, -0.6583,  0.1852, -0.7042, -0.1827, -0.4391, -1.1366,  0.0103,\n",
      "        -0.5711, -0.2553, -0.7999, -0.7019,  0.1096, -1.0638, -0.1011,  0.1430,\n",
      "         0.2027, -1.1028, -0.6041,  0.0384,  0.2437, -0.7438, -0.6167, -0.7056,\n",
      "        -0.8318, -0.6878, -0.4132, -0.5231, -0.7854,  0.1395, -0.1604, -0.3231,\n",
      "         0.0854, -0.9797, -0.4267, -0.9999, -1.0284,  0.2623, -0.6083, -0.9771,\n",
      "        -0.6025, -0.8641,  0.1620,  0.1420,  0.0782, -0.8282,  0.1725, -0.3154,\n",
      "        -0.6461,  0.1922, -0.2952, -0.1554, -0.4117, -0.4843,  0.0329,  0.1718],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1073, -1.0000,  0.1656, -1.0000, -1.0000, -1.0000,  0.1484,  0.0970,\n",
      "        -1.0000, -1.0000,  0.1648, -1.0000,  0.1596,  0.1210, -1.0000,  0.1321,\n",
      "        -1.0000,  0.1484, -1.0000, -1.0000,  0.1354, -1.0000,  0.1115,  0.1318,\n",
      "         0.1957, -1.0000, -1.0000,  0.1342,  0.1777, -1.0000,  0.0532, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1407,  0.1064,\n",
      "         0.1697, -1.0000,  0.1923, -1.0000,  0.1076,  0.0996,  0.0980, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1516,  0.1295,  0.1039, -1.0000,  0.1529,  0.1659,\n",
      "        -1.0000,  0.1957,  0.2057,  0.1550, -1.0000,  0.1699, -1.0000,  0.1586])\n",
      "state action values after:  tensor([-0.6602, -0.9660, -0.1966,  0.1129, -0.2507, -0.6894, -0.8032,  0.0442,\n",
      "        -0.3721, -0.8033, -0.6957,  0.1614, -0.8080, -0.1010, -1.0631, -0.3421,\n",
      "         0.0693, -0.8033, -0.2449, -0.6305, -0.1870,  0.0639, -0.1640, -0.0013,\n",
      "        -0.8019, -0.1315, -0.3715,  0.0922, -0.7030, -0.8078, -0.6987, -0.0856,\n",
      "        -1.0774,  0.1767,  0.1313, -0.7123,  0.0822, -0.6215, -0.7346,  0.0784,\n",
      "         0.0139, -0.1220, -0.8731, -0.0034,  0.1791, -0.2318, -0.9929,  0.0759,\n",
      "        -0.2318, -0.2968, -0.1363, -1.0583, -1.1371, -0.5168, -1.0899, -0.9927,\n",
      "        -0.9360, -0.2710,  0.1037, -0.0364, -0.3593,  0.1393,  0.2569, -0.2051],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000,  0.1637,  0.1022,  0.0880, -1.0000, -1.0000,  0.1064,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1739,  0.1502,  0.1473, -1.0000, -1.0000,\n",
      "         0.1039, -1.0000, -1.0000,  0.0961,  0.1226,  0.1503,  0.1596,  0.1622,\n",
      "        -1.0000,  0.1550, -1.0000,  0.1784, -1.0000, -1.0000, -1.0000,  0.1090,\n",
      "        -1.0000,  0.0939,  0.1380, -1.0000,  0.1606, -1.0000, -1.0000,  0.0797,\n",
      "        -1.0000,  0.1502, -1.0000,  0.1321,  0.1623, -1.0000, -1.0000,  0.1656,\n",
      "         0.1088,  0.1083,  0.1407, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.2057,  0.1354,  0.1307, -1.0000,  0.1318,  0.0996,  0.1423])\n",
      "state action values after:  tensor([-0.6916, -0.4085, -0.5146, -0.3709, -0.3803, -0.2771, -0.6738, -0.3146,\n",
      "         0.1898, -1.0047, -0.7398, -0.7750,  0.0661, -0.9904, -0.2207, -0.2093,\n",
      "        -0.5913, -0.7918, -0.8847, -0.2586, -0.2803, -0.6901, -1.0016, -0.1973,\n",
      "        -0.3984, -0.3078, -1.1691, -0.3326, -0.2380, -1.0728, -0.2816, -0.9865,\n",
      "        -0.6988, -0.4430, -0.6065,  0.0998, -0.4906,  0.0329,  0.1699, -0.0710,\n",
      "        -0.7828,  0.0193, -0.8009,  0.0384,  0.1832, -0.1069,  0.1645,  0.0410,\n",
      "        -0.2404,  0.1390, -0.1485,  0.0690, -0.7853, -0.0184, -0.7048, -0.9710,\n",
      "         0.1814, -0.8519, -0.8075, -0.0141, -0.1529, -0.3272, -0.5806, -0.0501],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1210, -1.0000, -1.0000,  0.1923,  0.1659, -1.0000,  0.1809,\n",
      "         0.1967, -1.0000, -1.0000, -1.0000,  0.1503, -1.0000, -1.0000,  0.1637,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1286, -1.0000, -1.0000,  0.1423,\n",
      "        -1.0000,  0.1064, -1.0000, -1.0000,  0.0880, -1.0000,  0.1083, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1098,  0.1064,  0.1586,  0.1090,\n",
      "        -1.0000,  0.1784, -1.0000,  0.1562,  0.1648,  0.1502,  0.1739,  0.1073,\n",
      "         0.1484,  0.1141,  0.1596,  0.1214,  0.1502,  0.1484, -1.0000, -1.0000,\n",
      "         0.1623, -1.0000, -1.0000,  0.1321,  0.1512,  0.1108,  0.0980,  0.1307])\n",
      "state action values after:  tensor([-0.9591, -0.9033, -0.6851, -0.9424,  0.0424, -0.4829,  0.0524,  0.0142,\n",
      "        -0.2087, -0.6840,  0.1874, -0.3921, -0.5582,  0.1749, -0.5861, -0.2872,\n",
      "        -0.6390, -0.2262, -0.2672, -0.6840,  0.0328, -0.6862, -0.1340, -0.7991,\n",
      "         0.1653,  0.1597,  0.1618,  0.1020, -0.0607,  0.1696, -0.6869, -0.0219,\n",
      "         0.0162,  0.1205, -0.6158, -0.2935,  0.2405,  0.0382, -0.3877,  0.1704,\n",
      "        -0.2334,  0.0494, -0.4319, -0.6090,  0.2201, -0.7943,  0.1959,  0.0908,\n",
      "         0.0409, -0.3886, -0.7943, -0.4222, -0.0559, -0.9499, -0.6348, -0.0013,\n",
      "         0.1087, -0.5649, -0.1449,  0.0729,  0.0899, -0.7611, -0.6897, -1.1110],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000, -1.0000, -1.0000,  0.0970,  0.1098,  0.1039, -1.0000,\n",
      "         0.1088, -1.0000,  0.1967, -1.0000, -1.0000,  0.1184, -1.0000,  0.1286,\n",
      "        -1.0000,  0.0880,  0.1083, -1.0000,  0.1073, -1.0000,  0.1512, -1.0000,\n",
      "         0.1529,  0.1278,  0.1503,  0.1068,  0.1307,  0.1529, -1.0000,  0.1321,\n",
      "         0.1064,  0.1515,  0.0532,  0.1656,  0.0996,  0.1316, -1.0000,  0.1516,\n",
      "         0.1484,  0.1606,  0.2007, -1.0000,  0.1777, -1.0000,  0.1957,  0.1295,\n",
      "         0.1656, -1.0000, -1.0000,  0.1699,  0.1090, -1.0000, -1.0000,  0.1784,\n",
      "         0.1060,  0.0980, -1.0000, -1.0000,  0.1783,  0.1502, -1.0000, -1.0000])\n",
      "state action values after:  tensor([ 0.0221, -0.4753, -0.9193, -0.1453, -1.0730, -0.3803, -0.4723,  0.1108,\n",
      "        -0.8092,  0.1032,  0.1758,  0.0174, -0.2265, -0.9554, -0.2999, -0.9231,\n",
      "        -0.3596,  0.0218, -0.1168, -0.9026, -0.7327,  0.0873, -0.9145, -0.6798,\n",
      "         0.0731,  0.1032, -0.2138, -1.0184, -1.1373, -0.9089, -0.3782, -0.9070,\n",
      "         0.1302, -0.7898, -1.0486, -0.9828, -0.0705, -0.9899, -0.7757, -0.6827,\n",
      "        -0.7773,  0.0327, -0.9990,  0.0290,  0.1619, -0.1614, -0.9499,  0.1850,\n",
      "        -0.2768, -0.0311, -0.2630, -0.4155,  0.1340, -0.0425, -0.9677,  0.1327,\n",
      "         0.0026, -0.8324, -0.2534, -0.2884,  0.2347,  0.0603,  0.0443, -0.1832],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1784, -1.0000, -1.0000,  0.0530, -1.0000, -1.0000,  0.1098,  0.1022,\n",
      "        -1.0000,  0.1068,  0.1046,  0.0598,  0.1484, -1.0000,  0.1323,  0.1076,\n",
      "        -1.0000,  0.0970,  0.1512, -1.0000, -1.0000,  0.1783, -1.0000, -1.0000,\n",
      "         0.1503,  0.1060,  0.0880, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1380, -1.0000, -1.0000, -1.0000,  0.1550, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1697, -1.0000,  0.1777,  0.1503, -1.0000, -1.0000,  0.1967,\n",
      "         0.1656,  0.1321, -1.0000,  0.2007,  0.1766,  0.1090, -1.0000,  0.1141,\n",
      "         0.1562, -1.0000,  0.1083,  0.1064,  0.0996,  0.1786,  0.1039,  0.1423])\n",
      "state action values after:  tensor([-0.0615,  0.0204,  0.1117,  0.0877, -0.0618, -0.7346, -0.2301, -0.3650,\n",
      "        -0.0999, -0.6918,  0.1665, -0.6815, -0.2879,  0.2098,  0.0225, -0.3671,\n",
      "        -0.2140, -0.1066, -0.0994, -0.6290,  0.0038, -0.0323, -0.9284, -0.8948,\n",
      "        -0.0407, -0.7121, -0.8584,  0.1022, -0.8946, -0.5278, -0.6963, -0.5595,\n",
      "         0.2232, -0.8757, -0.6755, -0.3037,  0.0679, -0.2782, -0.2950, -0.6130,\n",
      "         0.0058, -0.9378,  0.1818, -0.3992, -0.0193, -0.8956, -0.8166, -0.6241,\n",
      "        -0.3877, -0.2900, -0.7769, -1.1540,  0.1483, -0.9507, -0.0448, -0.1820,\n",
      "        -0.8091, -0.0658, -1.1361,  0.1034, -0.3732, -0.1794, -0.6772, -0.6755],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1484,  0.1022,  0.1783,  0.1502, -1.0000,  0.1659, -1.0000,\n",
      "         0.1617, -1.0000,  0.1739, -1.0000,  0.1323,  0.1957,  0.1316,  0.1210,\n",
      "         0.1484,  0.1596,  0.1512, -1.0000,  0.0598, -1.0000, -1.0000, -1.0000,\n",
      "         0.1321, -1.0000, -1.0000,  0.1884, -1.0000,  0.0980, -1.0000, -1.0000,\n",
      "         0.0996, -1.0000, -1.0000,  0.1286,  0.1354, -1.0000, -1.0000,  0.0532,\n",
      "         0.0970, -1.0000,  0.1648,  0.2007,  0.1451,  0.1076, -1.0000, -1.0000,\n",
      "         0.1699,  0.1809, -1.0000, -1.0000,  0.0939, -1.0000,  0.1622,  0.1088,\n",
      "        -1.0000,  0.1056, -1.0000,  0.1515, -1.0000, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([ 1.8375e-01,  1.6619e-01, -3.5557e-01, -2.9230e-01,  1.6677e-01,\n",
      "        -6.7488e-01, -3.1369e-01, -8.7108e-02, -7.8009e-01,  1.8136e-01,\n",
      "        -3.0035e-01, -1.4114e-02, -1.0192e+00, -6.8029e-01, -8.1005e-01,\n",
      "        -4.4948e-01, -6.4879e-01, -6.7060e-01, -5.9131e-02, -1.0909e+00,\n",
      "        -2.5139e-02,  2.1312e-01, -9.8537e-01, -7.3548e-01, -8.4348e-01,\n",
      "        -5.6041e-01, -1.7033e-01, -3.8097e-01, -2.8740e-01, -7.5038e-01,\n",
      "        -6.9556e-01,  1.7612e-01,  8.2265e-02, -6.9360e-01, -1.0180e-01,\n",
      "         1.0753e-01, -8.1244e-01, -2.7568e-01, -1.0051e+00, -1.3368e-01,\n",
      "        -1.7740e-01, -8.0330e-01, -3.5304e-01, -9.9148e-01, -3.6929e-01,\n",
      "        -2.0829e-01, -6.4849e-01, -7.2964e-01, -1.1296e+00, -6.2496e-01,\n",
      "        -9.9251e-01, -1.5776e-01,  1.4565e-01, -4.1604e-01, -8.7069e-04,\n",
      "        -8.8240e-01, -9.0577e-01,  1.3884e-01, -9.0918e-01, -9.2280e-02,\n",
      "        -6.0936e-01, -2.1390e-01, -1.4096e-02, -9.3296e-01],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1648,  0.1278, -1.0000,  0.1108,  0.1503, -1.0000,  0.1286,  0.1056,\n",
      "        -1.0000,  0.1623,  0.1541,  0.1090, -1.0000, -1.0000, -1.0000,  0.1098,\n",
      "        -1.0000, -1.0000,  0.1622, -1.0000,  0.1784,  0.1957, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.2007, -1.0000, -1.0000, -1.0000,  0.1046,\n",
      "         0.1060, -1.0000,  0.1307,  0.1068, -1.0000,  0.1809, -1.0000,  0.0530,\n",
      "        -1.0000, -1.0000,  0.1210, -1.0000,  0.1699,  0.1226, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1423,  0.1184, -1.0000,  0.1777, -1.0000,\n",
      "        -1.0000,  0.0939, -1.0000,  0.1596,  0.0532,  0.1083,  0.0970, -1.0000])\n",
      "state action values after:  tensor([-0.6658, -0.4973, -0.6423, -0.1689, -0.9968, -0.2889, -0.8628, -1.0918,\n",
      "         0.1375, -0.5477,  0.0512, -1.0036, -1.1190, -0.2624, -0.5332, -1.0372,\n",
      "         0.1092,  0.0466,  0.2051, -0.6695, -0.9257, -0.1576, -0.1672,  0.1795,\n",
      "        -0.8956, -0.6727, -0.8109,  0.0866, -0.7250, -0.1533, -0.0289, -0.2221,\n",
      "        -0.7741, -0.3383,  0.1536, -0.0045, -0.6134, -0.7948, -0.9919, -0.8135,\n",
      "        -0.3491, -1.0264, -0.2017, -0.0845, -0.9929, -0.9005, -0.7149, -0.0781,\n",
      "         0.2132,  0.0850, -0.4193, -0.0208, -0.7434, -1.0122, -0.3664,  0.0869,\n",
      "        -0.5341,  0.1661, -0.3614, -0.1533, -0.6795, -0.1419, -1.1285, -0.0086],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1184, -1.0000,  0.0946, -1.0000, -1.0000,  0.1323, -1.0000, -1.0000,\n",
      "         0.1068,  0.1354,  0.0996, -1.0000, -1.0000,  0.2057,  0.0880,  0.1623,\n",
      "        -1.0000,  0.1502, -1.0000,  0.1503, -1.0000,  0.1115,  0.0970,  0.1656,\n",
      "        -1.0000,  0.1210,  0.1318,  0.1697, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1963,  0.2005, -1.0000, -1.0000, -1.0000,  0.1596,\n",
      "         0.1957,  0.1884, -1.0000,  0.1407, -1.0000, -1.0000, -1.0000,  0.1783,\n",
      "         0.0961,  0.1503,  0.2007,  0.1115, -1.0000,  0.1617, -1.0000,  0.1606])\n",
      "state action values after:  tensor([-0.0274, -1.0972, -0.4238, -1.1137,  0.1673, -0.4758, -0.0462, -0.8495,\n",
      "        -0.0682, -1.1260,  0.1659, -0.9893, -0.0255,  0.1718, -0.8811, -0.1939,\n",
      "        -0.7590, -1.0572, -0.4520, -1.0425, -0.2778, -0.1217, -0.8931,  0.1237,\n",
      "        -1.0660, -0.7174, -0.2793,  0.1986, -0.0489, -1.0058, -0.0146,  0.1783,\n",
      "        -0.7228, -0.9932, -1.0957, -0.0558, -0.6675, -0.6256, -0.5692, -0.1242,\n",
      "        -0.8552, -0.1466, -0.6085, -0.9995, -1.0217, -0.7545, -0.3202, -0.6943,\n",
      "        -0.1802, -0.9890,  0.0655, -0.1931, -0.6338,  0.1891, -0.0520, -0.6590,\n",
      "        -0.1400, -0.0208, -0.2839,  0.0097, -0.5440, -0.2331,  0.0723,  0.0701],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1656, -1.0000,  0.1098, -1.0000,  0.1529,  0.0980,  0.1512, -1.0000,\n",
      "         0.1321, -1.0000,  0.1278, -1.0000,  0.0797,  0.1046, -1.0000,  0.1473,\n",
      "        -1.0000,  0.1499, -1.0000, -1.0000,  0.1637,  0.0530, -1.0000,  0.1380,\n",
      "        -1.0000, -1.0000,  0.1108,  0.0996, -1.0000, -1.0000,  0.1606,  0.1777,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1064, -1.0000, -1.0000, -1.0000,  0.1307,\n",
      "        -1.0000,  0.1423, -1.0000, -1.0000, -1.0000, -1.0000,  0.1541, -1.0000,\n",
      "         0.1659, -1.0000,  0.1786,  0.1484, -1.0000,  0.1586,  0.1562, -1.0000,\n",
      "         0.2057,  0.1777, -1.0000,  0.1214, -1.0000, -1.0000,  0.1515,  0.1060])\n",
      "state action values after:  tensor([-0.0517, -0.1968, -0.9766, -0.8110, -0.1607, -0.6468, -0.0995, -0.2540,\n",
      "        -0.3972,  0.1221, -0.9985, -0.9844, -0.1163, -0.6801, -0.5584, -0.7002,\n",
      "        -0.0665, -0.9305, -0.1644, -0.0161, -0.9115, -0.7808,  0.1215, -0.0533,\n",
      "        -0.5617,  0.0379, -0.9844, -0.0947, -1.1030, -0.2395, -0.3272, -0.1926,\n",
      "        -0.1946, -0.6323, -0.4091,  0.2015, -0.1130, -0.4164, -0.7218, -1.1010,\n",
      "        -0.7750, -0.6663, -0.8598, -0.3375, -0.4568,  0.0779, -0.9598, -0.7512,\n",
      "         0.1728,  0.0245, -0.1898, -0.0702, -1.0964,  0.1137, -0.0402,  0.1267,\n",
      "        -1.0464,  0.1656,  0.1899, -0.6152, -1.0066,  0.0958,  0.0268, -0.7327],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1596,  0.1473, -1.0000, -1.0000,  0.1115, -1.0000, -1.0000,  0.1323,\n",
      "        -1.0000,  0.1380, -1.0000, -1.0000,  0.0530, -1.0000, -1.0000, -1.0000,\n",
      "         0.1064, -1.0000,  0.0880,  0.1295, -1.0000, -1.0000,  0.1215,  0.1451,\n",
      "        -1.0000,  0.1354, -1.0000,  0.1784, -1.0000, -1.0000,  0.2007,  0.1484,\n",
      "         0.1083, -1.0000,  0.1098,  0.1516, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.0980,  0.1884, -1.0000, -1.0000,\n",
      "         0.1623,  0.1550,  0.1656, -1.0000, -1.0000,  0.1068,  0.0970,  0.1184,\n",
      "        -1.0000,  0.1503,  0.1586, -1.0000, -1.0000,  0.1503,  0.0946, -1.0000])\n",
      "state action values after:  tensor([ 0.2110,  0.1712,  0.1961, -1.0968, -0.3264,  0.0861, -0.0318, -0.8641,\n",
      "         0.0682, -0.4626,  0.1653, -0.2279, -0.1893, -0.6272,  0.1654, -0.1548,\n",
      "         0.1881,  0.2021, -0.0765, -0.0317, -0.8729,  0.1571, -0.1621,  0.0390,\n",
      "        -0.1538, -0.6231, -0.9249, -0.0420, -1.0146, -0.2771, -0.0224, -0.5447,\n",
      "        -1.0389, -0.2605, -0.0634, -0.2272, -0.6061, -0.4325, -0.7862, -0.1548,\n",
      "        -0.8560, -0.1334,  0.1803, -1.0943, -0.5987, -0.3139, -0.6314, -0.3937,\n",
      "         0.1242, -0.2874, -0.0864, -0.6588, -0.9920, -0.0178, -0.7514,  0.0292,\n",
      "        -1.0960, -0.7117, -0.6350, -1.0314, -0.0853, -1.0054,  0.1216,  0.0012],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1957,  0.1623,  0.0996, -1.0000,  0.1286,  0.1484,  0.1697, -1.0000,\n",
      "         0.1060, -1.0000,  0.1503,  0.1226,  0.1484,  0.1502,  0.1046,  0.1659,\n",
      "         0.1586,  0.1516,  0.1622,  0.1295, -1.0000,  0.1318,  0.1115,  0.1090,\n",
      "         0.1963, -1.0000, -1.0000,  0.1596, -1.0000, -1.0000,  0.1606, -1.0000,\n",
      "        -1.0000,  0.1809,  0.1562,  0.1923, -1.0000, -1.0000, -1.0000,  0.1659,\n",
      "        -1.0000,  0.1307,  0.1967, -1.0000,  0.0532,  0.2007, -1.0000,  0.1098,\n",
      "         0.1184,  0.1637,  0.1342, -1.0000, -1.0000,  0.1512, -1.0000,  0.1407,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.1321, -1.0000,  0.1380,  0.1214])\n",
      "state action values after:  tensor([-0.1595, -0.0395, -0.3474, -0.9480, -0.1786,  0.1233,  0.1595, -0.3488,\n",
      "         0.1767, -0.1054, -0.4832, -0.7922,  0.1094, -0.6312, -0.1294, -0.0683,\n",
      "        -0.6257, -0.3217, -1.0257, -0.2823,  0.2110, -0.7206, -0.2452,  0.0807,\n",
      "         0.0739, -0.8284, -0.6452, -0.8666, -0.2902, -0.2923, -0.6576, -0.1383,\n",
      "        -0.8184, -0.6041,  0.1607,  0.2059, -0.6262, -0.3266, -1.1215, -0.8686,\n",
      "         0.1899, -0.0048, -0.8698, -0.1007, -0.9679, -0.7352,  0.1495, -0.1595,\n",
      "        -0.8604, -0.2902, -0.8894, -0.6353, -1.0833, -0.6996,  0.1802, -1.0276,\n",
      "        -0.9178,  0.0960,  0.1706, -0.1826, -0.1265, -1.0058, -0.9853, -0.2388],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1115,  0.0797,  0.1541, -1.0000,  0.1617,  0.1766,  0.1739, -1.0000,\n",
      "         0.1648,  0.0530, -1.0000, -1.0000,  0.1022, -1.0000,  0.1307,  0.1622,\n",
      "        -1.0000,  0.1286, -1.0000, -1.0000,  0.1957, -1.0000, -1.0000,  0.1783,\n",
      "         0.1060, -1.0000, -1.0000, -1.0000, -1.0000,  0.1210, -1.0000,  0.1963,\n",
      "        -1.0000, -1.0000,  0.1046,  0.1957, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1777,  0.1512, -1.0000,  0.2057, -1.0000, -1.0000,  0.1318,  0.1115,\n",
      "        -1.0000,  0.1699, -1.0000, -1.0000, -1.0000, -1.0000,  0.1586, -1.0000,\n",
      "        -1.0000,  0.1503,  0.1623,  0.1484, -1.0000, -1.0000, -1.0000,  0.1323])\n",
      "state action values after:  tensor([ 0.0926, -0.1277,  0.1629,  0.2117, -0.2809, -1.0734,  0.1023, -0.0554,\n",
      "         0.1710,  0.0818,  0.2107, -0.0959, -0.2940, -0.8223, -0.9115, -0.9405,\n",
      "        -0.4871, -0.6934,  0.2016, -1.0781, -0.7500,  0.1086, -0.9748,  0.2039,\n",
      "        -0.1895, -0.0450, -0.7060,  0.1234, -1.0255, -0.0538,  0.0799, -0.4270,\n",
      "        -0.8656, -0.6688,  0.1767,  0.0023,  0.1185, -0.5678, -0.1300, -0.7641,\n",
      "        -0.3375, -0.1284,  0.1090, -0.9908, -0.1170,  0.1329, -0.1518, -0.2935,\n",
      "        -0.1658, -0.0657, -0.6233, -0.0639, -0.3431, -0.2369, -0.9908, -0.2353,\n",
      "        -1.1182,  0.0211, -0.5506, -0.4600, -0.1779, -0.0857, -0.0958, -0.1014],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1884, -1.0000,  0.1503,  0.0996,  0.1637, -1.0000,  0.1844,  0.1622,\n",
      "         0.1623,  0.1060,  0.1957,  0.1342, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1957, -1.0000, -1.0000,  0.1484, -1.0000,  0.1777,\n",
      "        -1.0000,  0.1777, -1.0000,  0.1380, -1.0000,  0.1656,  0.1783, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1967,  0.1512,  0.1141, -1.0000,  0.1423, -1.0000,\n",
      "        -1.0000,  0.1963,  0.1022, -1.0000,  0.2005,  0.0939,  0.0880, -1.0000,\n",
      "         0.1656,  0.1451, -1.0000,  0.0970, -1.0000,  0.1323, -1.0000,  0.1226,\n",
      "        -1.0000,  0.1697, -1.0000, -1.0000,  0.1473,  0.1064,  0.2057,  0.0530])\n",
      "state action values after:  tensor([-0.6191, -0.0474, -0.1326, -0.1852,  0.1097, -1.1200, -0.7765, -0.0551,\n",
      "         0.1064, -0.9820,  0.1247, -1.0700,  0.1635, -0.3205, -0.4592, -1.0630,\n",
      "        -0.1308, -0.8825, -1.0378,  0.0045, -0.7230,  0.0953, -0.3087, -0.1667,\n",
      "         0.0184, -0.1507,  0.1677, -0.1371, -0.0283, -0.2342, -0.6607,  0.0955,\n",
      "        -0.0470, -0.7045,  0.1563, -0.4584,  0.0590, -0.8740, -0.5673, -0.5914,\n",
      "        -0.7375, -0.6335,  0.1175,  0.1885, -0.1485, -0.7000,  0.1639,  0.2185,\n",
      "        -0.1328, -0.8177, -0.2866, -0.3464, -0.3435,  0.1375, -0.7641,  0.1442,\n",
      "        -0.8401, -0.9916, -0.0291, -0.8411, -0.3435, -0.0300, -1.0497, -0.9357],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.0797,  0.1784,  0.1435,  0.1022, -1.0000,  0.1076,  0.1295,\n",
      "         0.1068, -1.0000,  0.1380, -1.0000,  0.1503, -1.0000, -1.0000, -1.0000,\n",
      "         0.1423, -1.0000, -1.0000,  0.1512, -1.0000,  0.1503, -1.0000,  0.1473,\n",
      "         0.1316, -1.0000,  0.1529,  0.1088,  0.1596,  0.1226, -1.0000,  0.1515,\n",
      "         0.1777, -1.0000,  0.1046, -1.0000,  0.1090, -1.0000, -1.0000,  0.0532,\n",
      "        -1.0000, -1.0000,  0.1141,  0.1516, -1.0000, -1.0000,  0.1257,  0.0996,\n",
      "         0.1659, -1.0000,  0.2007, -1.0000, -1.0000,  0.1318, -1.0000,  0.1529,\n",
      "        -1.0000, -1.0000,  0.1610, -1.0000, -1.0000,  0.0598, -1.0000, -1.0000])\n",
      "state action values after:  tensor([ 0.1182, -0.5990, -0.2878, -0.1476, -1.1123, -1.0680, -0.0410, -0.8349,\n",
      "        -0.4646, -0.1218,  0.1284,  0.0719, -0.0519, -0.0349,  0.0568, -0.3535,\n",
      "        -0.0509, -0.6835, -0.0333, -0.9471, -0.6887, -0.0604,  0.1668,  0.0557,\n",
      "         0.1741, -0.5958, -0.0721,  0.1741, -0.1375, -1.1170, -0.6372,  0.0557,\n",
      "        -0.7461,  0.1858, -0.8630, -0.1497, -0.7841, -1.1171,  0.1111,  0.0115,\n",
      "        -0.7813, -0.7697, -0.8775,  0.1621, -0.7761,  0.1364,  0.0272, -0.1879,\n",
      "        -0.8825,  0.1054, -0.9898, -0.1375, -0.4260, -0.5126, -1.0620, -0.0522,\n",
      "        -0.3619,  0.0120, -0.7698, -0.3590, -0.3495, -0.8731, -0.3248, -0.1606],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1766, -1.0000,  0.2007,  0.1115, -1.0000, -1.0000,  0.1622, -1.0000,\n",
      "        -1.0000,  0.1963,  0.1380,  0.1354,  0.1777,  0.1610,  0.1407, -1.0000,\n",
      "         0.1697, -1.0000,  0.1596, -1.0000, -1.0000,  0.1295,  0.1529,  0.1090,\n",
      "         0.1623,  0.1502,  0.0970,  0.1648,  0.1659, -1.0000, -1.0000,  0.1090,\n",
      "        -1.0000,  0.1516, -1.0000,  0.1784, -1.0000, -1.0000,  0.1484,  0.1039,\n",
      "        -1.0000,  0.1076, -1.0000,  0.1503, -1.0000,  0.0939,  0.1697,  0.1484,\n",
      "        -1.0000,  0.1068, -1.0000,  0.1659, -1.0000, -1.0000, -1.0000,  0.0797,\n",
      "        -1.0000,  0.1214, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.2105, -0.7278,  0.0681, -0.2625, -0.5938, -0.1327, -0.2032, -0.2360,\n",
      "        -0.2858,  0.1747, -0.0772, -0.2650, -0.1858,  0.1656, -0.8801, -0.1399,\n",
      "        -0.5877, -1.0232, -0.6162, -0.0554, -0.6924, -0.9986, -0.6655, -0.0435,\n",
      "        -1.0011,  0.2132, -0.3709, -0.6366, -0.9319, -1.1034, -0.1300, -0.6055,\n",
      "        -0.3699,  0.1635,  0.0582, -1.0841, -0.9894, -0.3612, -0.1374, -0.6914,\n",
      "        -0.9799, -0.8280, -0.6192,  0.1824, -0.6260, -0.3605, -0.8563, -0.4751,\n",
      "         0.1614, -0.7598, -0.1037, -0.1536, -0.1707, -0.3425, -0.3166, -0.4840,\n",
      "        -0.1285,  0.1096, -0.2664, -0.7890, -0.8866, -1.0556, -0.1928, -0.7890],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1617, -1.0000,  0.1354, -1.0000, -1.0000,  0.1423,  0.1923,  0.1323,\n",
      "         0.2007,  0.1967,  0.0970, -1.0000,  0.1435,  0.1529, -1.0000,  0.1659,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1817, -1.0000, -1.0000, -1.0000,  0.0598,\n",
      "        -1.0000,  0.1777, -1.0000, -1.0000, -1.0000, -1.0000,  0.1307, -1.0000,\n",
      "        -1.0000,  0.1278,  0.1407, -1.0000, -1.0000,  0.1541, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1516, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         0.1257,  0.1076,  0.2057,  0.1784,  0.1056, -1.0000,  0.1286, -1.0000,\n",
      "         0.1088,  0.1844, -1.0000, -1.0000, -1.0000, -1.0000,  0.1484, -1.0000])\n",
      "state action values after:  tensor([-0.8683, -0.1929, -0.1551, -1.1223, -0.8452, -0.0475, -0.2293, -1.0191,\n",
      "        -0.7631, -0.5771,  0.1885,  0.1686, -0.0570, -0.1551,  0.0235, -1.0139,\n",
      "        -0.6809, -0.8935, -0.4856, -0.0049, -0.6456, -0.0423, -0.3723, -0.8832,\n",
      "         0.1078, -0.3231, -0.4976,  0.2101,  0.0952, -0.4381,  0.2107,  0.0106,\n",
      "        -0.2033, -0.1778, -0.2341, -0.8259, -0.1560, -0.9545, -0.9603, -0.2952,\n",
      "        -0.0034, -0.2922,  0.1789, -0.8590, -0.2692, -0.8826, -0.1199, -0.6272,\n",
      "        -0.1495, -0.1723, -0.0294,  0.0264, -1.1040, -0.6447, -0.0914, -0.8845,\n",
      "        -0.1495, -0.2922,  0.1593, -0.1780, -1.1096, -1.0078, -0.8029, -0.8884],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([-1.0000,  0.1435,  0.1115, -1.0000, -1.0000,  0.1622, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1502,  0.1957,  0.1648,  0.1777,  0.1115,  0.1502, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1512, -1.0000,  0.1697,  0.1415, -1.0000,\n",
      "         0.1022,  0.1286, -1.0000,  0.1777,  0.1884, -1.0000,  0.0996,  0.1039,\n",
      "         0.1484, -1.0000,  0.1226, -1.0000,  0.1784, -1.0000,  0.1499,  0.1210,\n",
      "         0.1214,  0.1637,  0.1516, -1.0000,  0.1108, -1.0000,  0.1963, -1.0000,\n",
      "         0.1659,  0.0880,  0.1610,  0.1697, -1.0000, -1.0000,  0.1562, -1.0000,\n",
      "         0.1659, -1.0000,  0.1257, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n",
      "state action values after:  tensor([-0.0883, -0.6936, -0.4885, -0.1558, -0.0221, -0.7707, -0.3754, -0.8527,\n",
      "        -0.1880, -0.2223,  0.1425, -0.1367,  0.1850, -1.1049, -1.0270, -0.7803,\n",
      "        -0.9921, -0.8810, -0.1616, -0.3153, -0.2766, -0.9921, -0.7185, -0.3087,\n",
      "        -0.6321, -0.1836, -0.0196, -0.8704, -0.3681, -0.3482, -0.2982, -0.6082,\n",
      "        -0.2196, -0.6233, -0.0700,  0.1028, -0.6565, -0.9497, -0.0554, -0.6156,\n",
      "         0.0439, -0.0421,  0.1091, -0.9138, -0.1478,  0.1600,  0.1809, -0.2197,\n",
      "        -0.9568, -0.7726,  0.0480, -0.0954,  0.1289,  0.1065, -0.5836, -0.8900,\n",
      "         0.1082,  0.1161, -0.5151, -0.7421,  0.1004,  0.1280,  0.1680, -0.2100],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "expected state action values :  tensor([ 0.1342, -1.0000, -1.0000,  0.1115,  0.1610, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.1064,  0.1046,  0.2005,  0.1957, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.1098,  0.1809, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  0.0880,  0.0946, -1.0000,  0.1541, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  0.1656,  0.1141, -1.0000,  0.1499,  0.0598, -1.0000,\n",
      "         0.1090,  0.1295,  0.1484, -1.0000,  0.1423,  0.1278,  0.1623,  0.1617,\n",
      "        -1.0000, -1.0000,  0.1407,  0.1562,  0.1215,  0.1022,  0.0532, -1.0000,\n",
      "         0.1766,  0.1318, -1.0000, -1.0000,  0.1068,  0.1184,  0.1967,  0.1083])\n",
      "Complete\n",
      "\n",
      "Heap 1: |               \t (1)\n",
      "\n",
      "Heap 2:                 \t (0)\n",
      "\n",
      "Heap 3: ||||||          \t (6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10001\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    heaps, _, _ = env.observe()\n",
    "    state = to_input(heaps)\n",
    "    #print(\"state : \", state.shape)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        is_available = env.check_valid(action)\n",
    "        if not is_available: \n",
    "            #i.e. if the action is not valid, we give the agent a negative reward\n",
    "            #print(\"is not available : \")\n",
    "            reward = torch.tensor([-1], device=device)\n",
    "            next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            break\n",
    "            \n",
    "        else : #if the action is valid, we make a step\n",
    "            heaps, done, reward = env.step(action)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == None: \n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else : \n",
    "                #print(\"reward not None : \", reward)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if not done:\n",
    "                next_state = to_input(heaps)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                #episode_durations.append(t + 1)\n",
    "                #plot_durations()\n",
    "                break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play against one player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game end, winner is player 0\n",
      "Optimal player 1 = 0\n",
      "DQN player 2 = 1\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "You cannot take more objects than there are in the heap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=13'>14</a>\u001b[0m     move \u001b[39m=\u001b[39m select_action_target(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=14'>15</a>\u001b[0m \u001b[39m#print(\"move : \", move)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=15'>16</a>\u001b[0m heaps, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(move)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=16'>17</a>\u001b[0m \u001b[39m#env.render()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000011?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n",
      "File \u001b[0;32m~/Documents/GitHub/ANN_NIM/nim_env.py:59\u001b[0m, in \u001b[0;36mNimEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=56'>57</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheap_avail[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mThe selected heap is already empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=57'>58</a>\u001b[0m \u001b[39massert\u001b[39;00m n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou must take at least 1 object from the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=58'>59</a>\u001b[0m \u001b[39massert\u001b[39;00m (n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]), \u001b[39m\"\u001b[39m\u001b[39mYou cannot take more objects than there are in the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n  \u001b[39m# core of the action\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: You cannot take more objects than there are in the heap"
     ]
    }
   ],
   "source": [
    "Turns = np.array([0,1])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    heaps, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0.5, player=Turns[0]) #not so optimal player (best : epsi = 0)\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1]) #our player\n",
    "    #env.render()\n",
    "    while not env.end:\n",
    "        if env.current_player == player_opt_1.player:\n",
    "            move = player_opt_1.act(heaps)\n",
    "        else:\n",
    "            state = to_input(heaps)\n",
    "            move = select_action_target(state)\n",
    "        #print(\"move : \", move)\n",
    "        heaps, end, winner = env.step(move)\n",
    "        #env.render()\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  str(Turns[0]))\n",
    "            print('DQN player 2 = ' +  str(Turns[1]))\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You cannot take more objects than there are in the heap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000018?line=2'>3</a>\u001b[0m Player_DQN \u001b[39m=\u001b[39m DQN_Player(epsilon\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, player \u001b[39m=\u001b[39m Turns[\u001b[39m1\u001b[39m], target_net \u001b[39m=\u001b[39m target_net)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000018?line=3'>4</a>\u001b[0m Player_opt \u001b[39m=\u001b[39m OptimalPlayer(epsilon\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, player\u001b[39m=\u001b[39mTurns[\u001b[39m0\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000018?line=4'>5</a>\u001b[0m reward \u001b[39m=\u001b[39m QL_one_game(Player_DQN, Player_opt, eps \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, eps_opt \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, alpha \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, gamma \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, env \u001b[39m=\u001b[39;49m env, update \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/ANN_NIM/helpers.py:29\u001b[0m, in \u001b[0;36mQL_one_game\u001b[0;34m(playerQL, playerOpt, eps, eps_opt, alpha, gamma, env, update)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/helpers.py?line=26'>27</a>\u001b[0m         heaps_before \u001b[39m=\u001b[39m heaps\u001b[39m.\u001b[39mcopy()    \n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/helpers.py?line=27'>28</a>\u001b[0m         move, action \u001b[39m=\u001b[39m playerQL\u001b[39m.\u001b[39mact(heaps)\n\u001b[0;32m---> <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/helpers.py?line=28'>29</a>\u001b[0m         heaps, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(move)\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/helpers.py?line=30'>31</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/helpers.py?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39mreward(playerQL\u001b[39m.\u001b[39mplayer)\n",
      "File \u001b[0;32m~/Documents/GitHub/ANN_NIM/nim_env.py:59\u001b[0m, in \u001b[0;36mNimEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=56'>57</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheap_avail[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mThe selected heap is already empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=57'>58</a>\u001b[0m \u001b[39massert\u001b[39;00m n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou must take at least 1 object from the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=58'>59</a>\u001b[0m \u001b[39massert\u001b[39;00m (n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]), \u001b[39m\"\u001b[39m\u001b[39mYou cannot take more objects than there are in the heap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n  \u001b[39m# core of the action\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/Anya48/Documents/GitHub/ANN_NIM/nim_env.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaps[h \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: You cannot take more objects than there are in the heap"
     ]
    }
   ],
   "source": [
    "Turns = np.array([0,1])\n",
    "Turns = Turns[np.random.permutation(2)]\n",
    "Player_DQN = DQN_Player(epsilon=0.5, player = Turns[1], target_net = policy_net)\n",
    "Player_opt = OptimalPlayer(epsilon=0.5, player=Turns[0])\n",
    "reward = QL_one_game(Player_DQN, Player_opt, eps = 0, eps_opt = 0, alpha = 0, gamma = 0, env = env, update = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses :  []\n",
      "0\n",
      "loss :  0.022673431783914566\n",
      "500\n",
      "loss :  0.006362982094287872\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "loss :  0.004506346769630909\n",
      "loss :  0.0044937958009541035\n",
      "2500\n",
      "loss :  0.004374979995191097\n",
      "loss :  0.002913573756814003\n",
      "3000\n",
      "loss :  0.0028058644384145737\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "loss :  0.0011636404087767005\n",
      "5000\n",
      "5500\n",
      "loss :  0.0009431239450350404\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "loss :  0.0008056056103669107\n",
      "7500\n",
      "loss :  0.0007395156426355243\n",
      "8000\n",
      "8500\n",
      "loss :  0.000545498332940042\n",
      "9000\n",
      "loss :  0.0005287214880809188\n",
      "9500\n",
      "loss :  0.0007824922213330865\n",
      "loss :  0.0005555802490562201\n",
      "loss :  0.0005575870745815337\n",
      "loss :  0.00043190139695070684\n",
      "loss :  0.0008373021264560521\n",
      "10000\n",
      "loss :  0.0008611102239228785\n",
      "10500\n",
      "loss :  0.0017546917079016566\n",
      "11000\n",
      "loss :  0.0005231364048086107\n",
      "loss :  0.0008728599641472101\n",
      "11500\n",
      "loss :  0.0028638294897973537\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "loss :  0.0008176650735549629\n",
      "14500\n",
      "loss :  0.00042911991477012634\n",
      "15000\n",
      "15500\n",
      "loss :  0.0004916220204904675\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "loss :  0.0002499801921658218\n",
      "loss :  0.0002877411898225546\n",
      "19500\n",
      "Complete\n",
      "\n",
      "Heap 1: |               \t (1)\n",
      "\n",
      "Heap 2: |               \t (1)\n",
      "\n",
      "Heap 3: ||              \t (2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 20000\n",
    "losses = []\n",
    "print(\"losses : \", losses)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    heaps, _, _ = env.observe()\n",
    "    state = to_input(heaps)\n",
    "    #print(\"state : \", state.shape)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        is_available = env.check_valid(action)\n",
    "        if not is_available: \n",
    "            #i.e. if the action is not valid, we give the agent a negative reward\n",
    "            #print(\"is not available : \")\n",
    "            reward = torch.tensor([-1], device=device)\n",
    "            next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            break\n",
    "            \n",
    "        else : #if the action is valid, we make a step\n",
    "            _, done, reward = env.step(action)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == None: \n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else : \n",
    "                #print(\"reward not None : \", reward)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            heaps, _, _ = env.observe()\n",
    "            if not done:\n",
    "                next_state = to_input(heaps)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                #episode_durations.append(t + 1)\n",
    "                #plot_durations()\n",
    "                losses.append(loss)\n",
    "                print(\"loss : \", loss)\n",
    "                break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0011116937967017293, 0.0004791765531990677, 0.0014446345157921314, 0.0006353636854328215, 0.0004516641783993691, 0.0005009748274460435, 0.0006440210854634643, 0.0010924437083303928, 0.0003929889062419534, 0.0006850946228951216, 0.0006849135388620198, 0.0003566378145478666, 0.0007577439537271857, 0.0007167788571678102, 0.00022508430993184447, 0.00020175274403300136, 0.000519493711180985, 0.00035526545252650976, 0.00034041202161461115, 0.00026659510331228375, 0.00025759299751371145, 0.000324135588016361, 0.0002875941863749176, 0.00022826428175903857, 0.0002539473935030401, 0.0002804423857014626, 0.00017615678370930254, 0.00019927072571590543, 0.0002800890651997179]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 29 into shape (250)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Anya48/Documents/GitHub/ANN_NIM/DQN copy.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(losses)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=1'>2</a>\u001b[0m losses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(losses)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=2'>3</a>\u001b[0m aver_losses \u001b[39m=\u001b[39m (losses\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m250\u001b[39;49m))\u001b[39m.\u001b[39mmean(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Anya48/Documents/GitHub/ANN_NIM/DQN%20copy.ipynb#ch0000015?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(aver_losses)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 29 into shape (250)"
     ]
    }
   ],
   "source": [
    "print(losses)\n",
    "losses = np.array(losses)\n",
    "aver_losses = (losses.reshape(-1, 250)).mean(axis = 1)\n",
    "print(aver_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aa7871422cbf9296a09eca5272ae12f42feac121f15077f0682f5d4affb8114"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
