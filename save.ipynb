{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import helpers as h\n",
    "from nim_env import NimEnv, OptimalPlayer, QL_Player\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "plt.rc('text', usetex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dictionnary contains all outputs of all questions of part II\n",
    "# It is saved after e\n",
    "# No need to run again this cell !!!!\n",
    "saving_dict = {}\n",
    "saving_dict['Q1'] = {}\n",
    "saving_dict['Q2'] = {}\n",
    "saving_dict['Q3'] = {}\n",
    "saving_dict['Q4'] = {}\n",
    "saving_dict['Q7'] = {}\n",
    "saving_dict['Q8'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dic):\n",
    "    with open('./Data/saving_dict.pickle', 'wb') as f:\n",
    "        pickle.dump(dic, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_dict(dic_name):\n",
    "    with open('./Data/' + dic_name + '.pickle', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "saving_dict = load_dict('saving_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_dict = load_dict('saving_dict')\n",
    "Q1_rewards_5_samples, Q1_time_5_samples = h.Q1(save = True)\n",
    "saving_dict['Q1']['samples_5'] = {}\n",
    "saving_dict['Q1']['samples_5']['Rewards'] = Q1_rewards_5_samples\n",
    "saving_dict['Q1']['samples_5']['Training'] = Q1_time_5_samples\n",
    "save_dict(saving_dict)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_star = [1, 1000, 5000, 10000, 20000]\n",
    "saving_dict = load_dict('saving_dict')\n",
    "Q2_dict_rewards_5_samples, Q2_times_5_samples = h.Q2(N_star = N_star, save = True)\n",
    "saving_dict['Q2']['samples_5'] = {}\n",
    "saving_dict['Q2']['samples_5']['Rewards'] = Q2_dict_rewards_5_samples\n",
    "saving_dict['Q2']['samples_5']['Training'] = Q2_times_5_samples\n",
    "save_dict(saving_dict)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a big $n_*$, the decrease of epsilon is very slow, and stays very close to $\\epsilon_\\text{max} = 0.8$. This prevents the algorithm from reaching a good performance after $20000$ games, compared to having a fixed epsilon. Surprisingly, with almost no exploration, the algorithm yields a similar performance as with 1000 games of exploration. The following cell allows to zoom more in detail to see the differences between $n_* = 1$ and $n_* = 1000$ (last sentence not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_star = [1, 1000, 5000, 10000, 20000]\n",
    "saving_dict = load_dict('saving_dict')\n",
    "Q3_dictMopt_5_samples, Q3_dictMrand_5_samples, Q3_times_opt_5_samples, Q3_times_rand_5_samples = h.Q3(N_star = N_star, save = True)\n",
    "saving_dict['Q3']['samples_5'] = {}\n",
    "saving_dict['Q3']['samples_5']['mopt'] = Q3_dictMopt_5_samples\n",
    "saving_dict['Q3']['samples_5']['mrand'] = Q3_dictMrand_5_samples\n",
    "saving_dict['Q3']['samples_5']['Training_Opt'] = Q3_times_opt_5_samples\n",
    "saving_dict['Q3']['samples_5']['Training_Rand'] = Q3_times_rand_5_samples\n",
    "save_dict(saving_dict)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eps_opt = [0, 0.01, 0.05, 0.1, 0.8]\n",
    "n_star = 20000     # to be modified according to what has been found in Q2 and Q3\n",
    "\n",
    "saving_dict = load_dict('saving_dict')\n",
    "Q4_final_Mopt_5_samples, Q4_final_Mrand_5_samples, Q4_times_opt_5_samples, Q4_times_rand_5_samples = h.Q4(Eps_opt = Eps_opt, n_star = n_star, save = True)\n",
    "saving_dict['Q4']['samples_5'] = {}\n",
    "saving_dict['Q4']['samples_5']['mopt_final'] = Q4_final_Mopt_5_samples\n",
    "saving_dict['Q4']['samples_5']['mrand_final'] = Q4_final_Mrand_5_samples\n",
    "saving_dict['Q4']['samples_5']['Training_Opt'] = Q4_times_opt_5_samples\n",
    "saving_dict['Q4']['samples_5']['Training_Rand'] = Q4_times_rand_5_samples\n",
    "save_dict(saving_dict)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first graph shows, as expected, that when the randomness increases for the optimal player, the performance against an optimal player decreases. It makes sense since the QL player didn't learn at all to play against a \"good\" player. Ont he second graph we can see that $M_{\\text{rand}}$ has similar evolution for $\\varepsilon_{\\text{opt}} < 1$, but for $1$, the performance is really bad. This can be explained by the fact that the QL-player has been trained to play a player that sometimes plays the optimal coup. But when it is faced to a totally random player, it sees configurations of the game that it has never seen before. Hence Q-values are still not updated and it is as if the training was useless. Hence the \"bad\" performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('q4 mopts: ', saving_dict['Q4']['samples_5']['mopt_final'], '\\nq4 mrands: ', saving_dict['Q4']['samples_5']['mrand_final'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, they don't have the same values. Playing against $\\text{opt}(0)$ will tend to make the QL-player biased toward playing the optimal policy and updating only the relevant Q-values (exploitation is favored). On the other hand, playing against $\\text{opt}(1)$ will tend to make the QL-player update all its Q-values (exploration is favored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eps = [0, 0.05, 0.1, 0.2, 0.8]\n",
    "Q7_final_Mopt_5_samples, Q7_final_Mrand_5_samples, Q7_times_opt_5_samples, Q7_times_rand_5_samples = h.Q7(Eps = Eps, save = True)\n",
    "saving_dict['Q7']['samples_5'] = {}\n",
    "saving_dict['Q7']['samples_5']['mopt_final'] = Q7_final_Mopt_5_samples\n",
    "saving_dict['Q7']['samples_5']['mrand_final'] = Q7_final_Mrand_5_samples\n",
    "saving_dict['Q7']['samples_5']['Training_Opt'] = Q7_times_opt_5_samples\n",
    "saving_dict['Q7']['samples_5']['Training_Rand'] = Q7_times_rand_5_samples\n",
    "save_dict(saving_dict)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that as soon as $\\varepsilon > 0$, the performance against an optimal player is drastically reduced. On the other hand it seems that it has less impact against a random player. What is surprising is that even for very low $\\varepsilon$, the performance is bad against optimal player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_star = [1, 1000, 5000, 10000, 20000]\n",
    "Q8_final_Mopt_5_samples, Q8_final_Mrand_5_samples, Q8_final_qvals_5_samples = h.Q8(N_star = N_star, save = True)\n",
    "saving_dict['Q8']['samples_5'] = {}\n",
    "saving_dict['Q8']['samples_5']['mopt_final'] = Q8_final_Mopt_5_samples\n",
    "saving_dict['Q8']['samples_5']['mrand_final'] = Q8_final_Mrand_5_samples\n",
    "saving_dict['Q8']['samples_5']['qvals_final'] = Q8_final_qvals_5_samples\n",
    "save_dict(saving_dict)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Against the optimal player it seems that increasing $n_*$ doesn't have a big impact on the final result, the only differences being the performance for early games. As expected when the exploration is higher (big $n_*$), the early performance is less good compared to small $n_*$. This can also be seen against the random player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('q8 mopts: ', saving_dict['Q8']['samples_5']['mopt_final'], '\\nq8 mrands: ', saving_dict['Q8']['samples_5']['mrand_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(['1', '1000', '5000', '10000', '20000']):\n",
    "    print(r'$n_* = {}: $'.format(int(n)), saving_dict['Q8']['samples_5']['mopt_final'][n] + 1 - saving_dict['Q8']['samples_5']['mrand_final'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvals = saving_dict['Q8']['samples_5']['qvals_final']['1'] # chnage last value to get the one corresponding to best score\n",
    "h.Q10(qval = qvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we see ? For the first configuration, it seems that although the players knows the best action, there is still a positive Q-value for a move that would lead to a loss against the optimal. This discrepancy can be explained by the fact that the player trains against himself with a lot of exploration. Should he be only greedy, this situation would probably not happend. The second plot shows that the player has a perfect understanding for this situation and would win the game. Finally for the last plot, we can use the optimal policy to determine if the prefered move is the correct one. The future position should have a nim-sum of 0 to ensure that the QL-player will win.  \n",
    "Here:   \n",
    "$002 \\to 000_2 \\oplus 000_2 \\oplus 010_2 = 010_2 \\neq 000_2$   \n",
    "$012 \\to 000_2 \\oplus 001_2 \\oplus 010_2 = 011_2 \\neq 000_2$  \n",
    "$022 \\to 000_2 \\oplus 010_2 \\oplus 010_2 = 000_2$  \n",
    "$030 \\to 000_2 \\oplus 011_2 \\oplus 000_2 = 011_2 \\neq 000_2$  \n",
    "$031 \\to 000_2 \\oplus 011_2 \\oplus 001_2 = 001_2 \\neq 000_2$  \n",
    "Hence, the best choice is indeed the move that is preferred by the QL-player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save of q10 with bars\n",
    "'''\n",
    "first_config = configs[0]\n",
    "second_config = configs[1]\n",
    "third_config = configs[2]\n",
    "fig, axs = plt.subplots(1, 3, figsize = (18, 6))\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "ax3 = axs[2]\n",
    "len1 = len(qval[first_config])\n",
    "len2 = len(qval[second_config])\n",
    "len3 = len(qval[third_config])\n",
    "tick_positions1 = np.linspace(1. / len1 , 1, len1, endpoint = False)\n",
    "tick_positions2 = np.linspace(1. / len2 , 1, len2, endpoint = False)\n",
    "tick_positions3 = np.linspace(1. / len3 , 1, len3, endpoint = False)\n",
    "\n",
    "keys1 = [t for t in qval[first_config].keys()]\n",
    "tick_labels1 = [str(int(first_config[0]) - int(keys1[i][0])) + str(int(first_config[1]) - int(keys1[i][1])) + str(int(first_config[2]) - int(keys1[i][2])) for i in range(len(keys1))]\n",
    "qvals1 = [q for q in qval[first_config].values()]\n",
    "ax1.set_xticks(tick_positions1)\n",
    "ax1.set_xticklabels(tick_labels1)\n",
    "ax1.set_xlabel('Possible actions')\n",
    "ax1.set_ylabel('Q-values')\n",
    "ax1.set_title('Current configuration: ' + str(first_config[0]) + ' | ' + str(first_config[1]) + ' | ' + str(first_config[2]))\n",
    "ax1.bar(tick_positions1, qvals1, width = 1. / (2 * len1))\n",
    "    \n",
    "keys2 = [t for t in qval[second_config].keys()]\n",
    "tick_labels2 = [str(int(second_config[0]) - int(keys2[i][0])) + str(int(second_config[1]) - int(keys2[i][1])) + str(int(second_config[2]) - int(keys2[i][2])) for i in range(len(keys2))]\n",
    "qvals2 = [q for q in qval[second_config].values()]\n",
    "ax2.set_xticks(tick_positions2)\n",
    "ax2.set_xticklabels(tick_labels2)\n",
    "ax2.set_xlabel('Possible actions')\n",
    "ax2.set_ylabel('Q-values')\n",
    "ax2.set_title('Current configuration: ' + str(second_config[0]) + ' | ' + str(second_config[1]) + ' | ' + str(second_config[2]))\n",
    "ax2.bar(tick_positions2, qvals2, width = 1. / (2 * len2))\n",
    "    \n",
    "keys3 = [t for t in qval[third_config].keys()]\n",
    "tick_labels3 = [str(int(third_config[0]) - int(keys3[i][0])) + str(int(third_config[1]) - int(keys3[i][1])) + str(int(third_config[2]) - int(keys3[i][2])) for i in range(len(keys3))]\n",
    "qvals3 = [q for q in qval[third_config].values()]\n",
    "ax3.set_xticks(tick_positions3)\n",
    "ax3.set_xticklabels(tick_labels3)\n",
    "ax3.set_xlabel('Possible actions')\n",
    "ax3.set_ylabel('Q-values')\n",
    "ax3.set_title('Current configuration: ' + str(third_config[0]) + ' | ' + str(third_config[1]) + ' | ' + str(third_config[2]))\n",
    "ax3.bar(tick_positions3, qvals3, width = 1. / (2 * len3))\n",
    "if save:\n",
    "    fig.savefig('./Data/' + question + '.png')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52af2100627e19856ff19ffeecc72f0dde92b1b054ee87d4964f694fb586b018"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
